{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.0)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.4.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install -U imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: filelock in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gahyunlee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from collections import Counter\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier, Pool, cv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 레이블 인코딩할 칼럼들\n",
        "cat_columns = [\n",
        "    \"customer_country\",\n",
        "    \"business_subarea\",\n",
        "    \"business_area\",\n",
        "    \"business_unit\",\n",
        "    \"customer_type\",\n",
        "    \"customer_idx\",\n",
        "    \"enterprise\",\n",
        "    \"customer_job\",\n",
        "    \"inquiry_type\",\n",
        "    \"product_category\",\n",
        "    \"product_subcategory\",\n",
        "    \"product_modelname\",\n",
        "    \"customer_position\",\n",
        "    \"response_corporate\",\n",
        "    \"expected_timeline\",\n",
        "    \"category\",\n",
        "    \"product_count\",\n",
        "    \"timeline_count\",\n",
        "    \"idit_all\",\n",
        "    \"lead_owner\",\n",
        "    \"bant_submit_count\",\n",
        "    \"com_reg_count\",\n",
        "    \"idx_count\",\n",
        "    \"lead_count\",\n",
        "    \"enterprise_count\",\n",
        "    \"enterprise_weight\"\n",
        "]\n",
        "\n",
        "def index_processing(context_df, train, test, column_name):\n",
        "    idx = {v:k for k,v in enumerate(context_df[column_name].unique())}\n",
        "    train.loc[:, column_name] = train[column_name].map(idx)\n",
        "    test.loc[:, column_name] = test[column_name].map(idx)\n",
        "    return idx\n",
        "\n",
        "def process_context_data(train_df, test_df):\n",
        "    context_df = pd.concat([train_df[cat_columns], test_df[cat_columns]]).reset_index(drop=True)\n",
        "    idx = {}\n",
        "    for col in cat_columns:\n",
        "        idx_name = index_processing(context_df, train_df, test_df, col)\n",
        "        idx[col+'2idx'] = idx_name\n",
        "    return idx, train_df, test_df\n",
        "\n",
        "def context_data_load():\n",
        "    ######################## DATA LOAD\n",
        "    train = pd.read_csv('train_final.csv', low_memory=False)\n",
        "    test = pd.read_csv('submission_final.csv')\n",
        "\n",
        "    idx, context_train, context_test = process_context_data(train, test)\n",
        "    field_dims = np.array([len(toidx) for toidx in idx], dtype=np.int32)\n",
        "\n",
        "    data = {\n",
        "            'train':context_train.fillna(0),\n",
        "            'test':context_test.fillna(0),\n",
        "            'field_dims':field_dims,\n",
        "            'cat_columns' : cat_columns,\n",
        "            }\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "def context_data_split(data):\n",
        "    # SMOTE를 사용하여 데이터 오버샘플링\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(data['train'].drop(['is_converted'], axis=1), data['train']['is_converted'])\n",
        "\n",
        "    # 샘플링된 데이터를 다시 훈련 데이터와 테스트 데이터로 분할\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_resampled, \n",
        "                                                      y_resampled, \n",
        "                                                      test_size=0.2, \n",
        "                                                      random_state=42, \n",
        "                                                      stratify=y_resampled)\n",
        "\n",
        "    y_train = y_train.astype(np.int32) ; y_valid = y_valid.astype(np.int32)\n",
        "    data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
        "    \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\298523317.py:54: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  'train':context_train.fillna(0),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\298523317.py:55: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  'test':context_test.fillna(0),\n"
          ]
        }
      ],
      "source": [
        "data = context_data_load()\n",
        "data = context_data_split(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost 모델 훈련\n",
        "train_pool = Pool(data['X_train'], label=data['y_train'])\n",
        "valid_pool = Pool(data['X_valid'], label=data['y_valid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_catboost(trial):\n",
        "    param = {\n",
        "        \"random_state\": 42,\n",
        "        'early_stopping_rounds': 20,\n",
        "        'loss_function': 'Logloss',\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 3000),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 8),\n",
        "        'random_strength': trial.suggest_int('random_strength', 0, 50),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 3e-5),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
        "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
        "    }\n",
        "\n",
        "    model = cb.CatBoostClassifier(**param, verbose=0)\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "\n",
        "    # 검증 데이터셋에 대한 예측 및 정확도 계산\n",
        "    pred = model.predict(data['X_valid'])\n",
        "    # pred = np.vectorize(lambda x: x.lower())(model.predict(x_val))\n",
        "    pred = [val == 1 for val in pred]\n",
        "    F1 = f1_score(data['y_valid'], pred, labels=[True, False])\n",
        "    return F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-20 18:00:09,916] A new study created in memory with name: no-name-1ace51dc-5738-47d2-b979-41d756df2a5c\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:00:53,628] Trial 0 finished with value: 0.9710066982838703 and parameters: {'learning_rate': 0.07762312957102822, 'bagging_temperature': 0.4514295330868797, 'n_estimators': 1694, 'max_depth': 2, 'random_strength': 11, 'l2_leaf_reg': 2.6565538705911684e-05, 'min_child_samples': 34, 'max_bin': 257}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:01:50,448] Trial 1 finished with value: 0.9703232125367287 and parameters: {'learning_rate': 0.02564420640306185, 'bagging_temperature': 0.1213048080692085, 'n_estimators': 2153, 'max_depth': 3, 'random_strength': 16, 'l2_leaf_reg': 2.315535063460672e-05, 'min_child_samples': 23, 'max_bin': 203}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:02:26,677] Trial 2 finished with value: 0.9172389940214437 and parameters: {'learning_rate': 0.03641138974533642, 'bagging_temperature': 0.0995623847711883, 'n_estimators': 1812, 'max_depth': 1, 'random_strength': 32, 'l2_leaf_reg': 2.303846247898941e-05, 'min_child_samples': 32, 'max_bin': 236}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:03:24,743] Trial 3 finished with value: 0.9675776275835047 and parameters: {'learning_rate': 0.01411057805851433, 'bagging_temperature': 11.769699585241982, 'n_estimators': 1659, 'max_depth': 5, 'random_strength': 45, 'l2_leaf_reg': 1.1897214724857686e-05, 'min_child_samples': 15, 'max_bin': 155}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:03:42,380] Trial 4 finished with value: 0.9526609820640988 and parameters: {'learning_rate': 0.051901394289235875, 'bagging_temperature': 7.075496533398243, 'n_estimators': 756, 'max_depth': 2, 'random_strength': 8, 'l2_leaf_reg': 1.930976318268324e-05, 'min_child_samples': 40, 'max_bin': 168}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:03:59,568] Trial 5 finished with value: 0.9060366422712314 and parameters: {'learning_rate': 0.020508570607604196, 'bagging_temperature': 8.147749232582369, 'n_estimators': 728, 'max_depth': 2, 'random_strength': 26, 'l2_leaf_reg': 1.545056119929905e-05, 'min_child_samples': 9, 'max_bin': 263}. Best is trial 0 with value: 0.9710066982838703.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:04:26,253] Trial 6 finished with value: 0.9752778185734567 and parameters: {'learning_rate': 0.09184108754670571, 'bagging_temperature': 16.166511131602373, 'n_estimators': 933, 'max_depth': 3, 'random_strength': 29, 'l2_leaf_reg': 3.0276647125178833e-06, 'min_child_samples': 17, 'max_bin': 186}. Best is trial 6 with value: 0.9752778185734567.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:05:16,849] Trial 7 finished with value: 0.9176761433868974 and parameters: {'learning_rate': 0.023600135032210652, 'bagging_temperature': 0.7587889989134294, 'n_estimators': 2473, 'max_depth': 1, 'random_strength': 13, 'l2_leaf_reg': 1.7323462577006207e-05, 'min_child_samples': 19, 'max_bin': 178}. Best is trial 6 with value: 0.9752778185734567.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:06:17,997] Trial 8 finished with value: 0.9655747351902707 and parameters: {'learning_rate': 0.028688278003136638, 'bagging_temperature': 0.023046616965520253, 'n_estimators': 2675, 'max_depth': 2, 'random_strength': 22, 'l2_leaf_reg': 2.3328234717319712e-05, 'min_child_samples': 26, 'max_bin': 176}. Best is trial 6 with value: 0.9752778185734567.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:07:43,129] Trial 9 finished with value: 0.9863722952180921 and parameters: {'learning_rate': 0.021528827165120498, 'bagging_temperature': 0.06139893322806228, 'n_estimators': 1649, 'max_depth': 7, 'random_strength': 24, 'l2_leaf_reg': 2.2001691998635834e-05, 'min_child_samples': 11, 'max_bin': 286}. Best is trial 9 with value: 0.9863722952180921.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:07:52,498] Trial 10 finished with value: 0.9378170749383968 and parameters: {'learning_rate': 0.010047894559108459, 'bagging_temperature': 0.019731355830137666, 'n_estimators': 1235, 'max_depth': 8, 'random_strength': 1, 'l2_leaf_reg': 2.966245939897694e-05, 'min_child_samples': 49, 'max_bin': 299}. Best is trial 9 with value: 0.9863722952180921.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:08:36,233] Trial 11 finished with value: 0.9883538853004502 and parameters: {'learning_rate': 0.08831682935817756, 'bagging_temperature': 88.01275532231095, 'n_estimators': 1078, 'max_depth': 6, 'random_strength': 36, 'l2_leaf_reg': 4.739528601492276e-06, 'min_child_samples': 5, 'max_bin': 210}. Best is trial 11 with value: 0.9883538853004502.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:09:39,574] Trial 12 finished with value: 0.9887146416532318 and parameters: {'learning_rate': 0.04759175205781002, 'bagging_temperature': 80.48201437419605, 'n_estimators': 1311, 'max_depth': 7, 'random_strength': 38, 'l2_leaf_reg': 1.2233058502908398e-06, 'min_child_samples': 5, 'max_bin': 219}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:10:31,615] Trial 13 finished with value: 0.986741034297177 and parameters: {'learning_rate': 0.05264043980692631, 'bagging_temperature': 90.16969642704284, 'n_estimators': 1268, 'max_depth': 6, 'random_strength': 43, 'l2_leaf_reg': 3.939628455342768e-07, 'min_child_samples': 5, 'max_bin': 210}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:11:21,044] Trial 14 finished with value: 0.9877291615741872 and parameters: {'learning_rate': 0.06095093569918837, 'bagging_temperature': 75.30627209808233, 'n_estimators': 1185, 'max_depth': 6, 'random_strength': 36, 'l2_leaf_reg': 7.019570282682978e-06, 'min_child_samples': 6, 'max_bin': 229}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "Training has stopped (degenerate solution on iteration 137, probably too small l2-regularization, try to increase it)\n",
            "[I 2024-02-20 18:11:29,205] Trial 15 finished with value: 0.9246240968560828 and parameters: {'learning_rate': 0.03842858049705486, 'bagging_temperature': 29.883107059230202, 'n_estimators': 518, 'max_depth': 8, 'random_strength': 39, 'l2_leaf_reg': 7.319282523510824e-06, 'min_child_samples': 12, 'max_bin': 207}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:12:18,693] Trial 16 finished with value: 0.9862116174457266 and parameters: {'learning_rate': 0.07284027643845213, 'bagging_temperature': 4.127990203432391, 'n_estimators': 1395, 'max_depth': 5, 'random_strength': 50, 'l2_leaf_reg': 1.0355915939851198e-05, 'min_child_samples': 21, 'max_bin': 242}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:12:33,345] Trial 17 finished with value: 0.9804782820888238 and parameters: {'learning_rate': 0.09429331837993148, 'bagging_temperature': 2.3035894697310515, 'n_estimators': 2011, 'max_depth': 7, 'random_strength': 35, 'l2_leaf_reg': 3.927374100393843e-06, 'min_child_samples': 6, 'max_bin': 196}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:13:15,218] Trial 18 finished with value: 0.9850396010560282 and parameters: {'learning_rate': 0.047623941867088786, 'bagging_temperature': 42.44257400010365, 'n_estimators': 1016, 'max_depth': 6, 'random_strength': 41, 'l2_leaf_reg': 5.014039016659759e-07, 'min_child_samples': 14, 'max_bin': 217}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:13:57,456] Trial 19 finished with value: 0.9820704479945284 and parameters: {'learning_rate': 0.06816828584304767, 'bagging_temperature': 28.886273740987694, 'n_estimators': 1377, 'max_depth': 4, 'random_strength': 49, 'l2_leaf_reg': 6.01363726053063e-06, 'min_child_samples': 38, 'max_bin': 249}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:14:11,769] Trial 20 finished with value: 0.951001901418751 and parameters: {'learning_rate': 0.04240052232158057, 'bagging_temperature': 2.9391612783930654, 'n_estimators': 516, 'max_depth': 7, 'random_strength': 20, 'l2_leaf_reg': 1.0445807693787225e-05, 'min_child_samples': 29, 'max_bin': 228}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:14:59,608] Trial 21 finished with value: 0.9871945259042033 and parameters: {'learning_rate': 0.06261710514120379, 'bagging_temperature': 75.13202550296559, 'n_estimators': 1177, 'max_depth': 6, 'random_strength': 36, 'l2_leaf_reg': 7.752620038426772e-06, 'min_child_samples': 8, 'max_bin': 223}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:15:37,875] Trial 22 finished with value: 0.9857722583484085 and parameters: {'learning_rate': 0.05709667940635484, 'bagging_temperature': 72.5789814849284, 'n_estimators': 922, 'max_depth': 6, 'random_strength': 32, 'l2_leaf_reg': 3.486513469593588e-06, 'min_child_samples': 6, 'max_bin': 227}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:16:41,643] Trial 23 finished with value: 0.9868987094251076 and parameters: {'learning_rate': 0.08016223361124018, 'bagging_temperature': 25.082661421705236, 'n_estimators': 2996, 'max_depth': 5, 'random_strength': 38, 'l2_leaf_reg': 5.469658434617645e-06, 'min_child_samples': 11, 'max_bin': 194}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:17:52,960] Trial 24 finished with value: 0.9882226457508674 and parameters: {'learning_rate': 0.03277913794080914, 'bagging_temperature': 49.67336668483726, 'n_estimators': 1434, 'max_depth': 7, 'random_strength': 31, 'l2_leaf_reg': 1.2372355569881186e-05, 'min_child_samples': 5, 'max_bin': 263}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:18:14,478] Trial 25 finished with value: 0.9496641028137475 and parameters: {'learning_rate': 0.03270029565403103, 'bagging_temperature': 44.20737343144143, 'n_estimators': 1507, 'max_depth': 8, 'random_strength': 28, 'l2_leaf_reg': 1.2470664259304562e-05, 'min_child_samples': 15, 'max_bin': 269}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:19:56,440] Trial 26 finished with value: 0.9861728636341428 and parameters: {'learning_rate': 0.018343449176138622, 'bagging_temperature': 14.486924144242161, 'n_estimators': 1905, 'max_depth': 7, 'random_strength': 32, 'l2_leaf_reg': 1.8507553070882544e-06, 'min_child_samples': 10, 'max_bin': 276}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:20:16,017] Trial 27 finished with value: 0.9418542205731583 and parameters: {'learning_rate': 0.03252866542423569, 'bagging_temperature': 4.905800291624949, 'n_estimators': 1516, 'max_depth': 7, 'random_strength': 45, 'l2_leaf_reg': 9.077210906132991e-06, 'min_child_samples': 5, 'max_bin': 247}. Best is trial 12 with value: 0.9887146416532318.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:21:34,913] Trial 28 finished with value: 0.9900845015386118 and parameters: {'learning_rate': 0.043993713485506236, 'bagging_temperature': 1.3269459430776536, 'n_estimators': 2211, 'max_depth': 8, 'random_strength': 30, 'l2_leaf_reg': 1.3429761994753098e-05, 'min_child_samples': 46, 'max_bin': 281}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:22:42,677] Trial 29 finished with value: 0.9894479726428921 and parameters: {'learning_rate': 0.04544825594914348, 'bagging_temperature': 0.32137843751421075, 'n_estimators': 2259, 'max_depth': 8, 'random_strength': 40, 'l2_leaf_reg': 1.5362135675251832e-05, 'min_child_samples': 43, 'max_bin': 300}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:24:10,926] Trial 30 finished with value: 0.9896423685753372 and parameters: {'learning_rate': 0.04456104827680579, 'bagging_temperature': 0.33255606865267584, 'n_estimators': 2320, 'max_depth': 8, 'random_strength': 46, 'l2_leaf_reg': 1.4954969947050157e-05, 'min_child_samples': 50, 'max_bin': 300}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:25:29,072] Trial 31 finished with value: 0.9893996385130184 and parameters: {'learning_rate': 0.04407798039510026, 'bagging_temperature': 0.46041598812364726, 'n_estimators': 2389, 'max_depth': 8, 'random_strength': 47, 'l2_leaf_reg': 1.5986573906899633e-05, 'min_child_samples': 50, 'max_bin': 299}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:25:56,832] Trial 32 finished with value: 0.9730624969555264 and parameters: {'learning_rate': 0.04203041190870887, 'bagging_temperature': 0.3003389286104469, 'n_estimators': 2297, 'max_depth': 8, 'random_strength': 47, 'l2_leaf_reg': 1.5282235286534878e-05, 'min_child_samples': 49, 'max_bin': 300}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:27:27,560] Trial 33 finished with value: 0.989942388438629 and parameters: {'learning_rate': 0.03947086916527769, 'bagging_temperature': 0.3118750484795663, 'n_estimators': 2279, 'max_depth': 8, 'random_strength': 43, 'l2_leaf_reg': 1.861073239109036e-05, 'min_child_samples': 44, 'max_bin': 288}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:28:08,270] Trial 34 finished with value: 0.9745039730902355 and parameters: {'learning_rate': 0.027673865137161722, 'bagging_temperature': 0.18635813339565402, 'n_estimators': 2137, 'max_depth': 8, 'random_strength': 41, 'l2_leaf_reg': 1.9307469123123902e-05, 'min_child_samples': 45, 'max_bin': 285}. Best is trial 28 with value: 0.9900845015386118.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:29:46,494] Trial 35 finished with value: 0.9907144951617632 and parameters: {'learning_rate': 0.040075823012502135, 'bagging_temperature': 1.388074362792074, 'n_estimators': 2679, 'max_depth': 8, 'random_strength': 43, 'l2_leaf_reg': 1.814211208460745e-05, 'min_child_samples': 44, 'max_bin': 288}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:31:23,806] Trial 36 finished with value: 0.9898407736641595 and parameters: {'learning_rate': 0.038182965823275045, 'bagging_temperature': 1.0950125048712318, 'n_estimators': 2622, 'max_depth': 8, 'random_strength': 43, 'l2_leaf_reg': 1.8716213672524175e-05, 'min_child_samples': 36, 'max_bin': 286}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:32:43,909] Trial 37 finished with value: 0.98310546875 and parameters: {'learning_rate': 0.03748166740454418, 'bagging_temperature': 1.5760446257418357, 'n_estimators': 2592, 'max_depth': 4, 'random_strength': 43, 'l2_leaf_reg': 2.0554148658554007e-05, 'min_child_samples': 36, 'max_bin': 286}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "Training has stopped (degenerate solution on iteration 238, probably too small l2-regularization, try to increase it)\n",
            "[I 2024-02-20 18:33:00,111] Trial 38 finished with value: 0.948599269183922 and parameters: {'learning_rate': 0.037152478316441015, 'bagging_temperature': 0.8807719019735674, 'n_estimators': 2842, 'max_depth': 8, 'random_strength': 18, 'l2_leaf_reg': 2.5950513132200837e-05, 'min_child_samples': 42, 'max_bin': 276}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "Training has stopped (degenerate solution on iteration 1315, probably too small l2-regularization, try to increase it)\n",
            "[I 2024-02-20 18:34:09,448] Trial 39 finished with value: 0.9869115061535456 and parameters: {'learning_rate': 0.028726076010189866, 'bagging_temperature': 1.6420716852365942, 'n_estimators': 2678, 'max_depth': 7, 'random_strength': 43, 'l2_leaf_reg': 1.8015912475194764e-05, 'min_child_samples': 33, 'max_bin': 278}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:35:19,424] Trial 40 finished with value: 0.9897440906427036 and parameters: {'learning_rate': 0.0523152380716557, 'bagging_temperature': 0.6053275350952758, 'n_estimators': 2538, 'max_depth': 8, 'random_strength': 10, 'l2_leaf_reg': 1.3728934778849812e-05, 'min_child_samples': 46, 'max_bin': 258}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:36:22,513] Trial 41 finished with value: 0.9901299716603147 and parameters: {'learning_rate': 0.053462327381303604, 'bagging_temperature': 0.6760036435281646, 'n_estimators': 2511, 'max_depth': 8, 'random_strength': 11, 'l2_leaf_reg': 1.3501885451236544e-05, 'min_child_samples': 45, 'max_bin': 256}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:38:01,507] Trial 42 finished with value: 0.9899828976301002 and parameters: {'learning_rate': 0.03836934147255141, 'bagging_temperature': 1.4688264702858553, 'n_estimators': 2808, 'max_depth': 8, 'random_strength': 5, 'l2_leaf_reg': 1.7516044109352018e-05, 'min_child_samples': 39, 'max_bin': 291}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:39:36,553] Trial 43 finished with value: 0.9897380766223612 and parameters: {'learning_rate': 0.03449709550475534, 'bagging_temperature': 0.14642002883257724, 'n_estimators': 2806, 'max_depth': 7, 'random_strength': 4, 'l2_leaf_reg': 2.0581023178458917e-05, 'min_child_samples': 46, 'max_bin': 290}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:42:00,642] Trial 44 finished with value: 0.9895014405000244 and parameters: {'learning_rate': 0.0246289710368959, 'bagging_temperature': 0.07611762704765314, 'n_estimators': 2452, 'max_depth': 8, 'random_strength': 14, 'l2_leaf_reg': 1.6845499499275748e-05, 'min_child_samples': 40, 'max_bin': 269}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:43:04,577] Trial 45 finished with value: 0.9899819185847627 and parameters: {'learning_rate': 0.05379460905346367, 'bagging_temperature': 1.4769624690913055, 'n_estimators': 2786, 'max_depth': 7, 'random_strength': 5, 'l2_leaf_reg': 1.4043235372807119e-05, 'min_child_samples': 43, 'max_bin': 291}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:44:01,168] Trial 46 finished with value: 0.9892021302584648 and parameters: {'learning_rate': 0.05648568161921774, 'bagging_temperature': 1.374832013575868, 'n_estimators': 2979, 'max_depth': 7, 'random_strength': 6, 'l2_leaf_reg': 1.4497745044890452e-05, 'min_child_samples': 41, 'max_bin': 292}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:45:06,817] Trial 47 finished with value: 0.9805417033343111 and parameters: {'learning_rate': 0.04924879627181907, 'bagging_temperature': 2.3255323138018396, 'n_estimators': 2802, 'max_depth': 3, 'random_strength': 1, 'l2_leaf_reg': 1.3536283918328062e-05, 'min_child_samples': 47, 'max_bin': 279}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:45:54,545] Trial 48 finished with value: 0.9407473134181209 and parameters: {'learning_rate': 0.06678325889760525, 'bagging_temperature': 5.794874801202777, 'n_estimators': 2729, 'max_depth': 1, 'random_strength': 9, 'l2_leaf_reg': 1.0815360078883591e-05, 'min_child_samples': 39, 'max_bin': 267}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:46:54,880] Trial 49 finished with value: 0.9895848613759718 and parameters: {'learning_rate': 0.05488228302259861, 'bagging_temperature': 8.632006797979047, 'n_estimators': 2163, 'max_depth': 7, 'random_strength': 5, 'l2_leaf_reg': 2.444437453480184e-05, 'min_child_samples': 36, 'max_bin': 256}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:47:07,763] Trial 50 finished with value: 0.9677639267627581 and parameters: {'learning_rate': 0.07561756316994651, 'bagging_temperature': 0.5641476133932428, 'n_estimators': 2891, 'max_depth': 8, 'random_strength': 13, 'l2_leaf_reg': 1.6933321688774924e-05, 'min_child_samples': 48, 'max_bin': 294}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:48:00,477] Trial 51 finished with value: 0.9893502686858818 and parameters: {'learning_rate': 0.04942628327854426, 'bagging_temperature': 0.8141548069799995, 'n_estimators': 2503, 'max_depth': 8, 'random_strength': 0, 'l2_leaf_reg': 2.142057665532937e-05, 'min_child_samples': 44, 'max_bin': 283}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:49:26,964] Trial 52 finished with value: 0.9901261120344119 and parameters: {'learning_rate': 0.04054865393065432, 'bagging_temperature': 1.1216825480351837, 'n_estimators': 2718, 'max_depth': 8, 'random_strength': 4, 'l2_leaf_reg': 1.760130025265526e-05, 'min_child_samples': 42, 'max_bin': 292}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:51:25,226] Trial 53 finished with value: 0.9894932316864585 and parameters: {'learning_rate': 0.02859666177991991, 'bagging_temperature': 3.235355680147254, 'n_estimators': 2736, 'max_depth': 7, 'random_strength': 3, 'l2_leaf_reg': 1.3721306365488039e-05, 'min_child_samples': 41, 'max_bin': 155}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:52:30,435] Trial 54 finished with value: 0.9892515145593122 and parameters: {'learning_rate': 0.04170841561045552, 'bagging_temperature': 1.9695482598249894, 'n_estimators': 2908, 'max_depth': 8, 'random_strength': 8, 'l2_leaf_reg': 1.6397952148275167e-05, 'min_child_samples': 30, 'max_bin': 274}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "Training has stopped (degenerate solution on iteration 822, probably too small l2-regularization, try to increase it)\n",
            "[I 2024-02-20 18:53:08,262] Trial 55 finished with value: 0.9859842750402891 and parameters: {'learning_rate': 0.03546719707409152, 'bagging_temperature': 1.0271503371884703, 'n_estimators': 2599, 'max_depth': 7, 'random_strength': 7, 'l2_leaf_reg': 1.7675527613144326e-05, 'min_child_samples': 24, 'max_bin': 293}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:55:24,078] Trial 56 finished with value: 0.9880341880341881 and parameters: {'learning_rate': 0.012206420120523863, 'bagging_temperature': 3.0335361046431784, 'n_estimators': 2428, 'max_depth': 8, 'random_strength': 11, 'l2_leaf_reg': 1.1728496688408317e-05, 'min_child_samples': 38, 'max_bin': 280}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:56:30,019] Trial 57 finished with value: 0.9894407508799374 and parameters: {'learning_rate': 0.0605990573035413, 'bagging_temperature': 1.2049711818346565, 'n_estimators': 2733, 'max_depth': 7, 'random_strength': 2, 'l2_leaf_reg': 9.467462581522425e-06, 'min_child_samples': 42, 'max_bin': 273}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:57:47,745] Trial 58 finished with value: 0.9899335418295543 and parameters: {'learning_rate': 0.04061083039164121, 'bagging_temperature': 0.5480269147229794, 'n_estimators': 2543, 'max_depth': 8, 'random_strength': 24, 'l2_leaf_reg': 1.2790165512063998e-05, 'min_child_samples': 47, 'max_bin': 256}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:58:52,448] Trial 59 finished with value: 0.9888573941941159 and parameters: {'learning_rate': 0.0469479382082337, 'bagging_temperature': 0.03912704460529419, 'n_estimators': 1755, 'max_depth': 6, 'random_strength': 15, 'l2_leaf_reg': 1.951317744973535e-05, 'min_child_samples': 44, 'max_bin': 263}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 18:59:20,603] Trial 60 finished with value: 0.9752195121951219 and parameters: {'learning_rate': 0.030020849871601755, 'bagging_temperature': 0.21010720834023588, 'n_estimators': 2078, 'max_depth': 7, 'random_strength': 17, 'l2_leaf_reg': 1.1312090958977175e-05, 'min_child_samples': 38, 'max_bin': 234}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:00:52,062] Trial 61 finished with value: 0.9901754728970136 and parameters: {'learning_rate': 0.03915430769584641, 'bagging_temperature': 0.6694713563227093, 'n_estimators': 1935, 'max_depth': 8, 'random_strength': 11, 'l2_leaf_reg': 1.83231996284787e-05, 'min_child_samples': 45, 'max_bin': 291}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:01:52,196] Trial 62 finished with value: 0.9898368025017101 and parameters: {'learning_rate': 0.050902461619271075, 'bagging_temperature': 0.6608118314497309, 'n_estimators': 2386, 'max_depth': 8, 'random_strength': 12, 'l2_leaf_reg': 2.2668327406880958e-05, 'min_child_samples': 45, 'max_bin': 294}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:02:57,751] Trial 63 finished with value: 0.9889057230829382 and parameters: {'learning_rate': 0.03444233653974486, 'bagging_temperature': 1.9673224999361008, 'n_estimators': 1944, 'max_depth': 8, 'random_strength': 20, 'l2_leaf_reg': 1.9917370689630642e-05, 'min_child_samples': 43, 'max_bin': 293}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:03:43,561] Trial 64 finished with value: 0.9898891222585845 and parameters: {'learning_rate': 0.06471038161293585, 'bagging_temperature': 0.4275034481308222, 'n_estimators': 2175, 'max_depth': 8, 'random_strength': 5, 'l2_leaf_reg': 1.4419170708790972e-05, 'min_child_samples': 48, 'max_bin': 283}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:04:46,280] Trial 65 finished with value: 0.9895838427306959 and parameters: {'learning_rate': 0.05901573120444929, 'bagging_temperature': 3.6651060987937143, 'n_estimators': 2671, 'max_depth': 7, 'random_strength': 7, 'l2_leaf_reg': 1.7911768806663736e-05, 'min_child_samples': 41, 'max_bin': 281}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:05:01,636] Trial 66 finished with value: 0.9730860952937168 and parameters: {'learning_rate': 0.04340685678801861, 'bagging_temperature': 0.7418884210275366, 'n_estimators': 1936, 'max_depth': 8, 'random_strength': 3, 'l2_leaf_reg': 1.5978013909797676e-05, 'min_child_samples': 46, 'max_bin': 289}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:05:16,253] Trial 67 finished with value: 0.9779989267769159 and parameters: {'learning_rate': 0.08365444183880048, 'bagging_temperature': 1.3443306949593847, 'n_estimators': 2951, 'max_depth': 8, 'random_strength': 26, 'l2_leaf_reg': 1.312782002229917e-05, 'min_child_samples': 43, 'max_bin': 296}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:06:41,324] Trial 68 finished with value: 0.9890550180787648 and parameters: {'learning_rate': 0.03130688673118263, 'bagging_temperature': 2.2399239950989798, 'n_estimators': 1836, 'max_depth': 7, 'random_strength': 9, 'l2_leaf_reg': 2.1110490673765094e-05, 'min_child_samples': 40, 'max_bin': 273}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:07:49,996] Trial 69 finished with value: 0.988862837045721 and parameters: {'learning_rate': 0.04706701422033344, 'bagging_temperature': 0.9616317761631195, 'n_estimators': 2827, 'max_depth': 8, 'random_strength': 34, 'l2_leaf_reg': 1.7245745174217763e-05, 'min_child_samples': 49, 'max_bin': 250}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:09:02,600] Trial 70 finished with value: 0.9873949579831933 and parameters: {'learning_rate': 0.05362312267407759, 'bagging_temperature': 0.01360258010202117, 'n_estimators': 2211, 'max_depth': 5, 'random_strength': 11, 'l2_leaf_reg': 1.4546192925934328e-05, 'min_child_samples': 45, 'max_bin': 289}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:10:27,361] Trial 71 finished with value: 0.9905227161700049 and parameters: {'learning_rate': 0.03985914742973796, 'bagging_temperature': 0.2426350726394998, 'n_estimators': 2049, 'max_depth': 8, 'random_strength': 6, 'l2_leaf_reg': 1.8803164674174395e-05, 'min_child_samples': 43, 'max_bin': 288}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:11:41,204] Trial 72 finished with value: 0.989739079448842 and parameters: {'learning_rate': 0.03935231249770342, 'bagging_temperature': 0.42038625440877, 'n_estimators': 1996, 'max_depth': 8, 'random_strength': 7, 'l2_leaf_reg': 1.8512375813832357e-05, 'min_child_samples': 42, 'max_bin': 283}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:12:56,711] Trial 73 finished with value: 0.9893513091051191 and parameters: {'learning_rate': 0.036477165363577625, 'bagging_temperature': 1.5881425196035637, 'n_estimators': 2064, 'max_depth': 8, 'random_strength': 5, 'l2_leaf_reg': 1.6140600563536823e-05, 'min_child_samples': 47, 'max_bin': 297}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:13:59,505] Trial 74 finished with value: 0.9899315738025416 and parameters: {'learning_rate': 0.045055162861599926, 'bagging_temperature': 0.2479838157815892, 'n_estimators': 2353, 'max_depth': 8, 'random_strength': 0, 'l2_leaf_reg': 2.23168120012172e-05, 'min_child_samples': 44, 'max_bin': 168}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:15:02,858] Trial 75 finished with value: 0.9885652853792025 and parameters: {'learning_rate': 0.04032525628877892, 'bagging_temperature': 0.10368723402358657, 'n_estimators': 1692, 'max_depth': 7, 'random_strength': 2, 'l2_leaf_reg': 1.9066895800717547e-05, 'min_child_samples': 39, 'max_bin': 288}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:16:46,235] Trial 76 finished with value: 0.990470605483067 and parameters: {'learning_rate': 0.026749952830043017, 'bagging_temperature': 2.6644376694097884, 'n_estimators': 1867, 'max_depth': 8, 'random_strength': 9, 'l2_leaf_reg': 1.5563897934227105e-05, 'min_child_samples': 34, 'max_bin': 270}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:17:22,330] Trial 77 finished with value: 0.9491042944785276 and parameters: {'learning_rate': 0.020109074626127675, 'bagging_temperature': 0.7773609610040044, 'n_estimators': 1572, 'max_depth': 2, 'random_strength': 9, 'l2_leaf_reg': 2.0228329010068996e-05, 'min_child_samples': 27, 'max_bin': 265}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:17:49,927] Trial 78 finished with value: 0.9653460527596612 and parameters: {'learning_rate': 0.02603361423205643, 'bagging_temperature': 4.362021775083158, 'n_estimators': 1764, 'max_depth': 8, 'random_strength': 13, 'l2_leaf_reg': 2.985472963618167e-05, 'min_child_samples': 34, 'max_bin': 260}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:19:34,208] Trial 79 finished with value: 0.98876953125 and parameters: {'learning_rate': 0.02195568193156598, 'bagging_temperature': 2.5159379184079387, 'n_estimators': 1817, 'max_depth': 8, 'random_strength': 22, 'l2_leaf_reg': 1.531801713513104e-05, 'min_child_samples': 37, 'max_bin': 270}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:19:45,268] Trial 80 finished with value: 0.952603406326034 and parameters: {'learning_rate': 0.03090713642240168, 'bagging_temperature': 0.13685212912388395, 'n_estimators': 2045, 'max_depth': 8, 'random_strength': 8, 'l2_leaf_reg': 2.371929242320879e-05, 'min_child_samples': 30, 'max_bin': 277}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:21:10,522] Trial 81 finished with value: 0.9899868118985982 and parameters: {'learning_rate': 0.03313520938952858, 'bagging_temperature': 1.1120432250623584, 'n_estimators': 1891, 'max_depth': 8, 'random_strength': 4, 'l2_leaf_reg': 1.7018063036909292e-05, 'min_child_samples': 42, 'max_bin': 285}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "Training has stopped (degenerate solution on iteration 356, probably too small l2-regularization, try to increase it)\n",
            "[I 2024-02-20 19:21:31,557] Trial 82 finished with value: 0.9782301660741246 and parameters: {'learning_rate': 0.03453755399177981, 'bagging_temperature': 1.0406055952852034, 'n_estimators': 1909, 'max_depth': 8, 'random_strength': 3, 'l2_leaf_reg': 1.81861904777006e-05, 'min_child_samples': 40, 'max_bin': 284}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:22:51,574] Trial 83 finished with value: 0.9894942584901051 and parameters: {'learning_rate': 0.032982741652944764, 'bagging_temperature': 0.5162034543126166, 'n_estimators': 1615, 'max_depth': 8, 'random_strength': 6, 'l2_leaf_reg': 1.6904406040943828e-05, 'min_child_samples': 35, 'max_bin': 150}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:23:50,174] Trial 84 finished with value: 0.9776539044545499 and parameters: {'learning_rate': 0.02674525778847163, 'bagging_temperature': 1.7997295028089069, 'n_estimators': 2117, 'max_depth': 4, 'random_strength': 29, 'l2_leaf_reg': 1.5921208781507083e-05, 'min_child_samples': 45, 'max_bin': 280}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:26:21,356] Trial 85 finished with value: 0.9896433805569126 and parameters: {'learning_rate': 0.02367369118173397, 'bagging_temperature': 0.40161559309716754, 'n_estimators': 2234, 'max_depth': 8, 'random_strength': 15, 'l2_leaf_reg': 1.7482357651986487e-05, 'min_child_samples': 42, 'max_bin': 296}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:27:38,795] Trial 86 finished with value: 0.989057156814851 and parameters: {'learning_rate': 0.03750962026780509, 'bagging_temperature': 6.5710209237897255, 'n_estimators': 1873, 'max_depth': 8, 'random_strength': 10, 'l2_leaf_reg': 1.9431910679020127e-05, 'min_child_samples': 48, 'max_bin': 272}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:28:51,009] Trial 87 finished with value: 0.9900825638770824 and parameters: {'learning_rate': 0.042604354124270126, 'bagging_temperature': 1.179665725482469, 'n_estimators': 1974, 'max_depth': 7, 'random_strength': 4, 'l2_leaf_reg': 1.5227725729954513e-05, 'min_child_samples': 39, 'max_bin': 287}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:30:08,724] Trial 88 finished with value: 0.9895919863181041 and parameters: {'learning_rate': 0.04258875528974397, 'bagging_temperature': 0.6662983418402321, 'n_estimators': 1977, 'max_depth': 7, 'random_strength': 4, 'l2_leaf_reg': 1.4980421525827204e-05, 'min_child_samples': 46, 'max_bin': 276}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:31:19,140] Trial 89 finished with value: 0.9897814501540116 and parameters: {'learning_rate': 0.049621928676456435, 'bagging_temperature': 2.60230833082252, 'n_estimators': 1852, 'max_depth': 7, 'random_strength': 1, 'l2_leaf_reg': 1.6294333272842747e-05, 'min_child_samples': 50, 'max_bin': 286}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:32:27,768] Trial 90 finished with value: 0.9900371166243407 and parameters: {'learning_rate': 0.046492217409665276, 'bagging_temperature': 1.165480742607996, 'n_estimators': 2017, 'max_depth': 8, 'random_strength': 10, 'l2_leaf_reg': 1.2362322335323019e-05, 'min_child_samples': 41, 'max_bin': 281}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:33:35,096] Trial 91 finished with value: 0.9901783532860983 and parameters: {'learning_rate': 0.04499625957010431, 'bagging_temperature': 1.1727173578223082, 'n_estimators': 2023, 'max_depth': 8, 'random_strength': 10, 'l2_leaf_reg': 1.3059348432208773e-05, 'min_child_samples': 41, 'max_bin': 287}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:34:36,767] Trial 92 finished with value: 0.9896957562142892 and parameters: {'learning_rate': 0.04635229407390474, 'bagging_temperature': 0.8597613702472289, 'n_estimators': 2118, 'max_depth': 8, 'random_strength': 12, 'l2_leaf_reg': 1.18531052140031e-05, 'min_child_samples': 40, 'max_bin': 279}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:35:45,678] Trial 93 finished with value: 0.9897874419741022 and parameters: {'learning_rate': 0.044292139974099316, 'bagging_temperature': 1.284571417586921, 'n_estimators': 2015, 'max_depth': 8, 'random_strength': 9, 'l2_leaf_reg': 9.992041881689935e-06, 'min_child_samples': 43, 'max_bin': 288}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:36:47,569] Trial 94 finished with value: 0.9898851698021012 and parameters: {'learning_rate': 0.041260764879910226, 'bagging_temperature': 1.7964376215504512, 'n_estimators': 1782, 'max_depth': 8, 'random_strength': 10, 'l2_leaf_reg': 1.2471818754896294e-05, 'min_child_samples': 41, 'max_bin': 297}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:37:59,939] Trial 95 finished with value: 0.9902276947131828 and parameters: {'learning_rate': 0.048788755683043854, 'bagging_temperature': 0.9329801243607697, 'n_estimators': 2070, 'max_depth': 8, 'random_strength': 12, 'l2_leaf_reg': 1.3192160338252454e-05, 'min_child_samples': 32, 'max_bin': 268}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:38:17,950] Trial 96 finished with value: 0.9741661376178151 and parameters: {'learning_rate': 0.05100957746549299, 'bagging_temperature': 0.3786502003691201, 'n_estimators': 1713, 'max_depth': 7, 'random_strength': 16, 'l2_leaf_reg': 1.319768070312804e-05, 'min_child_samples': 31, 'max_bin': 267}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:39:16,840] Trial 97 finished with value: 0.9901716297491565 and parameters: {'learning_rate': 0.05785939821077359, 'bagging_temperature': 0.25392130899572857, 'n_estimators': 2307, 'max_depth': 8, 'random_strength': 13, 'l2_leaf_reg': 8.466170747665852e-06, 'min_child_samples': 32, 'max_bin': 270}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:40:06,324] Trial 98 finished with value: 0.9896897141461031 and parameters: {'learning_rate': 0.06997879235147968, 'bagging_temperature': 0.24251562500959525, 'n_estimators': 2301, 'max_depth': 8, 'random_strength': 14, 'l2_leaf_reg': 1.0918955242684431e-05, 'min_child_samples': 32, 'max_bin': 250}. Best is trial 35 with value: 0.9907144951617632.\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "C:\\Users\\gahyunlee\\AppData\\Local\\Temp\\ipykernel_5436\\3964961730.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
            "[I 2024-02-20 19:41:05,021] Trial 99 finished with value: 0.9891453158615294 and parameters: {'learning_rate': 0.05570560496801221, 'bagging_temperature': 0.07874055065355928, 'n_estimators': 2201, 'max_depth': 8, 'random_strength': 20, 'l2_leaf_reg': 8.154160549681643e-06, 'min_child_samples': 28, 'max_bin': 245}. Best is trial 35 with value: 0.9907144951617632.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: {'learning_rate': 0.040075823012502135, 'bagging_temperature': 1.388074362792074, 'n_estimators': 2679, 'max_depth': 8, 'random_strength': 43, 'l2_leaf_reg': 1.814211208460745e-05, 'min_child_samples': 44, 'max_bin': 288}\n"
          ]
        }
      ],
      "source": [
        "# Optuna 최적화\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective_catboost, n_trials=100)\n",
        "\n",
        "# 최적의 파라미터와 그때의 정확도 출력\n",
        "print(f\"Best trial: {study.best_trial.params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6634309\ttotal: 106ms\tremaining: 4m 44s\n",
            "1:\tlearn: 0.6422457\ttotal: 189ms\tremaining: 4m 12s\n",
            "2:\tlearn: 0.6212268\ttotal: 405ms\tremaining: 6m\n",
            "3:\tlearn: 0.6038759\ttotal: 587ms\tremaining: 6m 32s\n",
            "4:\tlearn: 0.5801994\ttotal: 703ms\tremaining: 6m 16s\n",
            "5:\tlearn: 0.5519085\ttotal: 803ms\tremaining: 5m 57s\n",
            "6:\tlearn: 0.5408572\ttotal: 881ms\tremaining: 5m 36s\n",
            "7:\tlearn: 0.5142132\ttotal: 996ms\tremaining: 5m 32s\n",
            "8:\tlearn: 0.5013682\ttotal: 1.07s\tremaining: 5m 18s\n",
            "9:\tlearn: 0.4897126\ttotal: 1.14s\tremaining: 5m 3s\n",
            "10:\tlearn: 0.4807653\ttotal: 1.2s\tremaining: 4m 50s\n",
            "11:\tlearn: 0.4703158\ttotal: 1.29s\tremaining: 4m 48s\n",
            "12:\tlearn: 0.4614261\ttotal: 1.46s\tremaining: 4m 59s\n",
            "13:\tlearn: 0.4446748\ttotal: 1.59s\tremaining: 5m 2s\n",
            "14:\tlearn: 0.4323109\ttotal: 1.68s\tremaining: 4m 59s\n",
            "15:\tlearn: 0.4230853\ttotal: 1.76s\tremaining: 4m 53s\n",
            "16:\tlearn: 0.4149646\ttotal: 1.86s\tremaining: 4m 50s\n",
            "17:\tlearn: 0.4048475\ttotal: 1.98s\tremaining: 4m 52s\n",
            "18:\tlearn: 0.4012575\ttotal: 2.11s\tremaining: 4m 54s\n",
            "19:\tlearn: 0.3977366\ttotal: 2.2s\tremaining: 4m 52s\n",
            "20:\tlearn: 0.3910213\ttotal: 2.27s\tremaining: 4m 47s\n",
            "21:\tlearn: 0.3864124\ttotal: 2.33s\tremaining: 4m 40s\n",
            "22:\tlearn: 0.3807066\ttotal: 2.4s\tremaining: 4m 36s\n",
            "23:\tlearn: 0.3758910\ttotal: 2.47s\tremaining: 4m 33s\n",
            "24:\tlearn: 0.3725893\ttotal: 2.55s\tremaining: 4m 30s\n",
            "25:\tlearn: 0.3692330\ttotal: 2.63s\tremaining: 4m 27s\n",
            "26:\tlearn: 0.3641128\ttotal: 2.71s\tremaining: 4m 26s\n",
            "27:\tlearn: 0.3603735\ttotal: 2.77s\tremaining: 4m 22s\n",
            "28:\tlearn: 0.3552343\ttotal: 2.86s\tremaining: 4m 21s\n",
            "29:\tlearn: 0.3496169\ttotal: 2.92s\tremaining: 4m 18s\n",
            "30:\tlearn: 0.3481045\ttotal: 2.98s\tremaining: 4m 14s\n",
            "31:\tlearn: 0.3437859\ttotal: 3.06s\tremaining: 4m 13s\n",
            "32:\tlearn: 0.3398388\ttotal: 3.13s\tremaining: 4m 11s\n",
            "33:\tlearn: 0.3376891\ttotal: 3.21s\tremaining: 4m 9s\n",
            "34:\tlearn: 0.3360414\ttotal: 3.28s\tremaining: 4m 7s\n",
            "35:\tlearn: 0.3308539\ttotal: 3.35s\tremaining: 4m 6s\n",
            "36:\tlearn: 0.3282147\ttotal: 3.41s\tremaining: 4m 3s\n",
            "37:\tlearn: 0.3265344\ttotal: 3.51s\tremaining: 4m 3s\n",
            "38:\tlearn: 0.3222017\ttotal: 3.58s\tremaining: 4m 2s\n",
            "39:\tlearn: 0.3203332\ttotal: 3.66s\tremaining: 4m 1s\n",
            "40:\tlearn: 0.3183536\ttotal: 3.75s\tremaining: 4m 1s\n",
            "41:\tlearn: 0.3162624\ttotal: 3.83s\tremaining: 4m\n",
            "42:\tlearn: 0.3134805\ttotal: 3.9s\tremaining: 3m 59s\n",
            "43:\tlearn: 0.3111393\ttotal: 3.99s\tremaining: 3m 58s\n",
            "44:\tlearn: 0.3101368\ttotal: 4.07s\tremaining: 3m 58s\n",
            "45:\tlearn: 0.3087952\ttotal: 4.13s\tremaining: 3m 56s\n",
            "46:\tlearn: 0.3079035\ttotal: 4.2s\tremaining: 3m 54s\n",
            "47:\tlearn: 0.3062173\ttotal: 4.27s\tremaining: 3m 54s\n",
            "48:\tlearn: 0.3054009\ttotal: 4.31s\tremaining: 3m 51s\n",
            "49:\tlearn: 0.3018928\ttotal: 4.39s\tremaining: 3m 50s\n",
            "50:\tlearn: 0.3010262\ttotal: 4.44s\tremaining: 3m 48s\n",
            "51:\tlearn: 0.2986810\ttotal: 4.52s\tremaining: 3m 48s\n",
            "52:\tlearn: 0.2981509\ttotal: 4.57s\tremaining: 3m 46s\n",
            "53:\tlearn: 0.2973630\ttotal: 4.63s\tremaining: 3m 45s\n",
            "54:\tlearn: 0.2960486\ttotal: 4.71s\tremaining: 3m 44s\n",
            "55:\tlearn: 0.2947789\ttotal: 4.78s\tremaining: 3m 44s\n",
            "56:\tlearn: 0.2933326\ttotal: 4.85s\tremaining: 3m 43s\n",
            "57:\tlearn: 0.2920999\ttotal: 4.92s\tremaining: 3m 42s\n",
            "58:\tlearn: 0.2900755\ttotal: 4.99s\tremaining: 3m 41s\n",
            "59:\tlearn: 0.2856672\ttotal: 5.06s\tremaining: 3m 40s\n",
            "60:\tlearn: 0.2841431\ttotal: 5.12s\tremaining: 3m 39s\n",
            "61:\tlearn: 0.2831109\ttotal: 5.18s\tremaining: 3m 38s\n",
            "62:\tlearn: 0.2796028\ttotal: 5.27s\tremaining: 3m 38s\n",
            "63:\tlearn: 0.2785444\ttotal: 5.34s\tremaining: 3m 38s\n",
            "64:\tlearn: 0.2774857\ttotal: 5.39s\tremaining: 3m 36s\n",
            "65:\tlearn: 0.2769711\ttotal: 5.45s\tremaining: 3m 35s\n",
            "66:\tlearn: 0.2763793\ttotal: 5.51s\tremaining: 3m 35s\n",
            "67:\tlearn: 0.2750740\ttotal: 5.64s\tremaining: 3m 36s\n",
            "68:\tlearn: 0.2742347\ttotal: 5.77s\tremaining: 3m 38s\n",
            "69:\tlearn: 0.2692867\ttotal: 5.96s\tremaining: 3m 42s\n",
            "70:\tlearn: 0.2674348\ttotal: 6.31s\tremaining: 3m 51s\n",
            "71:\tlearn: 0.2670087\ttotal: 6.5s\tremaining: 3m 55s\n",
            "72:\tlearn: 0.2663497\ttotal: 6.66s\tremaining: 3m 57s\n",
            "73:\tlearn: 0.2639773\ttotal: 6.73s\tremaining: 3m 56s\n",
            "74:\tlearn: 0.2635507\ttotal: 6.8s\tremaining: 3m 56s\n",
            "75:\tlearn: 0.2628319\ttotal: 6.87s\tremaining: 3m 55s\n",
            "76:\tlearn: 0.2623704\ttotal: 6.97s\tremaining: 3m 55s\n",
            "77:\tlearn: 0.2615086\ttotal: 7.03s\tremaining: 3m 54s\n",
            "78:\tlearn: 0.2606966\ttotal: 7.09s\tremaining: 3m 53s\n",
            "79:\tlearn: 0.2594447\ttotal: 7.2s\tremaining: 3m 53s\n",
            "80:\tlearn: 0.2583777\ttotal: 7.27s\tremaining: 3m 53s\n",
            "81:\tlearn: 0.2579167\ttotal: 7.35s\tremaining: 3m 52s\n",
            "82:\tlearn: 0.2567778\ttotal: 7.42s\tremaining: 3m 52s\n",
            "83:\tlearn: 0.2543917\ttotal: 7.48s\tremaining: 3m 51s\n",
            "84:\tlearn: 0.2537250\ttotal: 7.55s\tremaining: 3m 50s\n",
            "85:\tlearn: 0.2532743\ttotal: 7.63s\tremaining: 3m 50s\n",
            "86:\tlearn: 0.2478566\ttotal: 7.7s\tremaining: 3m 49s\n",
            "87:\tlearn: 0.2475136\ttotal: 7.75s\tremaining: 3m 48s\n",
            "88:\tlearn: 0.2469449\ttotal: 7.81s\tremaining: 3m 47s\n",
            "89:\tlearn: 0.2465046\ttotal: 7.87s\tremaining: 3m 46s\n",
            "90:\tlearn: 0.2445561\ttotal: 8.02s\tremaining: 3m 47s\n",
            "91:\tlearn: 0.2443197\ttotal: 8.14s\tremaining: 3m 48s\n",
            "92:\tlearn: 0.2431864\ttotal: 8.29s\tremaining: 3m 50s\n",
            "93:\tlearn: 0.2429127\ttotal: 8.37s\tremaining: 3m 50s\n",
            "94:\tlearn: 0.2421428\ttotal: 8.46s\tremaining: 3m 49s\n",
            "95:\tlearn: 0.2404715\ttotal: 8.51s\tremaining: 3m 48s\n",
            "96:\tlearn: 0.2403435\ttotal: 8.54s\tremaining: 3m 47s\n",
            "97:\tlearn: 0.2399154\ttotal: 8.6s\tremaining: 3m 46s\n",
            "98:\tlearn: 0.2395267\ttotal: 8.68s\tremaining: 3m 46s\n",
            "99:\tlearn: 0.2390349\ttotal: 8.74s\tremaining: 3m 45s\n",
            "100:\tlearn: 0.2374008\ttotal: 8.8s\tremaining: 3m 44s\n",
            "101:\tlearn: 0.2369759\ttotal: 8.87s\tremaining: 3m 44s\n",
            "102:\tlearn: 0.2366783\ttotal: 8.95s\tremaining: 3m 43s\n",
            "103:\tlearn: 0.2348902\ttotal: 9s\tremaining: 3m 42s\n",
            "104:\tlearn: 0.2340227\ttotal: 9.07s\tremaining: 3m 42s\n",
            "105:\tlearn: 0.2332227\ttotal: 9.15s\tremaining: 3m 42s\n",
            "106:\tlearn: 0.2299142\ttotal: 9.22s\tremaining: 3m 41s\n",
            "107:\tlearn: 0.2293360\ttotal: 9.31s\tremaining: 3m 41s\n",
            "108:\tlearn: 0.2279715\ttotal: 9.42s\tremaining: 3m 42s\n",
            "109:\tlearn: 0.2269338\ttotal: 9.56s\tremaining: 3m 43s\n",
            "110:\tlearn: 0.2262333\ttotal: 9.66s\tremaining: 3m 43s\n",
            "111:\tlearn: 0.2259513\ttotal: 9.77s\tremaining: 3m 43s\n",
            "112:\tlearn: 0.2257998\ttotal: 9.96s\tremaining: 3m 46s\n",
            "113:\tlearn: 0.2255302\ttotal: 10.1s\tremaining: 3m 46s\n",
            "114:\tlearn: 0.2252172\ttotal: 10.2s\tremaining: 3m 47s\n",
            "115:\tlearn: 0.2248374\ttotal: 10.5s\tremaining: 3m 51s\n",
            "116:\tlearn: 0.2224460\ttotal: 10.6s\tremaining: 3m 52s\n",
            "117:\tlearn: 0.2213223\ttotal: 10.8s\tremaining: 3m 53s\n",
            "118:\tlearn: 0.2204319\ttotal: 10.9s\tremaining: 3m 53s\n",
            "119:\tlearn: 0.2202502\ttotal: 10.9s\tremaining: 3m 52s\n",
            "120:\tlearn: 0.2199093\ttotal: 11s\tremaining: 3m 51s\n",
            "121:\tlearn: 0.2192755\ttotal: 11.1s\tremaining: 3m 52s\n",
            "122:\tlearn: 0.2189725\ttotal: 11.2s\tremaining: 3m 51s\n",
            "123:\tlearn: 0.2186635\ttotal: 11.2s\tremaining: 3m 51s\n",
            "124:\tlearn: 0.2180589\ttotal: 11.4s\tremaining: 3m 52s\n",
            "125:\tlearn: 0.2177072\ttotal: 11.5s\tremaining: 3m 52s\n",
            "126:\tlearn: 0.2171578\ttotal: 11.6s\tremaining: 3m 52s\n",
            "127:\tlearn: 0.2170048\ttotal: 11.7s\tremaining: 3m 52s\n",
            "128:\tlearn: 0.2165948\ttotal: 11.8s\tremaining: 3m 53s\n",
            "129:\tlearn: 0.2157493\ttotal: 11.9s\tremaining: 3m 52s\n",
            "130:\tlearn: 0.2154660\ttotal: 12s\tremaining: 3m 54s\n",
            "131:\tlearn: 0.2153231\ttotal: 12.6s\tremaining: 4m 3s\n",
            "132:\tlearn: 0.2150355\ttotal: 12.8s\tremaining: 4m 5s\n",
            "133:\tlearn: 0.2146258\ttotal: 13.1s\tremaining: 4m 9s\n",
            "134:\tlearn: 0.2142758\ttotal: 13.2s\tremaining: 4m 9s\n",
            "135:\tlearn: 0.2129387\ttotal: 13.4s\tremaining: 4m 10s\n",
            "136:\tlearn: 0.2123941\ttotal: 13.5s\tremaining: 4m 10s\n",
            "137:\tlearn: 0.2118125\ttotal: 13.6s\tremaining: 4m 10s\n",
            "138:\tlearn: 0.2114836\ttotal: 13.7s\tremaining: 4m 9s\n",
            "139:\tlearn: 0.2103014\ttotal: 13.7s\tremaining: 4m 9s\n",
            "140:\tlearn: 0.2098506\ttotal: 13.8s\tremaining: 4m 8s\n",
            "141:\tlearn: 0.2092747\ttotal: 13.9s\tremaining: 4m 9s\n",
            "142:\tlearn: 0.2075088\ttotal: 14.1s\tremaining: 4m 9s\n",
            "143:\tlearn: 0.2072619\ttotal: 14.2s\tremaining: 4m 9s\n",
            "144:\tlearn: 0.2068265\ttotal: 14.3s\tremaining: 4m 9s\n",
            "145:\tlearn: 0.2066092\ttotal: 14.4s\tremaining: 4m 9s\n",
            "146:\tlearn: 0.2063894\ttotal: 14.4s\tremaining: 4m 8s\n",
            "147:\tlearn: 0.2058676\ttotal: 14.5s\tremaining: 4m 7s\n",
            "148:\tlearn: 0.2054605\ttotal: 14.5s\tremaining: 4m 6s\n",
            "149:\tlearn: 0.2053986\ttotal: 14.6s\tremaining: 4m 6s\n",
            "150:\tlearn: 0.2049429\ttotal: 14.7s\tremaining: 4m 5s\n",
            "151:\tlearn: 0.2047728\ttotal: 14.7s\tremaining: 4m 4s\n",
            "152:\tlearn: 0.2045396\ttotal: 14.8s\tremaining: 4m 3s\n",
            "153:\tlearn: 0.2025358\ttotal: 14.8s\tremaining: 4m 3s\n",
            "154:\tlearn: 0.2023555\ttotal: 14.9s\tremaining: 4m 2s\n",
            "155:\tlearn: 0.2022357\ttotal: 15s\tremaining: 4m 2s\n",
            "156:\tlearn: 0.2020454\ttotal: 15.1s\tremaining: 4m 2s\n",
            "157:\tlearn: 0.2015627\ttotal: 15.2s\tremaining: 4m 2s\n",
            "158:\tlearn: 0.2010632\ttotal: 15.3s\tremaining: 4m 2s\n",
            "159:\tlearn: 0.2008491\ttotal: 15.4s\tremaining: 4m 2s\n",
            "160:\tlearn: 0.2004039\ttotal: 15.5s\tremaining: 4m 1s\n",
            "161:\tlearn: 0.2002680\ttotal: 15.6s\tremaining: 4m 1s\n",
            "162:\tlearn: 0.2000296\ttotal: 15.7s\tremaining: 4m 1s\n",
            "163:\tlearn: 0.1991630\ttotal: 15.8s\tremaining: 4m 2s\n",
            "164:\tlearn: 0.1989602\ttotal: 16.1s\tremaining: 4m 4s\n",
            "165:\tlearn: 0.1985571\ttotal: 16.2s\tremaining: 4m 5s\n",
            "166:\tlearn: 0.1980033\ttotal: 16.4s\tremaining: 4m 6s\n",
            "167:\tlearn: 0.1978935\ttotal: 16.4s\tremaining: 4m 5s\n",
            "168:\tlearn: 0.1975826\ttotal: 16.9s\tremaining: 4m 11s\n",
            "169:\tlearn: 0.1973295\ttotal: 17.5s\tremaining: 4m 18s\n",
            "170:\tlearn: 0.1967097\ttotal: 17.9s\tremaining: 4m 23s\n",
            "171:\tlearn: 0.1962255\ttotal: 18s\tremaining: 4m 22s\n",
            "172:\tlearn: 0.1955155\ttotal: 18.1s\tremaining: 4m 22s\n",
            "173:\tlearn: 0.1952288\ttotal: 18.2s\tremaining: 4m 22s\n",
            "174:\tlearn: 0.1950403\ttotal: 18.4s\tremaining: 4m 22s\n",
            "175:\tlearn: 0.1948865\ttotal: 18.5s\tremaining: 4m 23s\n",
            "176:\tlearn: 0.1946926\ttotal: 18.6s\tremaining: 4m 23s\n",
            "177:\tlearn: 0.1943940\ttotal: 18.8s\tremaining: 4m 24s\n",
            "178:\tlearn: 0.1941397\ttotal: 19s\tremaining: 4m 25s\n",
            "179:\tlearn: 0.1939574\ttotal: 19.2s\tremaining: 4m 26s\n",
            "180:\tlearn: 0.1939231\ttotal: 19.3s\tremaining: 4m 25s\n",
            "181:\tlearn: 0.1938178\ttotal: 19.4s\tremaining: 4m 25s\n",
            "182:\tlearn: 0.1933409\ttotal: 19.5s\tremaining: 4m 26s\n",
            "183:\tlearn: 0.1931589\ttotal: 19.6s\tremaining: 4m 25s\n",
            "184:\tlearn: 0.1920727\ttotal: 19.7s\tremaining: 4m 26s\n",
            "185:\tlearn: 0.1917209\ttotal: 19.8s\tremaining: 4m 25s\n",
            "186:\tlearn: 0.1912946\ttotal: 19.9s\tremaining: 4m 25s\n",
            "187:\tlearn: 0.1910724\ttotal: 20s\tremaining: 4m 24s\n",
            "188:\tlearn: 0.1907943\ttotal: 20.1s\tremaining: 4m 24s\n",
            "189:\tlearn: 0.1890283\ttotal: 20.2s\tremaining: 4m 23s\n",
            "190:\tlearn: 0.1883592\ttotal: 20.2s\tremaining: 4m 23s\n",
            "191:\tlearn: 0.1882435\ttotal: 20.3s\tremaining: 4m 23s\n",
            "192:\tlearn: 0.1880068\ttotal: 20.4s\tremaining: 4m 22s\n",
            "193:\tlearn: 0.1873232\ttotal: 20.4s\tremaining: 4m 21s\n",
            "194:\tlearn: 0.1868779\ttotal: 20.5s\tremaining: 4m 21s\n",
            "195:\tlearn: 0.1865197\ttotal: 20.6s\tremaining: 4m 20s\n",
            "196:\tlearn: 0.1862844\ttotal: 20.6s\tremaining: 4m 20s\n",
            "197:\tlearn: 0.1862576\ttotal: 20.7s\tremaining: 4m 19s\n",
            "198:\tlearn: 0.1860016\ttotal: 20.8s\tremaining: 4m 18s\n",
            "199:\tlearn: 0.1856716\ttotal: 21s\tremaining: 4m 20s\n",
            "200:\tlearn: 0.1855105\ttotal: 21.1s\tremaining: 4m 20s\n",
            "201:\tlearn: 0.1848662\ttotal: 21.3s\tremaining: 4m 21s\n",
            "202:\tlearn: 0.1845483\ttotal: 21.5s\tremaining: 4m 22s\n",
            "203:\tlearn: 0.1841513\ttotal: 21.7s\tremaining: 4m 23s\n",
            "204:\tlearn: 0.1837457\ttotal: 21.8s\tremaining: 4m 22s\n",
            "205:\tlearn: 0.1834057\ttotal: 21.9s\tremaining: 4m 22s\n",
            "206:\tlearn: 0.1832043\ttotal: 22s\tremaining: 4m 22s\n",
            "207:\tlearn: 0.1825299\ttotal: 22s\tremaining: 4m 21s\n",
            "208:\tlearn: 0.1824409\ttotal: 22.1s\tremaining: 4m 21s\n",
            "209:\tlearn: 0.1822949\ttotal: 22.2s\tremaining: 4m 20s\n",
            "210:\tlearn: 0.1821060\ttotal: 22.2s\tremaining: 4m 19s\n",
            "211:\tlearn: 0.1810927\ttotal: 22.3s\tremaining: 4m 19s\n",
            "212:\tlearn: 0.1809407\ttotal: 22.4s\tremaining: 4m 18s\n",
            "213:\tlearn: 0.1807939\ttotal: 22.4s\tremaining: 4m 18s\n",
            "214:\tlearn: 0.1805594\ttotal: 22.5s\tremaining: 4m 17s\n",
            "215:\tlearn: 0.1803005\ttotal: 22.5s\tremaining: 4m 17s\n",
            "216:\tlearn: 0.1802254\ttotal: 22.6s\tremaining: 4m 16s\n",
            "217:\tlearn: 0.1796947\ttotal: 22.7s\tremaining: 4m 15s\n",
            "218:\tlearn: 0.1796476\ttotal: 22.8s\tremaining: 4m 15s\n",
            "219:\tlearn: 0.1793316\ttotal: 22.8s\tremaining: 4m 15s\n",
            "220:\tlearn: 0.1783145\ttotal: 22.9s\tremaining: 4m 14s\n",
            "221:\tlearn: 0.1780139\ttotal: 22.9s\tremaining: 4m 13s\n",
            "222:\tlearn: 0.1769253\ttotal: 23s\tremaining: 4m 13s\n",
            "223:\tlearn: 0.1766869\ttotal: 23.1s\tremaining: 4m 13s\n",
            "224:\tlearn: 0.1764560\ttotal: 23.2s\tremaining: 4m 12s\n",
            "225:\tlearn: 0.1761986\ttotal: 23.2s\tremaining: 4m 11s\n",
            "226:\tlearn: 0.1760968\ttotal: 23.3s\tremaining: 4m 11s\n",
            "227:\tlearn: 0.1760005\ttotal: 23.4s\tremaining: 4m 11s\n",
            "228:\tlearn: 0.1757849\ttotal: 23.4s\tremaining: 4m 10s\n",
            "229:\tlearn: 0.1756658\ttotal: 23.5s\tremaining: 4m 9s\n",
            "230:\tlearn: 0.1755511\ttotal: 23.5s\tremaining: 4m 9s\n",
            "231:\tlearn: 0.1753445\ttotal: 23.6s\tremaining: 4m 9s\n",
            "232:\tlearn: 0.1752022\ttotal: 23.7s\tremaining: 4m 8s\n",
            "233:\tlearn: 0.1749674\ttotal: 23.7s\tremaining: 4m 8s\n",
            "234:\tlearn: 0.1747382\ttotal: 23.8s\tremaining: 4m 7s\n",
            "235:\tlearn: 0.1745284\ttotal: 23.9s\tremaining: 4m 7s\n",
            "236:\tlearn: 0.1738675\ttotal: 23.9s\tremaining: 4m 6s\n",
            "237:\tlearn: 0.1733798\ttotal: 24.1s\tremaining: 4m 6s\n",
            "238:\tlearn: 0.1723615\ttotal: 24.1s\tremaining: 4m 6s\n",
            "239:\tlearn: 0.1721902\ttotal: 24.2s\tremaining: 4m 6s\n",
            "240:\tlearn: 0.1721144\ttotal: 24.3s\tremaining: 4m 5s\n",
            "241:\tlearn: 0.1720487\ttotal: 24.3s\tremaining: 4m 5s\n",
            "242:\tlearn: 0.1719266\ttotal: 24.4s\tremaining: 4m 4s\n",
            "243:\tlearn: 0.1715976\ttotal: 24.5s\tremaining: 4m 4s\n",
            "244:\tlearn: 0.1712538\ttotal: 24.6s\tremaining: 4m 4s\n",
            "245:\tlearn: 0.1709858\ttotal: 24.6s\tremaining: 4m 3s\n",
            "246:\tlearn: 0.1709017\ttotal: 24.7s\tremaining: 4m 3s\n",
            "247:\tlearn: 0.1707522\ttotal: 24.8s\tremaining: 4m 2s\n",
            "248:\tlearn: 0.1705504\ttotal: 24.9s\tremaining: 4m 2s\n",
            "249:\tlearn: 0.1697210\ttotal: 24.9s\tremaining: 4m 2s\n",
            "250:\tlearn: 0.1695340\ttotal: 25s\tremaining: 4m 1s\n",
            "251:\tlearn: 0.1694641\ttotal: 25.1s\tremaining: 4m 1s\n",
            "252:\tlearn: 0.1689151\ttotal: 25.2s\tremaining: 4m 1s\n",
            "253:\tlearn: 0.1686104\ttotal: 25.2s\tremaining: 4m\n",
            "254:\tlearn: 0.1684899\ttotal: 25.3s\tremaining: 4m\n",
            "255:\tlearn: 0.1684541\ttotal: 25.4s\tremaining: 4m\n",
            "256:\tlearn: 0.1681846\ttotal: 25.4s\tremaining: 3m 59s\n",
            "257:\tlearn: 0.1680526\ttotal: 25.5s\tremaining: 3m 59s\n",
            "258:\tlearn: 0.1676035\ttotal: 25.6s\tremaining: 3m 58s\n",
            "259:\tlearn: 0.1674033\ttotal: 25.6s\tremaining: 3m 58s\n",
            "260:\tlearn: 0.1672802\ttotal: 25.7s\tremaining: 3m 57s\n",
            "261:\tlearn: 0.1667789\ttotal: 25.8s\tremaining: 3m 57s\n",
            "262:\tlearn: 0.1665523\ttotal: 25.8s\tremaining: 3m 57s\n",
            "263:\tlearn: 0.1664615\ttotal: 25.9s\tremaining: 3m 56s\n",
            "264:\tlearn: 0.1663726\ttotal: 26s\tremaining: 3m 56s\n",
            "265:\tlearn: 0.1657532\ttotal: 26s\tremaining: 3m 56s\n",
            "266:\tlearn: 0.1656833\ttotal: 26.1s\tremaining: 3m 55s\n",
            "267:\tlearn: 0.1645413\ttotal: 26.1s\tremaining: 3m 55s\n",
            "268:\tlearn: 0.1644313\ttotal: 26.2s\tremaining: 3m 54s\n",
            "269:\tlearn: 0.1640524\ttotal: 26.3s\tremaining: 3m 54s\n",
            "270:\tlearn: 0.1639817\ttotal: 26.3s\tremaining: 3m 54s\n",
            "271:\tlearn: 0.1638701\ttotal: 26.4s\tremaining: 3m 53s\n",
            "272:\tlearn: 0.1634262\ttotal: 26.5s\tremaining: 3m 53s\n",
            "273:\tlearn: 0.1631503\ttotal: 26.5s\tremaining: 3m 52s\n",
            "274:\tlearn: 0.1627774\ttotal: 26.6s\tremaining: 3m 52s\n",
            "275:\tlearn: 0.1622330\ttotal: 26.7s\tremaining: 3m 52s\n",
            "276:\tlearn: 0.1620293\ttotal: 26.7s\tremaining: 3m 51s\n",
            "277:\tlearn: 0.1615648\ttotal: 26.8s\tremaining: 3m 51s\n",
            "278:\tlearn: 0.1614559\ttotal: 26.9s\tremaining: 3m 51s\n",
            "279:\tlearn: 0.1613528\ttotal: 26.9s\tremaining: 3m 50s\n",
            "280:\tlearn: 0.1607621\ttotal: 27s\tremaining: 3m 50s\n",
            "281:\tlearn: 0.1607070\ttotal: 27.1s\tremaining: 3m 50s\n",
            "282:\tlearn: 0.1605289\ttotal: 27.1s\tremaining: 3m 49s\n",
            "283:\tlearn: 0.1591855\ttotal: 27.2s\tremaining: 3m 49s\n",
            "284:\tlearn: 0.1583788\ttotal: 27.3s\tremaining: 3m 49s\n",
            "285:\tlearn: 0.1582708\ttotal: 27.4s\tremaining: 3m 49s\n",
            "286:\tlearn: 0.1580069\ttotal: 27.5s\tremaining: 3m 48s\n",
            "287:\tlearn: 0.1578894\ttotal: 27.5s\tremaining: 3m 48s\n",
            "288:\tlearn: 0.1565752\ttotal: 27.6s\tremaining: 3m 48s\n",
            "289:\tlearn: 0.1564940\ttotal: 27.7s\tremaining: 3m 48s\n",
            "290:\tlearn: 0.1562873\ttotal: 27.7s\tremaining: 3m 47s\n",
            "291:\tlearn: 0.1554666\ttotal: 27.8s\tremaining: 3m 47s\n",
            "292:\tlearn: 0.1551886\ttotal: 27.9s\tremaining: 3m 47s\n",
            "293:\tlearn: 0.1549125\ttotal: 28s\tremaining: 3m 46s\n",
            "294:\tlearn: 0.1532708\ttotal: 28s\tremaining: 3m 46s\n",
            "295:\tlearn: 0.1530321\ttotal: 28.1s\tremaining: 3m 45s\n",
            "296:\tlearn: 0.1521704\ttotal: 28.2s\tremaining: 3m 45s\n",
            "297:\tlearn: 0.1520858\ttotal: 28.3s\tremaining: 3m 45s\n",
            "298:\tlearn: 0.1518571\ttotal: 28.4s\tremaining: 3m 45s\n",
            "299:\tlearn: 0.1515027\ttotal: 28.5s\tremaining: 3m 45s\n",
            "300:\tlearn: 0.1507960\ttotal: 28.6s\tremaining: 3m 45s\n",
            "301:\tlearn: 0.1498925\ttotal: 28.6s\tremaining: 3m 45s\n",
            "302:\tlearn: 0.1497165\ttotal: 28.8s\tremaining: 3m 45s\n",
            "303:\tlearn: 0.1494331\ttotal: 28.8s\tremaining: 3m 45s\n",
            "304:\tlearn: 0.1493758\ttotal: 28.9s\tremaining: 3m 44s\n",
            "305:\tlearn: 0.1492020\ttotal: 29s\tremaining: 3m 44s\n",
            "306:\tlearn: 0.1487318\ttotal: 29s\tremaining: 3m 44s\n",
            "307:\tlearn: 0.1482790\ttotal: 29.1s\tremaining: 3m 44s\n",
            "308:\tlearn: 0.1479317\ttotal: 29.2s\tremaining: 3m 43s\n",
            "309:\tlearn: 0.1467788\ttotal: 29.2s\tremaining: 3m 43s\n",
            "310:\tlearn: 0.1460236\ttotal: 29.3s\tremaining: 3m 43s\n",
            "311:\tlearn: 0.1448266\ttotal: 29.4s\tremaining: 3m 43s\n",
            "312:\tlearn: 0.1439820\ttotal: 29.5s\tremaining: 3m 42s\n",
            "313:\tlearn: 0.1434426\ttotal: 29.6s\tremaining: 3m 42s\n",
            "314:\tlearn: 0.1428183\ttotal: 29.6s\tremaining: 3m 42s\n",
            "315:\tlearn: 0.1423328\ttotal: 29.7s\tremaining: 3m 42s\n",
            "316:\tlearn: 0.1415743\ttotal: 29.8s\tremaining: 3m 41s\n",
            "317:\tlearn: 0.1413763\ttotal: 29.8s\tremaining: 3m 41s\n",
            "318:\tlearn: 0.1407384\ttotal: 29.9s\tremaining: 3m 41s\n",
            "319:\tlearn: 0.1400429\ttotal: 30s\tremaining: 3m 40s\n",
            "320:\tlearn: 0.1390585\ttotal: 30s\tremaining: 3m 40s\n",
            "321:\tlearn: 0.1383345\ttotal: 30.1s\tremaining: 3m 40s\n",
            "322:\tlearn: 0.1369335\ttotal: 30.2s\tremaining: 3m 40s\n",
            "323:\tlearn: 0.1365384\ttotal: 30.3s\tremaining: 3m 39s\n",
            "324:\tlearn: 0.1360623\ttotal: 30.3s\tremaining: 3m 39s\n",
            "325:\tlearn: 0.1353539\ttotal: 30.4s\tremaining: 3m 39s\n",
            "326:\tlearn: 0.1347071\ttotal: 30.5s\tremaining: 3m 39s\n",
            "327:\tlearn: 0.1340865\ttotal: 30.6s\tremaining: 3m 39s\n",
            "328:\tlearn: 0.1338290\ttotal: 30.6s\tremaining: 3m 38s\n",
            "329:\tlearn: 0.1317261\ttotal: 30.7s\tremaining: 3m 38s\n",
            "330:\tlearn: 0.1303587\ttotal: 30.8s\tremaining: 3m 38s\n",
            "331:\tlearn: 0.1297143\ttotal: 30.9s\tremaining: 3m 38s\n",
            "332:\tlearn: 0.1294869\ttotal: 30.9s\tremaining: 3m 37s\n",
            "333:\tlearn: 0.1294444\ttotal: 31s\tremaining: 3m 37s\n",
            "334:\tlearn: 0.1284664\ttotal: 31s\tremaining: 3m 37s\n",
            "335:\tlearn: 0.1274633\ttotal: 31.1s\tremaining: 3m 37s\n",
            "336:\tlearn: 0.1266337\ttotal: 31.2s\tremaining: 3m 36s\n",
            "337:\tlearn: 0.1261461\ttotal: 31.3s\tremaining: 3m 36s\n",
            "338:\tlearn: 0.1245231\ttotal: 31.4s\tremaining: 3m 36s\n",
            "339:\tlearn: 0.1239195\ttotal: 31.4s\tremaining: 3m 36s\n",
            "340:\tlearn: 0.1233121\ttotal: 31.5s\tremaining: 3m 36s\n",
            "341:\tlearn: 0.1223546\ttotal: 31.6s\tremaining: 3m 36s\n",
            "342:\tlearn: 0.1218433\ttotal: 31.7s\tremaining: 3m 35s\n",
            "343:\tlearn: 0.1207473\ttotal: 31.8s\tremaining: 3m 35s\n",
            "344:\tlearn: 0.1201970\ttotal: 31.8s\tremaining: 3m 35s\n",
            "345:\tlearn: 0.1196706\ttotal: 31.9s\tremaining: 3m 35s\n",
            "346:\tlearn: 0.1187494\ttotal: 32s\tremaining: 3m 34s\n",
            "347:\tlearn: 0.1178606\ttotal: 32.1s\tremaining: 3m 34s\n",
            "348:\tlearn: 0.1170358\ttotal: 32.1s\tremaining: 3m 34s\n",
            "349:\tlearn: 0.1164961\ttotal: 32.2s\tremaining: 3m 34s\n",
            "350:\tlearn: 0.1160171\ttotal: 32.3s\tremaining: 3m 34s\n",
            "351:\tlearn: 0.1153310\ttotal: 32.4s\tremaining: 3m 33s\n",
            "352:\tlearn: 0.1146706\ttotal: 32.4s\tremaining: 3m 33s\n",
            "353:\tlearn: 0.1138038\ttotal: 32.5s\tremaining: 3m 33s\n",
            "354:\tlearn: 0.1116285\ttotal: 32.6s\tremaining: 3m 33s\n",
            "355:\tlearn: 0.1107669\ttotal: 32.7s\tremaining: 3m 33s\n",
            "356:\tlearn: 0.1101443\ttotal: 32.8s\tremaining: 3m 33s\n",
            "357:\tlearn: 0.1089391\ttotal: 32.9s\tremaining: 3m 33s\n",
            "358:\tlearn: 0.1085740\ttotal: 33s\tremaining: 3m 33s\n",
            "359:\tlearn: 0.1065054\ttotal: 33.1s\tremaining: 3m 33s\n",
            "360:\tlearn: 0.1060726\ttotal: 33.2s\tremaining: 3m 32s\n",
            "361:\tlearn: 0.1054400\ttotal: 33.2s\tremaining: 3m 32s\n",
            "362:\tlearn: 0.1046909\ttotal: 33.3s\tremaining: 3m 32s\n",
            "363:\tlearn: 0.1043014\ttotal: 33.4s\tremaining: 3m 32s\n",
            "364:\tlearn: 0.1033875\ttotal: 33.5s\tremaining: 3m 32s\n",
            "365:\tlearn: 0.1029084\ttotal: 33.7s\tremaining: 3m 32s\n",
            "366:\tlearn: 0.1019808\ttotal: 33.8s\tremaining: 3m 32s\n",
            "367:\tlearn: 0.1014228\ttotal: 33.9s\tremaining: 3m 32s\n",
            "368:\tlearn: 0.1007921\ttotal: 33.9s\tremaining: 3m 32s\n",
            "369:\tlearn: 0.1001967\ttotal: 34s\tremaining: 3m 32s\n",
            "370:\tlearn: 0.0996226\ttotal: 34.1s\tremaining: 3m 32s\n",
            "371:\tlearn: 0.0991205\ttotal: 34.2s\tremaining: 3m 31s\n",
            "372:\tlearn: 0.0986664\ttotal: 34.2s\tremaining: 3m 31s\n",
            "373:\tlearn: 0.0982645\ttotal: 34.3s\tremaining: 3m 31s\n",
            "374:\tlearn: 0.0976125\ttotal: 34.4s\tremaining: 3m 31s\n",
            "375:\tlearn: 0.0971899\ttotal: 34.4s\tremaining: 3m 30s\n",
            "376:\tlearn: 0.0959207\ttotal: 34.5s\tremaining: 3m 30s\n",
            "377:\tlearn: 0.0954989\ttotal: 34.5s\tremaining: 3m 30s\n",
            "378:\tlearn: 0.0945987\ttotal: 34.6s\tremaining: 3m 30s\n",
            "379:\tlearn: 0.0937833\ttotal: 34.7s\tremaining: 3m 30s\n",
            "380:\tlearn: 0.0930245\ttotal: 34.8s\tremaining: 3m 29s\n",
            "381:\tlearn: 0.0924046\ttotal: 34.9s\tremaining: 3m 29s\n",
            "382:\tlearn: 0.0918639\ttotal: 35s\tremaining: 3m 29s\n",
            "383:\tlearn: 0.0916059\ttotal: 35s\tremaining: 3m 29s\n",
            "384:\tlearn: 0.0907453\ttotal: 35.1s\tremaining: 3m 29s\n",
            "385:\tlearn: 0.0904285\ttotal: 35.2s\tremaining: 3m 28s\n",
            "386:\tlearn: 0.0900377\ttotal: 35.2s\tremaining: 3m 28s\n",
            "387:\tlearn: 0.0892780\ttotal: 35.3s\tremaining: 3m 28s\n",
            "388:\tlearn: 0.0890035\ttotal: 35.4s\tremaining: 3m 28s\n",
            "389:\tlearn: 0.0875945\ttotal: 35.4s\tremaining: 3m 28s\n",
            "390:\tlearn: 0.0872262\ttotal: 35.5s\tremaining: 3m 27s\n",
            "391:\tlearn: 0.0865426\ttotal: 35.6s\tremaining: 3m 27s\n",
            "392:\tlearn: 0.0860345\ttotal: 35.7s\tremaining: 3m 27s\n",
            "393:\tlearn: 0.0857409\ttotal: 35.7s\tremaining: 3m 27s\n",
            "394:\tlearn: 0.0853991\ttotal: 35.8s\tremaining: 3m 27s\n",
            "395:\tlearn: 0.0850419\ttotal: 35.9s\tremaining: 3m 26s\n",
            "396:\tlearn: 0.0846424\ttotal: 36s\tremaining: 3m 26s\n",
            "397:\tlearn: 0.0844000\ttotal: 36s\tremaining: 3m 26s\n",
            "398:\tlearn: 0.0839794\ttotal: 36.1s\tremaining: 3m 26s\n",
            "399:\tlearn: 0.0838007\ttotal: 36.2s\tremaining: 3m 26s\n",
            "400:\tlearn: 0.0833576\ttotal: 36.2s\tremaining: 3m 25s\n",
            "401:\tlearn: 0.0831331\ttotal: 36.3s\tremaining: 3m 25s\n",
            "402:\tlearn: 0.0828725\ttotal: 36.4s\tremaining: 3m 25s\n",
            "403:\tlearn: 0.0824404\ttotal: 36.4s\tremaining: 3m 25s\n",
            "404:\tlearn: 0.0817416\ttotal: 36.5s\tremaining: 3m 25s\n",
            "405:\tlearn: 0.0811843\ttotal: 36.6s\tremaining: 3m 24s\n",
            "406:\tlearn: 0.0803077\ttotal: 36.7s\tremaining: 3m 24s\n",
            "407:\tlearn: 0.0797298\ttotal: 36.7s\tremaining: 3m 24s\n",
            "408:\tlearn: 0.0792508\ttotal: 36.9s\tremaining: 3m 24s\n",
            "409:\tlearn: 0.0789086\ttotal: 36.9s\tremaining: 3m 24s\n",
            "410:\tlearn: 0.0784695\ttotal: 37s\tremaining: 3m 24s\n",
            "411:\tlearn: 0.0781251\ttotal: 37s\tremaining: 3m 23s\n",
            "412:\tlearn: 0.0777852\ttotal: 37.1s\tremaining: 3m 23s\n",
            "413:\tlearn: 0.0775819\ttotal: 37.2s\tremaining: 3m 23s\n",
            "414:\tlearn: 0.0773019\ttotal: 37.3s\tremaining: 3m 23s\n",
            "415:\tlearn: 0.0770172\ttotal: 37.3s\tremaining: 3m 23s\n",
            "416:\tlearn: 0.0767216\ttotal: 37.4s\tremaining: 3m 23s\n",
            "417:\tlearn: 0.0762131\ttotal: 37.5s\tremaining: 3m 22s\n",
            "418:\tlearn: 0.0760473\ttotal: 37.6s\tremaining: 3m 22s\n",
            "419:\tlearn: 0.0758169\ttotal: 37.7s\tremaining: 3m 22s\n",
            "420:\tlearn: 0.0752700\ttotal: 37.7s\tremaining: 3m 22s\n",
            "421:\tlearn: 0.0746595\ttotal: 37.8s\tremaining: 3m 22s\n",
            "422:\tlearn: 0.0741761\ttotal: 37.9s\tremaining: 3m 22s\n",
            "423:\tlearn: 0.0739234\ttotal: 37.9s\tremaining: 3m 21s\n",
            "424:\tlearn: 0.0734784\ttotal: 38s\tremaining: 3m 21s\n",
            "425:\tlearn: 0.0732928\ttotal: 38.1s\tremaining: 3m 21s\n",
            "426:\tlearn: 0.0727940\ttotal: 38.2s\tremaining: 3m 21s\n",
            "427:\tlearn: 0.0724576\ttotal: 38.3s\tremaining: 3m 21s\n",
            "428:\tlearn: 0.0719644\ttotal: 38.4s\tremaining: 3m 21s\n",
            "429:\tlearn: 0.0714046\ttotal: 38.4s\tremaining: 3m 20s\n",
            "430:\tlearn: 0.0708913\ttotal: 38.5s\tremaining: 3m 20s\n",
            "431:\tlearn: 0.0705933\ttotal: 38.6s\tremaining: 3m 20s\n",
            "432:\tlearn: 0.0703530\ttotal: 38.6s\tremaining: 3m 20s\n",
            "433:\tlearn: 0.0701227\ttotal: 38.7s\tremaining: 3m 20s\n",
            "434:\tlearn: 0.0698395\ttotal: 38.8s\tremaining: 3m 20s\n",
            "435:\tlearn: 0.0696461\ttotal: 38.9s\tremaining: 3m 19s\n",
            "436:\tlearn: 0.0694410\ttotal: 38.9s\tremaining: 3m 19s\n",
            "437:\tlearn: 0.0691995\ttotal: 39s\tremaining: 3m 19s\n",
            "438:\tlearn: 0.0687493\ttotal: 39.1s\tremaining: 3m 19s\n",
            "439:\tlearn: 0.0684447\ttotal: 39.1s\tremaining: 3m 19s\n",
            "440:\tlearn: 0.0682620\ttotal: 39.2s\tremaining: 3m 18s\n",
            "441:\tlearn: 0.0679741\ttotal: 39.2s\tremaining: 3m 18s\n",
            "442:\tlearn: 0.0677444\ttotal: 39.3s\tremaining: 3m 18s\n",
            "443:\tlearn: 0.0675271\ttotal: 39.4s\tremaining: 3m 18s\n",
            "444:\tlearn: 0.0672862\ttotal: 39.5s\tremaining: 3m 18s\n",
            "445:\tlearn: 0.0671205\ttotal: 39.5s\tremaining: 3m 17s\n",
            "446:\tlearn: 0.0669919\ttotal: 39.6s\tremaining: 3m 17s\n",
            "447:\tlearn: 0.0667542\ttotal: 39.7s\tremaining: 3m 17s\n",
            "448:\tlearn: 0.0665017\ttotal: 39.7s\tremaining: 3m 17s\n",
            "449:\tlearn: 0.0661928\ttotal: 39.8s\tremaining: 3m 17s\n",
            "450:\tlearn: 0.0659425\ttotal: 39.9s\tremaining: 3m 16s\n",
            "451:\tlearn: 0.0657123\ttotal: 39.9s\tremaining: 3m 16s\n",
            "452:\tlearn: 0.0655273\ttotal: 40s\tremaining: 3m 16s\n",
            "453:\tlearn: 0.0652432\ttotal: 40.1s\tremaining: 3m 16s\n",
            "454:\tlearn: 0.0650664\ttotal: 40.2s\tremaining: 3m 16s\n",
            "455:\tlearn: 0.0647042\ttotal: 40.2s\tremaining: 3m 16s\n",
            "456:\tlearn: 0.0642942\ttotal: 40.3s\tremaining: 3m 15s\n",
            "457:\tlearn: 0.0640619\ttotal: 40.4s\tremaining: 3m 15s\n",
            "458:\tlearn: 0.0637645\ttotal: 40.4s\tremaining: 3m 15s\n",
            "459:\tlearn: 0.0633906\ttotal: 40.5s\tremaining: 3m 15s\n",
            "460:\tlearn: 0.0632459\ttotal: 40.6s\tremaining: 3m 15s\n",
            "461:\tlearn: 0.0631532\ttotal: 40.7s\tremaining: 3m 15s\n",
            "462:\tlearn: 0.0630209\ttotal: 40.7s\tremaining: 3m 14s\n",
            "463:\tlearn: 0.0628960\ttotal: 40.8s\tremaining: 3m 14s\n",
            "464:\tlearn: 0.0626974\ttotal: 40.8s\tremaining: 3m 14s\n",
            "465:\tlearn: 0.0635427\ttotal: 40.9s\tremaining: 3m 14s\n",
            "466:\tlearn: 0.0634794\ttotal: 41s\tremaining: 3m 14s\n",
            "467:\tlearn: 0.0632916\ttotal: 41s\tremaining: 3m 13s\n",
            "468:\tlearn: 0.0630112\ttotal: 41.1s\tremaining: 3m 13s\n",
            "469:\tlearn: 0.0627987\ttotal: 41.2s\tremaining: 3m 13s\n",
            "470:\tlearn: 0.0626193\ttotal: 41.3s\tremaining: 3m 13s\n",
            "471:\tlearn: 0.0623851\ttotal: 41.4s\tremaining: 3m 13s\n",
            "472:\tlearn: 0.0622466\ttotal: 41.4s\tremaining: 3m 13s\n",
            "473:\tlearn: 0.0620839\ttotal: 41.5s\tremaining: 3m 13s\n",
            "474:\tlearn: 0.0619068\ttotal: 41.6s\tremaining: 3m 12s\n",
            "475:\tlearn: 0.0617192\ttotal: 41.6s\tremaining: 3m 12s\n",
            "476:\tlearn: 0.0615637\ttotal: 41.7s\tremaining: 3m 12s\n",
            "477:\tlearn: 0.0614506\ttotal: 41.7s\tremaining: 3m 12s\n",
            "478:\tlearn: 0.0612264\ttotal: 41.8s\tremaining: 3m 11s\n",
            "479:\tlearn: 0.0609981\ttotal: 41.9s\tremaining: 3m 12s\n",
            "480:\tlearn: 0.0605953\ttotal: 42s\tremaining: 3m 11s\n",
            "481:\tlearn: 0.0604450\ttotal: 42s\tremaining: 3m 11s\n",
            "482:\tlearn: 0.0591630\ttotal: 42.1s\tremaining: 3m 11s\n",
            "483:\tlearn: 0.0590368\ttotal: 42.2s\tremaining: 3m 11s\n",
            "484:\tlearn: 0.0588684\ttotal: 42.3s\tremaining: 3m 11s\n",
            "485:\tlearn: 0.0586033\ttotal: 42.3s\tremaining: 3m 11s\n",
            "486:\tlearn: 0.0582668\ttotal: 42.4s\tremaining: 3m 10s\n",
            "487:\tlearn: 0.0581572\ttotal: 42.5s\tremaining: 3m 10s\n",
            "488:\tlearn: 0.0579171\ttotal: 42.5s\tremaining: 3m 10s\n",
            "489:\tlearn: 0.0577380\ttotal: 42.6s\tremaining: 3m 10s\n",
            "490:\tlearn: 0.0575657\ttotal: 43s\tremaining: 3m 11s\n",
            "491:\tlearn: 0.0573332\ttotal: 43.1s\tremaining: 3m 11s\n",
            "492:\tlearn: 0.0570869\ttotal: 43.3s\tremaining: 3m 12s\n",
            "493:\tlearn: 0.0568333\ttotal: 43.4s\tremaining: 3m 11s\n",
            "494:\tlearn: 0.0566470\ttotal: 43.5s\tremaining: 3m 11s\n",
            "495:\tlearn: 0.0564665\ttotal: 43.5s\tremaining: 3m 11s\n",
            "496:\tlearn: 0.0563751\ttotal: 43.6s\tremaining: 3m 11s\n",
            "497:\tlearn: 0.0561742\ttotal: 43.7s\tremaining: 3m 11s\n",
            "498:\tlearn: 0.0559718\ttotal: 43.7s\tremaining: 3m 11s\n",
            "499:\tlearn: 0.0557022\ttotal: 43.8s\tremaining: 3m 10s\n",
            "500:\tlearn: 0.0556114\ttotal: 43.9s\tremaining: 3m 10s\n",
            "501:\tlearn: 0.0554941\ttotal: 43.9s\tremaining: 3m 10s\n",
            "502:\tlearn: 0.0553548\ttotal: 44s\tremaining: 3m 10s\n",
            "503:\tlearn: 0.0552212\ttotal: 44.1s\tremaining: 3m 10s\n",
            "504:\tlearn: 0.0551309\ttotal: 44.2s\tremaining: 3m 10s\n",
            "505:\tlearn: 0.0549760\ttotal: 44.2s\tremaining: 3m 9s\n",
            "506:\tlearn: 0.0547363\ttotal: 44.3s\tremaining: 3m 9s\n",
            "507:\tlearn: 0.0545995\ttotal: 44.3s\tremaining: 3m 9s\n",
            "508:\tlearn: 0.0543612\ttotal: 44.4s\tremaining: 3m 9s\n",
            "509:\tlearn: 0.0542098\ttotal: 44.5s\tremaining: 3m 9s\n",
            "510:\tlearn: 0.0540504\ttotal: 44.6s\tremaining: 3m 9s\n",
            "511:\tlearn: 0.0537926\ttotal: 44.7s\tremaining: 3m 9s\n",
            "512:\tlearn: 0.0536905\ttotal: 44.7s\tremaining: 3m 8s\n",
            "513:\tlearn: 0.0535472\ttotal: 44.8s\tremaining: 3m 8s\n",
            "514:\tlearn: 0.0533454\ttotal: 45s\tremaining: 3m 9s\n",
            "515:\tlearn: 0.0530386\ttotal: 45.1s\tremaining: 3m 9s\n",
            "516:\tlearn: 0.0527683\ttotal: 45.2s\tremaining: 3m 9s\n",
            "517:\tlearn: 0.0526258\ttotal: 45.3s\tremaining: 3m 9s\n",
            "518:\tlearn: 0.0524807\ttotal: 45.7s\tremaining: 3m 10s\n",
            "519:\tlearn: 0.0523222\ttotal: 45.8s\tremaining: 3m 10s\n",
            "520:\tlearn: 0.0521213\ttotal: 46.1s\tremaining: 3m 10s\n",
            "521:\tlearn: 0.0519809\ttotal: 46.2s\tremaining: 3m 10s\n",
            "522:\tlearn: 0.0518752\ttotal: 46.3s\tremaining: 3m 11s\n",
            "523:\tlearn: 0.0516040\ttotal: 46.5s\tremaining: 3m 11s\n",
            "524:\tlearn: 0.0515134\ttotal: 46.7s\tremaining: 3m 11s\n",
            "525:\tlearn: 0.0514390\ttotal: 46.8s\tremaining: 3m 11s\n",
            "526:\tlearn: 0.0512886\ttotal: 46.9s\tremaining: 3m 11s\n",
            "527:\tlearn: 0.0511667\ttotal: 47s\tremaining: 3m 11s\n",
            "528:\tlearn: 0.0509977\ttotal: 47.1s\tremaining: 3m 11s\n",
            "529:\tlearn: 0.0507682\ttotal: 47.1s\tremaining: 3m 11s\n",
            "530:\tlearn: 0.0505454\ttotal: 47.3s\tremaining: 3m 11s\n",
            "531:\tlearn: 0.0504362\ttotal: 47.4s\tremaining: 3m 11s\n",
            "532:\tlearn: 0.0503244\ttotal: 47.4s\tremaining: 3m 10s\n",
            "533:\tlearn: 0.0501844\ttotal: 47.5s\tremaining: 3m 10s\n",
            "534:\tlearn: 0.0497736\ttotal: 47.6s\tremaining: 3m 10s\n",
            "535:\tlearn: 0.0496279\ttotal: 47.7s\tremaining: 3m 10s\n",
            "536:\tlearn: 0.0495597\ttotal: 47.8s\tremaining: 3m 10s\n",
            "537:\tlearn: 0.0493996\ttotal: 47.9s\tremaining: 3m 10s\n",
            "538:\tlearn: 0.0491370\ttotal: 47.9s\tremaining: 3m 10s\n",
            "539:\tlearn: 0.0489304\ttotal: 48s\tremaining: 3m 10s\n",
            "540:\tlearn: 0.0487893\ttotal: 48.1s\tremaining: 3m 10s\n",
            "541:\tlearn: 0.0486617\ttotal: 48.2s\tremaining: 3m 9s\n",
            "542:\tlearn: 0.0485140\ttotal: 48.2s\tremaining: 3m 9s\n",
            "543:\tlearn: 0.0483875\ttotal: 48.3s\tremaining: 3m 9s\n",
            "544:\tlearn: 0.0480701\ttotal: 48.3s\tremaining: 3m 9s\n",
            "545:\tlearn: 0.0479095\ttotal: 48.4s\tremaining: 3m 9s\n",
            "546:\tlearn: 0.0477984\ttotal: 48.5s\tremaining: 3m 8s\n",
            "547:\tlearn: 0.0476695\ttotal: 48.5s\tremaining: 3m 8s\n",
            "548:\tlearn: 0.0475558\ttotal: 48.6s\tremaining: 3m 8s\n",
            "549:\tlearn: 0.0474224\ttotal: 48.7s\tremaining: 3m 8s\n",
            "550:\tlearn: 0.0473144\ttotal: 48.8s\tremaining: 3m 8s\n",
            "551:\tlearn: 0.0471404\ttotal: 48.8s\tremaining: 3m 8s\n",
            "552:\tlearn: 0.0470172\ttotal: 48.9s\tremaining: 3m 8s\n",
            "553:\tlearn: 0.0469107\ttotal: 49s\tremaining: 3m 7s\n",
            "554:\tlearn: 0.0468443\ttotal: 49s\tremaining: 3m 7s\n",
            "555:\tlearn: 0.0467262\ttotal: 49.1s\tremaining: 3m 7s\n",
            "556:\tlearn: 0.0466418\ttotal: 49.2s\tremaining: 3m 7s\n",
            "557:\tlearn: 0.0465115\ttotal: 49.2s\tremaining: 3m 7s\n",
            "558:\tlearn: 0.0464138\ttotal: 49.3s\tremaining: 3m 6s\n",
            "559:\tlearn: 0.0462998\ttotal: 49.4s\tremaining: 3m 6s\n",
            "560:\tlearn: 0.0461620\ttotal: 49.4s\tremaining: 3m 6s\n",
            "561:\tlearn: 0.0460203\ttotal: 49.5s\tremaining: 3m 6s\n",
            "562:\tlearn: 0.0458969\ttotal: 49.6s\tremaining: 3m 6s\n",
            "563:\tlearn: 0.0457998\ttotal: 49.7s\tremaining: 3m 6s\n",
            "564:\tlearn: 0.0456697\ttotal: 49.8s\tremaining: 3m 6s\n",
            "565:\tlearn: 0.0455265\ttotal: 49.8s\tremaining: 3m 6s\n",
            "566:\tlearn: 0.0453764\ttotal: 49.9s\tremaining: 3m 5s\n",
            "567:\tlearn: 0.0452138\ttotal: 50s\tremaining: 3m 5s\n",
            "568:\tlearn: 0.0451063\ttotal: 50s\tremaining: 3m 5s\n",
            "569:\tlearn: 0.0448710\ttotal: 50.1s\tremaining: 3m 5s\n",
            "570:\tlearn: 0.0447738\ttotal: 50.2s\tremaining: 3m 5s\n",
            "571:\tlearn: 0.0445658\ttotal: 50.3s\tremaining: 3m 5s\n",
            "572:\tlearn: 0.0444118\ttotal: 50.4s\tremaining: 3m 5s\n",
            "573:\tlearn: 0.0441094\ttotal: 50.4s\tremaining: 3m 4s\n",
            "574:\tlearn: 0.0439212\ttotal: 50.5s\tremaining: 3m 4s\n",
            "575:\tlearn: 0.0437790\ttotal: 50.6s\tremaining: 3m 4s\n",
            "576:\tlearn: 0.0436281\ttotal: 50.6s\tremaining: 3m 4s\n",
            "577:\tlearn: 0.0435199\ttotal: 50.7s\tremaining: 3m 4s\n",
            "578:\tlearn: 0.0434104\ttotal: 50.8s\tremaining: 3m 4s\n",
            "579:\tlearn: 0.0432623\ttotal: 50.8s\tremaining: 3m 4s\n",
            "580:\tlearn: 0.0431357\ttotal: 50.9s\tremaining: 3m 3s\n",
            "581:\tlearn: 0.0430573\ttotal: 51s\tremaining: 3m 3s\n",
            "582:\tlearn: 0.0429139\ttotal: 51.1s\tremaining: 3m 3s\n",
            "583:\tlearn: 0.0428298\ttotal: 51.2s\tremaining: 3m 3s\n",
            "584:\tlearn: 0.0427117\ttotal: 51.2s\tremaining: 3m 3s\n",
            "585:\tlearn: 0.0425447\ttotal: 51.3s\tremaining: 3m 3s\n",
            "586:\tlearn: 0.0423938\ttotal: 51.4s\tremaining: 3m 3s\n",
            "587:\tlearn: 0.0423556\ttotal: 51.5s\tremaining: 3m 3s\n",
            "588:\tlearn: 0.0423211\ttotal: 51.5s\tremaining: 3m 2s\n",
            "589:\tlearn: 0.0422345\ttotal: 51.6s\tremaining: 3m 2s\n",
            "590:\tlearn: 0.0421041\ttotal: 51.7s\tremaining: 3m 2s\n",
            "591:\tlearn: 0.0419859\ttotal: 51.7s\tremaining: 3m 2s\n",
            "592:\tlearn: 0.0418929\ttotal: 51.8s\tremaining: 3m 2s\n",
            "593:\tlearn: 0.0417803\ttotal: 51.9s\tremaining: 3m 2s\n",
            "594:\tlearn: 0.0416933\ttotal: 52s\tremaining: 3m 1s\n",
            "595:\tlearn: 0.0416094\ttotal: 52s\tremaining: 3m 1s\n",
            "596:\tlearn: 0.0414160\ttotal: 52.1s\tremaining: 3m 1s\n",
            "597:\tlearn: 0.0412737\ttotal: 52.2s\tremaining: 3m 1s\n",
            "598:\tlearn: 0.0411875\ttotal: 52.2s\tremaining: 3m 1s\n",
            "599:\tlearn: 0.0411324\ttotal: 52.3s\tremaining: 3m 1s\n",
            "600:\tlearn: 0.0410229\ttotal: 52.4s\tremaining: 3m 1s\n",
            "601:\tlearn: 0.0408630\ttotal: 52.4s\tremaining: 3m\n",
            "602:\tlearn: 0.0406710\ttotal: 52.5s\tremaining: 3m\n",
            "603:\tlearn: 0.0405420\ttotal: 52.6s\tremaining: 3m\n",
            "604:\tlearn: 0.0404008\ttotal: 52.8s\tremaining: 3m\n",
            "605:\tlearn: 0.0402064\ttotal: 52.9s\tremaining: 3m 1s\n",
            "606:\tlearn: 0.0401118\ttotal: 53s\tremaining: 3m\n",
            "607:\tlearn: 0.0400296\ttotal: 53.1s\tremaining: 3m\n",
            "608:\tlearn: 0.0399686\ttotal: 53.2s\tremaining: 3m\n",
            "609:\tlearn: 0.0398598\ttotal: 53.2s\tremaining: 3m\n",
            "610:\tlearn: 0.0397755\ttotal: 53.3s\tremaining: 3m\n",
            "611:\tlearn: 0.0397020\ttotal: 53.4s\tremaining: 3m\n",
            "612:\tlearn: 0.0396310\ttotal: 53.4s\tremaining: 3m\n",
            "613:\tlearn: 0.0395302\ttotal: 53.5s\tremaining: 2m 59s\n",
            "614:\tlearn: 0.0394414\ttotal: 53.6s\tremaining: 2m 59s\n",
            "615:\tlearn: 0.0393967\ttotal: 53.6s\tremaining: 2m 59s\n",
            "616:\tlearn: 0.0393241\ttotal: 53.7s\tremaining: 2m 59s\n",
            "617:\tlearn: 0.0392619\ttotal: 53.8s\tremaining: 2m 59s\n",
            "618:\tlearn: 0.0391881\ttotal: 53.9s\tremaining: 2m 59s\n",
            "619:\tlearn: 0.0391314\ttotal: 53.9s\tremaining: 2m 59s\n",
            "620:\tlearn: 0.0390645\ttotal: 54s\tremaining: 2m 58s\n",
            "621:\tlearn: 0.0389805\ttotal: 54.1s\tremaining: 2m 58s\n",
            "622:\tlearn: 0.0389251\ttotal: 54.1s\tremaining: 2m 58s\n",
            "623:\tlearn: 0.0388928\ttotal: 54.2s\tremaining: 2m 58s\n",
            "624:\tlearn: 0.0387635\ttotal: 54.3s\tremaining: 2m 58s\n",
            "625:\tlearn: 0.0386345\ttotal: 54.4s\tremaining: 2m 58s\n",
            "626:\tlearn: 0.0384480\ttotal: 54.4s\tremaining: 2m 58s\n",
            "627:\tlearn: 0.0383937\ttotal: 54.5s\tremaining: 2m 57s\n",
            "628:\tlearn: 0.0383230\ttotal: 54.6s\tremaining: 2m 57s\n",
            "629:\tlearn: 0.0382514\ttotal: 54.6s\tremaining: 2m 57s\n",
            "630:\tlearn: 0.0381406\ttotal: 54.7s\tremaining: 2m 57s\n",
            "631:\tlearn: 0.0380455\ttotal: 54.8s\tremaining: 2m 57s\n",
            "632:\tlearn: 0.0379200\ttotal: 54.9s\tremaining: 2m 57s\n",
            "633:\tlearn: 0.0378036\ttotal: 54.9s\tremaining: 2m 57s\n",
            "634:\tlearn: 0.0377047\ttotal: 55s\tremaining: 2m 56s\n",
            "635:\tlearn: 0.0376044\ttotal: 55.1s\tremaining: 2m 56s\n",
            "636:\tlearn: 0.0375055\ttotal: 55.1s\tremaining: 2m 56s\n",
            "637:\tlearn: 0.0373901\ttotal: 55.2s\tremaining: 2m 56s\n",
            "638:\tlearn: 0.0373248\ttotal: 55.3s\tremaining: 2m 56s\n",
            "639:\tlearn: 0.0372349\ttotal: 55.3s\tremaining: 2m 56s\n",
            "640:\tlearn: 0.0371267\ttotal: 55.4s\tremaining: 2m 56s\n",
            "641:\tlearn: 0.0370471\ttotal: 55.5s\tremaining: 2m 56s\n",
            "642:\tlearn: 0.0369771\ttotal: 55.6s\tremaining: 2m 56s\n",
            "643:\tlearn: 0.0368851\ttotal: 55.7s\tremaining: 2m 55s\n",
            "644:\tlearn: 0.0368169\ttotal: 55.8s\tremaining: 2m 55s\n",
            "645:\tlearn: 0.0367127\ttotal: 55.8s\tremaining: 2m 55s\n",
            "646:\tlearn: 0.0366416\ttotal: 55.9s\tremaining: 2m 55s\n",
            "647:\tlearn: 0.0365435\ttotal: 55.9s\tremaining: 2m 55s\n",
            "648:\tlearn: 0.0363935\ttotal: 56.1s\tremaining: 2m 55s\n",
            "649:\tlearn: 0.0362919\ttotal: 56.2s\tremaining: 2m 55s\n",
            "650:\tlearn: 0.0361709\ttotal: 56.3s\tremaining: 2m 55s\n",
            "651:\tlearn: 0.0360630\ttotal: 56.3s\tremaining: 2m 55s\n",
            "652:\tlearn: 0.0359582\ttotal: 56.4s\tremaining: 2m 54s\n",
            "653:\tlearn: 0.0358932\ttotal: 56.5s\tremaining: 2m 54s\n",
            "654:\tlearn: 0.0357785\ttotal: 56.5s\tremaining: 2m 54s\n",
            "655:\tlearn: 0.0357317\ttotal: 56.6s\tremaining: 2m 54s\n",
            "656:\tlearn: 0.0356322\ttotal: 56.7s\tremaining: 2m 54s\n",
            "657:\tlearn: 0.0355265\ttotal: 56.7s\tremaining: 2m 54s\n",
            "658:\tlearn: 0.0354624\ttotal: 56.8s\tremaining: 2m 54s\n",
            "659:\tlearn: 0.0353714\ttotal: 56.9s\tremaining: 2m 53s\n",
            "660:\tlearn: 0.0352679\ttotal: 56.9s\tremaining: 2m 53s\n",
            "661:\tlearn: 0.0351845\ttotal: 57s\tremaining: 2m 53s\n",
            "662:\tlearn: 0.0350725\ttotal: 57.1s\tremaining: 2m 53s\n",
            "663:\tlearn: 0.0350220\ttotal: 57.1s\tremaining: 2m 53s\n",
            "664:\tlearn: 0.0349381\ttotal: 57.2s\tremaining: 2m 53s\n",
            "665:\tlearn: 0.0348749\ttotal: 57.3s\tremaining: 2m 53s\n",
            "666:\tlearn: 0.0347471\ttotal: 57.4s\tremaining: 2m 53s\n",
            "667:\tlearn: 0.0346767\ttotal: 57.4s\tremaining: 2m 52s\n",
            "668:\tlearn: 0.0346126\ttotal: 57.6s\tremaining: 2m 52s\n",
            "669:\tlearn: 0.0345095\ttotal: 57.6s\tremaining: 2m 52s\n",
            "670:\tlearn: 0.0344259\ttotal: 57.7s\tremaining: 2m 52s\n",
            "671:\tlearn: 0.0343646\ttotal: 57.8s\tremaining: 2m 52s\n",
            "672:\tlearn: 0.0343028\ttotal: 57.9s\tremaining: 2m 52s\n",
            "673:\tlearn: 0.0341881\ttotal: 58.1s\tremaining: 2m 52s\n",
            "674:\tlearn: 0.0340879\ttotal: 58.2s\tremaining: 2m 52s\n",
            "675:\tlearn: 0.0340116\ttotal: 58.3s\tremaining: 2m 52s\n",
            "676:\tlearn: 0.0339740\ttotal: 58.4s\tremaining: 2m 52s\n",
            "677:\tlearn: 0.0338811\ttotal: 58.5s\tremaining: 2m 52s\n",
            "678:\tlearn: 0.0338039\ttotal: 58.6s\tremaining: 2m 52s\n",
            "679:\tlearn: 0.0337125\ttotal: 58.7s\tremaining: 2m 52s\n",
            "680:\tlearn: 0.0335922\ttotal: 58.8s\tremaining: 2m 52s\n",
            "681:\tlearn: 0.0334881\ttotal: 58.9s\tremaining: 2m 52s\n",
            "682:\tlearn: 0.0334156\ttotal: 59s\tremaining: 2m 52s\n",
            "683:\tlearn: 0.0333367\ttotal: 59.1s\tremaining: 2m 52s\n",
            "684:\tlearn: 0.0332157\ttotal: 59.2s\tremaining: 2m 52s\n",
            "685:\tlearn: 0.0331561\ttotal: 59.3s\tremaining: 2m 52s\n",
            "686:\tlearn: 0.0330435\ttotal: 59.4s\tremaining: 2m 52s\n",
            "687:\tlearn: 0.0329896\ttotal: 59.4s\tremaining: 2m 51s\n",
            "688:\tlearn: 0.0329016\ttotal: 59.5s\tremaining: 2m 51s\n",
            "689:\tlearn: 0.0328341\ttotal: 59.6s\tremaining: 2m 51s\n",
            "690:\tlearn: 0.0327727\ttotal: 59.7s\tremaining: 2m 51s\n",
            "691:\tlearn: 0.0326848\ttotal: 59.7s\tremaining: 2m 51s\n",
            "692:\tlearn: 0.0326404\ttotal: 59.8s\tremaining: 2m 51s\n",
            "693:\tlearn: 0.0325778\ttotal: 59.9s\tremaining: 2m 51s\n",
            "694:\tlearn: 0.0325020\ttotal: 60s\tremaining: 2m 51s\n",
            "695:\tlearn: 0.0324227\ttotal: 1m\tremaining: 2m 51s\n",
            "696:\tlearn: 0.0323668\ttotal: 1m\tremaining: 2m 50s\n",
            "697:\tlearn: 0.0323165\ttotal: 1m\tremaining: 2m 50s\n",
            "698:\tlearn: 0.0322803\ttotal: 1m\tremaining: 2m 50s\n",
            "699:\tlearn: 0.0322112\ttotal: 1m\tremaining: 2m 50s\n",
            "700:\tlearn: 0.0321654\ttotal: 1m\tremaining: 2m 50s\n",
            "701:\tlearn: 0.0321341\ttotal: 1m\tremaining: 2m 50s\n",
            "702:\tlearn: 0.0320642\ttotal: 1m\tremaining: 2m 50s\n",
            "703:\tlearn: 0.0319817\ttotal: 1m\tremaining: 2m 50s\n",
            "704:\tlearn: 0.0318301\ttotal: 1m\tremaining: 2m 50s\n",
            "705:\tlearn: 0.0317722\ttotal: 1m\tremaining: 2m 50s\n",
            "706:\tlearn: 0.0317145\ttotal: 1m\tremaining: 2m 49s\n",
            "707:\tlearn: 0.0316543\ttotal: 1m\tremaining: 2m 49s\n",
            "708:\tlearn: 0.0315791\ttotal: 1m 1s\tremaining: 2m 49s\n",
            "709:\tlearn: 0.0314249\ttotal: 1m 1s\tremaining: 2m 49s\n",
            "710:\tlearn: 0.0313762\ttotal: 1m 1s\tremaining: 2m 49s\n",
            "711:\tlearn: 0.0313103\ttotal: 1m 1s\tremaining: 2m 49s\n",
            "712:\tlearn: 0.0312521\ttotal: 1m 1s\tremaining: 2m 50s\n",
            "713:\tlearn: 0.0312077\ttotal: 1m 1s\tremaining: 2m 50s\n",
            "714:\tlearn: 0.0311601\ttotal: 1m 1s\tremaining: 2m 50s\n",
            "715:\tlearn: 0.0310704\ttotal: 1m 2s\tremaining: 2m 50s\n",
            "716:\tlearn: 0.0309857\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "717:\tlearn: 0.0308965\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "718:\tlearn: 0.0308161\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "719:\tlearn: 0.0307392\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "720:\tlearn: 0.0306217\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "721:\tlearn: 0.0305793\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "722:\tlearn: 0.0305411\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "723:\tlearn: 0.0304792\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "724:\tlearn: 0.0304332\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "725:\tlearn: 0.0304005\ttotal: 1m 2s\tremaining: 2m 49s\n",
            "726:\tlearn: 0.0303526\ttotal: 1m 2s\tremaining: 2m 48s\n",
            "727:\tlearn: 0.0302887\ttotal: 1m 2s\tremaining: 2m 48s\n",
            "728:\tlearn: 0.0302536\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "729:\tlearn: 0.0301519\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "730:\tlearn: 0.0300830\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "731:\tlearn: 0.0300561\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "732:\tlearn: 0.0300281\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "733:\tlearn: 0.0299473\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "734:\tlearn: 0.0299169\ttotal: 1m 3s\tremaining: 2m 48s\n",
            "735:\tlearn: 0.0298233\ttotal: 1m 3s\tremaining: 2m 47s\n",
            "736:\tlearn: 0.0297467\ttotal: 1m 3s\tremaining: 2m 47s\n",
            "737:\tlearn: 0.0296797\ttotal: 1m 3s\tremaining: 2m 47s\n",
            "738:\tlearn: 0.0296186\ttotal: 1m 3s\tremaining: 2m 47s\n",
            "739:\tlearn: 0.0295674\ttotal: 1m 3s\tremaining: 2m 47s\n",
            "740:\tlearn: 0.0294739\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "741:\tlearn: 0.0293919\ttotal: 1m 4s\tremaining: 2m 47s\n",
            "742:\tlearn: 0.0293353\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "743:\tlearn: 0.0292971\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "744:\tlearn: 0.0292010\ttotal: 1m 4s\tremaining: 2m 48s\n",
            "745:\tlearn: 0.0290870\ttotal: 1m 5s\tremaining: 2m 48s\n",
            "746:\tlearn: 0.0290249\ttotal: 1m 5s\tremaining: 2m 49s\n",
            "747:\tlearn: 0.0289712\ttotal: 1m 5s\tremaining: 2m 49s\n",
            "748:\tlearn: 0.0289058\ttotal: 1m 5s\tremaining: 2m 49s\n",
            "749:\tlearn: 0.0288464\ttotal: 1m 5s\tremaining: 2m 49s\n",
            "750:\tlearn: 0.0287970\ttotal: 1m 5s\tremaining: 2m 49s\n",
            "751:\tlearn: 0.0287642\ttotal: 1m 5s\tremaining: 2m 48s\n",
            "752:\tlearn: 0.0287224\ttotal: 1m 5s\tremaining: 2m 48s\n",
            "753:\tlearn: 0.0286734\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "754:\tlearn: 0.0286261\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "755:\tlearn: 0.0285433\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "756:\tlearn: 0.0284903\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "757:\tlearn: 0.0284377\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "758:\tlearn: 0.0283503\ttotal: 1m 6s\tremaining: 2m 48s\n",
            "759:\tlearn: 0.0282943\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "760:\tlearn: 0.0282268\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "761:\tlearn: 0.0281645\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "762:\tlearn: 0.0281270\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "763:\tlearn: 0.0280129\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "764:\tlearn: 0.0279800\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "765:\tlearn: 0.0279228\ttotal: 1m 6s\tremaining: 2m 47s\n",
            "766:\tlearn: 0.0278562\ttotal: 1m 6s\tremaining: 2m 46s\n",
            "767:\tlearn: 0.0278074\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "768:\tlearn: 0.0277865\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "769:\tlearn: 0.0276736\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "770:\tlearn: 0.0276129\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "771:\tlearn: 0.0275782\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "772:\tlearn: 0.0275417\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "773:\tlearn: 0.0274686\ttotal: 1m 7s\tremaining: 2m 46s\n",
            "774:\tlearn: 0.0273838\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "775:\tlearn: 0.0273378\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "776:\tlearn: 0.0272812\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "777:\tlearn: 0.0272612\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "778:\tlearn: 0.0271893\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "779:\tlearn: 0.0271371\ttotal: 1m 7s\tremaining: 2m 45s\n",
            "780:\tlearn: 0.0270660\ttotal: 1m 8s\tremaining: 2m 45s\n",
            "781:\tlearn: 0.0269733\ttotal: 1m 8s\tremaining: 2m 45s\n",
            "782:\tlearn: 0.0268920\ttotal: 1m 8s\tremaining: 2m 45s\n",
            "783:\tlearn: 0.0268427\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "784:\tlearn: 0.0268022\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "785:\tlearn: 0.0267386\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "786:\tlearn: 0.0266648\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "787:\tlearn: 0.0266067\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "788:\tlearn: 0.0265630\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "789:\tlearn: 0.0264873\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "790:\tlearn: 0.0263971\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "791:\tlearn: 0.0263477\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "792:\tlearn: 0.0262640\ttotal: 1m 8s\tremaining: 2m 44s\n",
            "793:\tlearn: 0.0262084\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "794:\tlearn: 0.0261776\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "795:\tlearn: 0.0261329\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "796:\tlearn: 0.0260786\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "797:\tlearn: 0.0260327\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "798:\tlearn: 0.0259983\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "799:\tlearn: 0.0259646\ttotal: 1m 9s\tremaining: 2m 43s\n",
            "800:\tlearn: 0.0259349\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "801:\tlearn: 0.0258909\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "802:\tlearn: 0.0258573\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "803:\tlearn: 0.0258252\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "804:\tlearn: 0.0257804\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "805:\tlearn: 0.0257110\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "806:\tlearn: 0.0256331\ttotal: 1m 9s\tremaining: 2m 42s\n",
            "807:\tlearn: 0.0255611\ttotal: 1m 10s\tremaining: 2m 42s\n",
            "808:\tlearn: 0.0255010\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "809:\tlearn: 0.0254409\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "810:\tlearn: 0.0253730\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "811:\tlearn: 0.0253292\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "812:\tlearn: 0.0252860\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "813:\tlearn: 0.0252536\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "814:\tlearn: 0.0252164\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "815:\tlearn: 0.0251406\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "816:\tlearn: 0.0251105\ttotal: 1m 10s\tremaining: 2m 41s\n",
            "817:\tlearn: 0.0250809\ttotal: 1m 10s\tremaining: 2m 40s\n",
            "818:\tlearn: 0.0250238\ttotal: 1m 10s\tremaining: 2m 40s\n",
            "819:\tlearn: 0.0249707\ttotal: 1m 10s\tremaining: 2m 40s\n",
            "820:\tlearn: 0.0249260\ttotal: 1m 10s\tremaining: 2m 40s\n",
            "821:\tlearn: 0.0248994\ttotal: 1m 11s\tremaining: 2m 40s\n",
            "822:\tlearn: 0.0248568\ttotal: 1m 11s\tremaining: 2m 40s\n",
            "823:\tlearn: 0.0248026\ttotal: 1m 11s\tremaining: 2m 40s\n",
            "824:\tlearn: 0.0247508\ttotal: 1m 11s\tremaining: 2m 40s\n",
            "825:\tlearn: 0.0247191\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "826:\tlearn: 0.0246658\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "827:\tlearn: 0.0246024\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "828:\tlearn: 0.0245848\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "829:\tlearn: 0.0245447\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "830:\tlearn: 0.0244748\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "831:\tlearn: 0.0244028\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "832:\tlearn: 0.0242792\ttotal: 1m 11s\tremaining: 2m 39s\n",
            "833:\tlearn: 0.0242378\ttotal: 1m 11s\tremaining: 2m 38s\n",
            "834:\tlearn: 0.0241358\ttotal: 1m 11s\tremaining: 2m 38s\n",
            "835:\tlearn: 0.0241088\ttotal: 1m 11s\tremaining: 2m 38s\n",
            "836:\tlearn: 0.0240639\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "837:\tlearn: 0.0240186\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "838:\tlearn: 0.0239754\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "839:\tlearn: 0.0239539\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "840:\tlearn: 0.0239141\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "841:\tlearn: 0.0238491\ttotal: 1m 12s\tremaining: 2m 38s\n",
            "842:\tlearn: 0.0238188\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "843:\tlearn: 0.0237853\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "844:\tlearn: 0.0237375\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "845:\tlearn: 0.0236758\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "846:\tlearn: 0.0236331\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "847:\tlearn: 0.0235903\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "848:\tlearn: 0.0235570\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "849:\tlearn: 0.0235016\ttotal: 1m 12s\tremaining: 2m 37s\n",
            "850:\tlearn: 0.0234571\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "851:\tlearn: 0.0234099\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "852:\tlearn: 0.0233759\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "853:\tlearn: 0.0233274\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "854:\tlearn: 0.0232732\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "855:\tlearn: 0.0232282\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "856:\tlearn: 0.0231842\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "857:\tlearn: 0.0231369\ttotal: 1m 13s\tremaining: 2m 36s\n",
            "858:\tlearn: 0.0230686\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "859:\tlearn: 0.0230368\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "860:\tlearn: 0.0229866\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "861:\tlearn: 0.0229524\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "862:\tlearn: 0.0228922\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "863:\tlearn: 0.0228297\ttotal: 1m 13s\tremaining: 2m 35s\n",
            "864:\tlearn: 0.0227777\ttotal: 1m 14s\tremaining: 2m 35s\n",
            "865:\tlearn: 0.0227504\ttotal: 1m 14s\tremaining: 2m 35s\n",
            "866:\tlearn: 0.0227041\ttotal: 1m 14s\tremaining: 2m 35s\n",
            "867:\tlearn: 0.0226707\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "868:\tlearn: 0.0226271\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "869:\tlearn: 0.0225746\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "870:\tlearn: 0.0225399\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "871:\tlearn: 0.0225059\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "872:\tlearn: 0.0224280\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "873:\tlearn: 0.0223826\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "874:\tlearn: 0.0223422\ttotal: 1m 14s\tremaining: 2m 34s\n",
            "875:\tlearn: 0.0222901\ttotal: 1m 14s\tremaining: 2m 33s\n",
            "876:\tlearn: 0.0222414\ttotal: 1m 14s\tremaining: 2m 33s\n",
            "877:\tlearn: 0.0222079\ttotal: 1m 14s\tremaining: 2m 33s\n",
            "878:\tlearn: 0.0221639\ttotal: 1m 14s\tremaining: 2m 33s\n",
            "879:\tlearn: 0.0221056\ttotal: 1m 15s\tremaining: 2m 33s\n",
            "880:\tlearn: 0.0220564\ttotal: 1m 15s\tremaining: 2m 33s\n",
            "881:\tlearn: 0.0220014\ttotal: 1m 15s\tremaining: 2m 33s\n",
            "882:\tlearn: 0.0219481\ttotal: 1m 15s\tremaining: 2m 33s\n",
            "883:\tlearn: 0.0219125\ttotal: 1m 15s\tremaining: 2m 33s\n",
            "884:\tlearn: 0.0218926\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "885:\tlearn: 0.0218608\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "886:\tlearn: 0.0218251\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "887:\tlearn: 0.0217857\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "888:\tlearn: 0.0217425\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "889:\tlearn: 0.0217248\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "890:\tlearn: 0.0216274\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "891:\tlearn: 0.0215835\ttotal: 1m 15s\tremaining: 2m 32s\n",
            "892:\tlearn: 0.0215346\ttotal: 1m 16s\tremaining: 2m 32s\n",
            "893:\tlearn: 0.0215125\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "894:\tlearn: 0.0214925\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "895:\tlearn: 0.0214480\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "896:\tlearn: 0.0214030\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "897:\tlearn: 0.0213465\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "898:\tlearn: 0.0213234\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "899:\tlearn: 0.0213002\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "900:\tlearn: 0.0212241\ttotal: 1m 16s\tremaining: 2m 31s\n",
            "901:\tlearn: 0.0212061\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "902:\tlearn: 0.0211672\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "903:\tlearn: 0.0211359\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "904:\tlearn: 0.0210944\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "905:\tlearn: 0.0210689\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "906:\tlearn: 0.0210261\ttotal: 1m 16s\tremaining: 2m 30s\n",
            "907:\tlearn: 0.0209850\ttotal: 1m 17s\tremaining: 2m 30s\n",
            "908:\tlearn: 0.0209578\ttotal: 1m 17s\tremaining: 2m 30s\n",
            "909:\tlearn: 0.0209197\ttotal: 1m 17s\tremaining: 2m 30s\n",
            "910:\tlearn: 0.0208848\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "911:\tlearn: 0.0208437\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "912:\tlearn: 0.0208027\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "913:\tlearn: 0.0207777\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "914:\tlearn: 0.0207577\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "915:\tlearn: 0.0207398\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "916:\tlearn: 0.0206948\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "917:\tlearn: 0.0206330\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "918:\tlearn: 0.0206054\ttotal: 1m 17s\tremaining: 2m 29s\n",
            "919:\tlearn: 0.0205757\ttotal: 1m 17s\tremaining: 2m 28s\n",
            "920:\tlearn: 0.0205313\ttotal: 1m 17s\tremaining: 2m 28s\n",
            "921:\tlearn: 0.0204920\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "922:\tlearn: 0.0204425\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "923:\tlearn: 0.0204186\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "924:\tlearn: 0.0203832\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "925:\tlearn: 0.0203572\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "926:\tlearn: 0.0203401\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "927:\tlearn: 0.0203036\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "928:\tlearn: 0.0202719\ttotal: 1m 18s\tremaining: 2m 28s\n",
            "929:\tlearn: 0.0202424\ttotal: 1m 18s\tremaining: 2m 27s\n",
            "930:\tlearn: 0.0202011\ttotal: 1m 18s\tremaining: 2m 27s\n",
            "931:\tlearn: 0.0201620\ttotal: 1m 18s\tremaining: 2m 27s\n",
            "932:\tlearn: 0.0201296\ttotal: 1m 18s\tremaining: 2m 27s\n",
            "933:\tlearn: 0.0200816\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "934:\tlearn: 0.0200320\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "935:\tlearn: 0.0200008\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "936:\tlearn: 0.0199496\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "937:\tlearn: 0.0199097\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "938:\tlearn: 0.0198644\ttotal: 1m 19s\tremaining: 2m 26s\n",
            "939:\tlearn: 0.0198287\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "940:\tlearn: 0.0197963\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "941:\tlearn: 0.0197558\ttotal: 1m 19s\tremaining: 2m 27s\n",
            "942:\tlearn: 0.0196972\ttotal: 1m 20s\tremaining: 2m 27s\n",
            "943:\tlearn: 0.0196674\ttotal: 1m 20s\tremaining: 2m 27s\n",
            "944:\tlearn: 0.0196462\ttotal: 1m 20s\tremaining: 2m 27s\n",
            "945:\tlearn: 0.0196078\ttotal: 1m 20s\tremaining: 2m 27s\n",
            "946:\tlearn: 0.0195428\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "947:\tlearn: 0.0195239\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "948:\tlearn: 0.0194791\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "949:\tlearn: 0.0194339\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "950:\tlearn: 0.0193939\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "951:\tlearn: 0.0193583\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "952:\tlearn: 0.0193080\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "953:\tlearn: 0.0192444\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "954:\tlearn: 0.0192170\ttotal: 1m 20s\tremaining: 2m 26s\n",
            "955:\tlearn: 0.0192010\ttotal: 1m 20s\tremaining: 2m 25s\n",
            "956:\tlearn: 0.0191633\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "957:\tlearn: 0.0191290\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "958:\tlearn: 0.0190950\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "959:\tlearn: 0.0190649\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "960:\tlearn: 0.0190312\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "961:\tlearn: 0.0189874\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "962:\tlearn: 0.0189625\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "963:\tlearn: 0.0189137\ttotal: 1m 21s\tremaining: 2m 25s\n",
            "964:\tlearn: 0.0188704\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "965:\tlearn: 0.0188505\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "966:\tlearn: 0.0187976\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "967:\tlearn: 0.0187489\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "968:\tlearn: 0.0187216\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "969:\tlearn: 0.0186973\ttotal: 1m 21s\tremaining: 2m 24s\n",
            "970:\tlearn: 0.0186610\ttotal: 1m 22s\tremaining: 2m 24s\n",
            "971:\tlearn: 0.0186345\ttotal: 1m 22s\tremaining: 2m 24s\n",
            "972:\tlearn: 0.0186078\ttotal: 1m 22s\tremaining: 2m 24s\n",
            "973:\tlearn: 0.0185647\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "974:\tlearn: 0.0185534\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "975:\tlearn: 0.0185280\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "976:\tlearn: 0.0185000\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "977:\tlearn: 0.0184794\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "978:\tlearn: 0.0184357\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "979:\tlearn: 0.0184123\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "980:\tlearn: 0.0183788\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "981:\tlearn: 0.0183574\ttotal: 1m 22s\tremaining: 2m 23s\n",
            "982:\tlearn: 0.0183357\ttotal: 1m 22s\tremaining: 2m 22s\n",
            "983:\tlearn: 0.0183132\ttotal: 1m 22s\tremaining: 2m 22s\n",
            "984:\tlearn: 0.0182611\ttotal: 1m 22s\tremaining: 2m 22s\n",
            "985:\tlearn: 0.0182291\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "986:\tlearn: 0.0182083\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "987:\tlearn: 0.0181774\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "988:\tlearn: 0.0181478\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "989:\tlearn: 0.0181158\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "990:\tlearn: 0.0180862\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "991:\tlearn: 0.0180462\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "992:\tlearn: 0.0180132\ttotal: 1m 23s\tremaining: 2m 22s\n",
            "993:\tlearn: 0.0179848\ttotal: 1m 23s\tremaining: 2m 21s\n",
            "994:\tlearn: 0.0179528\ttotal: 1m 23s\tremaining: 2m 21s\n",
            "995:\tlearn: 0.0179308\ttotal: 1m 23s\tremaining: 2m 21s\n",
            "996:\tlearn: 0.0178981\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "997:\tlearn: 0.0178723\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "998:\tlearn: 0.0178421\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "999:\tlearn: 0.0178120\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "1000:\tlearn: 0.0177783\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "1001:\tlearn: 0.0177578\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "1002:\tlearn: 0.0177289\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "1003:\tlearn: 0.0176889\ttotal: 1m 24s\tremaining: 2m 21s\n",
            "1004:\tlearn: 0.0176522\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1005:\tlearn: 0.0176206\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1006:\tlearn: 0.0175974\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1007:\tlearn: 0.0175719\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1008:\tlearn: 0.0175444\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1009:\tlearn: 0.0175078\ttotal: 1m 24s\tremaining: 2m 20s\n",
            "1010:\tlearn: 0.0174496\ttotal: 1m 25s\tremaining: 2m 20s\n",
            "1011:\tlearn: 0.0174077\ttotal: 1m 25s\tremaining: 2m 20s\n",
            "1012:\tlearn: 0.0173828\ttotal: 1m 25s\tremaining: 2m 20s\n",
            "1013:\tlearn: 0.0173547\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1014:\tlearn: 0.0173278\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1015:\tlearn: 0.0172914\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1016:\tlearn: 0.0172655\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1017:\tlearn: 0.0172182\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1018:\tlearn: 0.0171809\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1019:\tlearn: 0.0171642\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1020:\tlearn: 0.0171276\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1021:\tlearn: 0.0171052\ttotal: 1m 25s\tremaining: 2m 19s\n",
            "1022:\tlearn: 0.0170854\ttotal: 1m 25s\tremaining: 2m 18s\n",
            "1023:\tlearn: 0.0170453\ttotal: 1m 25s\tremaining: 2m 18s\n",
            "1024:\tlearn: 0.0170077\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1025:\tlearn: 0.0169869\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1026:\tlearn: 0.0169602\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1027:\tlearn: 0.0169394\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1028:\tlearn: 0.0169053\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1029:\tlearn: 0.0168557\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1030:\tlearn: 0.0168365\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1031:\tlearn: 0.0168086\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1032:\tlearn: 0.0167821\ttotal: 1m 26s\tremaining: 2m 18s\n",
            "1033:\tlearn: 0.0167532\ttotal: 1m 26s\tremaining: 2m 17s\n",
            "1034:\tlearn: 0.0167298\ttotal: 1m 26s\tremaining: 2m 17s\n",
            "1035:\tlearn: 0.0166992\ttotal: 1m 26s\tremaining: 2m 17s\n",
            "1036:\tlearn: 0.0166635\ttotal: 1m 26s\tremaining: 2m 17s\n",
            "1037:\tlearn: 0.0166476\ttotal: 1m 27s\tremaining: 2m 17s\n",
            "1038:\tlearn: 0.0166336\ttotal: 1m 27s\tremaining: 2m 17s\n",
            "1039:\tlearn: 0.0166265\ttotal: 1m 27s\tremaining: 2m 17s\n",
            "1040:\tlearn: 0.0165848\ttotal: 1m 27s\tremaining: 2m 17s\n",
            "1041:\tlearn: 0.0165601\ttotal: 1m 27s\tremaining: 2m 17s\n",
            "1042:\tlearn: 0.0165217\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1043:\tlearn: 0.0164943\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1044:\tlearn: 0.0164562\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1045:\tlearn: 0.0164343\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1046:\tlearn: 0.0163989\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1047:\tlearn: 0.0163770\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1048:\tlearn: 0.0163479\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1049:\tlearn: 0.0163203\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1050:\tlearn: 0.0162987\ttotal: 1m 27s\tremaining: 2m 16s\n",
            "1051:\tlearn: 0.0162763\ttotal: 1m 27s\tremaining: 2m 15s\n",
            "1052:\tlearn: 0.0162226\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1053:\tlearn: 0.0162038\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1054:\tlearn: 0.0161807\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1055:\tlearn: 0.0161544\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1056:\tlearn: 0.0161410\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1057:\tlearn: 0.0161067\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1058:\tlearn: 0.0160603\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1059:\tlearn: 0.0160152\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1060:\tlearn: 0.0159943\ttotal: 1m 28s\tremaining: 2m 15s\n",
            "1061:\tlearn: 0.0159624\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1062:\tlearn: 0.0159404\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1063:\tlearn: 0.0159134\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1064:\tlearn: 0.0158988\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1065:\tlearn: 0.0158751\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1066:\tlearn: 0.0158351\ttotal: 1m 28s\tremaining: 2m 14s\n",
            "1067:\tlearn: 0.0158144\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1068:\tlearn: 0.0157914\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1069:\tlearn: 0.0157746\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1070:\tlearn: 0.0157461\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1071:\tlearn: 0.0157322\ttotal: 1m 29s\tremaining: 2m 14s\n",
            "1072:\tlearn: 0.0156938\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1073:\tlearn: 0.0156638\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1074:\tlearn: 0.0156511\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1075:\tlearn: 0.0156275\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1076:\tlearn: 0.0155891\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1077:\tlearn: 0.0155663\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1078:\tlearn: 0.0155445\ttotal: 1m 29s\tremaining: 2m 13s\n",
            "1079:\tlearn: 0.0155043\ttotal: 1m 30s\tremaining: 2m 13s\n",
            "1080:\tlearn: 0.0154710\ttotal: 1m 30s\tremaining: 2m 13s\n",
            "1081:\tlearn: 0.0154479\ttotal: 1m 30s\tremaining: 2m 13s\n",
            "1082:\tlearn: 0.0154331\ttotal: 1m 30s\tremaining: 2m 13s\n",
            "1083:\tlearn: 0.0154221\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1084:\tlearn: 0.0153975\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1085:\tlearn: 0.0153785\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1086:\tlearn: 0.0153580\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1087:\tlearn: 0.0153229\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1088:\tlearn: 0.0153076\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1089:\tlearn: 0.0152795\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1090:\tlearn: 0.0152497\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1091:\tlearn: 0.0152396\ttotal: 1m 30s\tremaining: 2m 12s\n",
            "1092:\tlearn: 0.0152110\ttotal: 1m 30s\tremaining: 2m 11s\n",
            "1093:\tlearn: 0.0151777\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1094:\tlearn: 0.0151645\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1095:\tlearn: 0.0151287\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1096:\tlearn: 0.0150816\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1097:\tlearn: 0.0150635\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1098:\tlearn: 0.0150360\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1099:\tlearn: 0.0150035\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1100:\tlearn: 0.0149886\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1101:\tlearn: 0.0149607\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1102:\tlearn: 0.0149399\ttotal: 1m 31s\tremaining: 2m 11s\n",
            "1103:\tlearn: 0.0149189\ttotal: 1m 31s\tremaining: 2m 10s\n",
            "1104:\tlearn: 0.0148973\ttotal: 1m 31s\tremaining: 2m 10s\n",
            "1105:\tlearn: 0.0148653\ttotal: 1m 31s\tremaining: 2m 10s\n",
            "1106:\tlearn: 0.0148520\ttotal: 1m 31s\tremaining: 2m 10s\n",
            "1107:\tlearn: 0.0148339\ttotal: 1m 32s\tremaining: 2m 10s\n",
            "1108:\tlearn: 0.0148172\ttotal: 1m 32s\tremaining: 2m 10s\n",
            "1109:\tlearn: 0.0147948\ttotal: 1m 32s\tremaining: 2m 10s\n",
            "1110:\tlearn: 0.0147651\ttotal: 1m 32s\tremaining: 2m 10s\n",
            "1111:\tlearn: 0.0147413\ttotal: 1m 32s\tremaining: 2m 10s\n",
            "1112:\tlearn: 0.0147178\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1113:\tlearn: 0.0147073\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1114:\tlearn: 0.0146843\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1115:\tlearn: 0.0146483\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1116:\tlearn: 0.0146261\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1117:\tlearn: 0.0145910\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1118:\tlearn: 0.0145638\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1119:\tlearn: 0.0145525\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1120:\tlearn: 0.0145235\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1121:\tlearn: 0.0144955\ttotal: 1m 32s\tremaining: 2m 9s\n",
            "1122:\tlearn: 0.0144478\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1123:\tlearn: 0.0144377\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1124:\tlearn: 0.0144281\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1125:\tlearn: 0.0143979\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1126:\tlearn: 0.0143624\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1127:\tlearn: 0.0143458\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1128:\tlearn: 0.0143392\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1129:\tlearn: 0.0143048\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1130:\tlearn: 0.0142724\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1131:\tlearn: 0.0142397\ttotal: 1m 33s\tremaining: 2m 8s\n",
            "1132:\tlearn: 0.0142224\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1133:\tlearn: 0.0142021\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1134:\tlearn: 0.0141730\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1135:\tlearn: 0.0141453\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1136:\tlearn: 0.0141250\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1137:\tlearn: 0.0140992\ttotal: 1m 34s\tremaining: 2m 8s\n",
            "1138:\tlearn: 0.0140828\ttotal: 1m 34s\tremaining: 2m 7s\n",
            "1139:\tlearn: 0.0140665\ttotal: 1m 34s\tremaining: 2m 7s\n",
            "1140:\tlearn: 0.0140460\ttotal: 1m 34s\tremaining: 2m 7s\n",
            "1141:\tlearn: 0.0140267\ttotal: 1m 34s\tremaining: 2m 7s\n",
            "1142:\tlearn: 0.0140032\ttotal: 1m 34s\tremaining: 2m 7s\n",
            "1143:\tlearn: 0.0139970\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1144:\tlearn: 0.0139854\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1145:\tlearn: 0.0139650\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1146:\tlearn: 0.0139385\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1147:\tlearn: 0.0139113\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1148:\tlearn: 0.0138831\ttotal: 1m 35s\tremaining: 2m 7s\n",
            "1149:\tlearn: 0.0138640\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1150:\tlearn: 0.0138246\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1151:\tlearn: 0.0138074\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1152:\tlearn: 0.0137744\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1153:\tlearn: 0.0137532\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1154:\tlearn: 0.0137259\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1155:\tlearn: 0.0136991\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1156:\tlearn: 0.0136618\ttotal: 1m 35s\tremaining: 2m 6s\n",
            "1157:\tlearn: 0.0136261\ttotal: 1m 36s\tremaining: 2m 6s\n",
            "1158:\tlearn: 0.0136105\ttotal: 1m 36s\tremaining: 2m 6s\n",
            "1159:\tlearn: 0.0135699\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1160:\tlearn: 0.0135553\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1161:\tlearn: 0.0135451\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1162:\tlearn: 0.0135246\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1163:\tlearn: 0.0135141\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1164:\tlearn: 0.0134951\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1165:\tlearn: 0.0134743\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1166:\tlearn: 0.0134505\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1167:\tlearn: 0.0134317\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1168:\tlearn: 0.0134131\ttotal: 1m 36s\tremaining: 2m 5s\n",
            "1169:\tlearn: 0.0133948\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1170:\tlearn: 0.0133800\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1171:\tlearn: 0.0133649\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1172:\tlearn: 0.0133449\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1173:\tlearn: 0.0133243\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1174:\tlearn: 0.0133058\ttotal: 1m 37s\tremaining: 2m 5s\n",
            "1175:\tlearn: 0.0132797\ttotal: 1m 37s\tremaining: 2m 4s\n",
            "1176:\tlearn: 0.0132605\ttotal: 1m 37s\tremaining: 2m 4s\n",
            "1177:\tlearn: 0.0132312\ttotal: 1m 37s\tremaining: 2m 4s\n",
            "1178:\tlearn: 0.0132141\ttotal: 1m 37s\tremaining: 2m 4s\n",
            "1179:\tlearn: 0.0131906\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1180:\tlearn: 0.0131634\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1181:\tlearn: 0.0131374\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1182:\tlearn: 0.0131192\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1183:\tlearn: 0.0130965\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1184:\tlearn: 0.0130714\ttotal: 1m 38s\tremaining: 2m 4s\n",
            "1185:\tlearn: 0.0130417\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1186:\tlearn: 0.0130137\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1187:\tlearn: 0.0129893\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1188:\tlearn: 0.0129715\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1189:\tlearn: 0.0129620\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1190:\tlearn: 0.0129422\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1191:\tlearn: 0.0129257\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1192:\tlearn: 0.0129069\ttotal: 1m 38s\tremaining: 2m 3s\n",
            "1193:\tlearn: 0.0128963\ttotal: 1m 39s\tremaining: 2m 3s\n",
            "1194:\tlearn: 0.0128807\ttotal: 1m 39s\tremaining: 2m 3s\n",
            "1195:\tlearn: 0.0128644\ttotal: 1m 39s\tremaining: 2m 3s\n",
            "1196:\tlearn: 0.0128534\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1197:\tlearn: 0.0128404\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1198:\tlearn: 0.0128279\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1199:\tlearn: 0.0128149\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1200:\tlearn: 0.0127980\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1201:\tlearn: 0.0127787\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1202:\tlearn: 0.0127629\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1203:\tlearn: 0.0127512\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1204:\tlearn: 0.0127290\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1205:\tlearn: 0.0127153\ttotal: 1m 39s\tremaining: 2m 2s\n",
            "1206:\tlearn: 0.0126943\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1207:\tlearn: 0.0126845\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1208:\tlearn: 0.0126620\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1209:\tlearn: 0.0126523\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1210:\tlearn: 0.0126337\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1211:\tlearn: 0.0126223\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1212:\tlearn: 0.0125988\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1213:\tlearn: 0.0125888\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1214:\tlearn: 0.0125766\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1215:\tlearn: 0.0125665\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1216:\tlearn: 0.0125398\ttotal: 1m 40s\tremaining: 2m 1s\n",
            "1217:\tlearn: 0.0125150\ttotal: 1m 40s\tremaining: 2m\n",
            "1218:\tlearn: 0.0124949\ttotal: 1m 40s\tremaining: 2m\n",
            "1219:\tlearn: 0.0124790\ttotal: 1m 40s\tremaining: 2m\n",
            "1220:\tlearn: 0.0124529\ttotal: 1m 41s\tremaining: 2m\n",
            "1221:\tlearn: 0.0124208\ttotal: 1m 41s\tremaining: 2m\n",
            "1222:\tlearn: 0.0123992\ttotal: 1m 41s\tremaining: 2m\n",
            "1223:\tlearn: 0.0123817\ttotal: 1m 41s\tremaining: 2m\n",
            "1224:\tlearn: 0.0123666\ttotal: 1m 41s\tremaining: 2m\n",
            "1225:\tlearn: 0.0123489\ttotal: 1m 41s\tremaining: 2m\n",
            "1226:\tlearn: 0.0123265\ttotal: 1m 41s\tremaining: 2m\n",
            "1227:\tlearn: 0.0123125\ttotal: 1m 41s\tremaining: 2m\n",
            "1228:\tlearn: 0.0122766\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1229:\tlearn: 0.0122553\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1230:\tlearn: 0.0122400\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1231:\tlearn: 0.0122118\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1232:\tlearn: 0.0121972\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1233:\tlearn: 0.0121741\ttotal: 1m 41s\tremaining: 1m 59s\n",
            "1234:\tlearn: 0.0121448\ttotal: 1m 42s\tremaining: 1m 59s\n",
            "1235:\tlearn: 0.0121278\ttotal: 1m 42s\tremaining: 1m 59s\n",
            "1236:\tlearn: 0.0121169\ttotal: 1m 42s\tremaining: 1m 59s\n",
            "1237:\tlearn: 0.0120966\ttotal: 1m 42s\tremaining: 1m 59s\n",
            "1238:\tlearn: 0.0120817\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1239:\tlearn: 0.0120617\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1240:\tlearn: 0.0120527\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1241:\tlearn: 0.0120329\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1242:\tlearn: 0.0120164\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1243:\tlearn: 0.0120023\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1244:\tlearn: 0.0119846\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1245:\tlearn: 0.0119699\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1246:\tlearn: 0.0119518\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1247:\tlearn: 0.0119302\ttotal: 1m 42s\tremaining: 1m 58s\n",
            "1248:\tlearn: 0.0119008\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1249:\tlearn: 0.0118874\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1250:\tlearn: 0.0118735\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1251:\tlearn: 0.0118655\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1252:\tlearn: 0.0118526\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1253:\tlearn: 0.0118317\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1254:\tlearn: 0.0118094\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1255:\tlearn: 0.0117980\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1256:\tlearn: 0.0117807\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1257:\tlearn: 0.0117619\ttotal: 1m 43s\tremaining: 1m 57s\n",
            "1258:\tlearn: 0.0117481\ttotal: 1m 43s\tremaining: 1m 56s\n",
            "1259:\tlearn: 0.0117263\ttotal: 1m 43s\tremaining: 1m 56s\n",
            "1260:\tlearn: 0.0117003\ttotal: 1m 43s\tremaining: 1m 56s\n",
            "1261:\tlearn: 0.0116860\ttotal: 1m 43s\tremaining: 1m 56s\n",
            "1262:\tlearn: 0.0116571\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1263:\tlearn: 0.0116405\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1264:\tlearn: 0.0116260\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1265:\tlearn: 0.0115996\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1266:\tlearn: 0.0115808\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1267:\tlearn: 0.0115573\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1268:\tlearn: 0.0115421\ttotal: 1m 44s\tremaining: 1m 56s\n",
            "1269:\tlearn: 0.0115178\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1270:\tlearn: 0.0114953\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1271:\tlearn: 0.0114696\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1272:\tlearn: 0.0114598\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1273:\tlearn: 0.0114333\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1274:\tlearn: 0.0114180\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1275:\tlearn: 0.0113976\ttotal: 1m 44s\tremaining: 1m 55s\n",
            "1276:\tlearn: 0.0113843\ttotal: 1m 45s\tremaining: 1m 55s\n",
            "1277:\tlearn: 0.0113738\ttotal: 1m 45s\tremaining: 1m 55s\n",
            "1278:\tlearn: 0.0113573\ttotal: 1m 45s\tremaining: 1m 55s\n",
            "1279:\tlearn: 0.0113507\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1280:\tlearn: 0.0113366\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1281:\tlearn: 0.0113127\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1282:\tlearn: 0.0113040\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1283:\tlearn: 0.0112904\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1284:\tlearn: 0.0112808\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1285:\tlearn: 0.0112724\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1286:\tlearn: 0.0112556\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1287:\tlearn: 0.0112443\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1288:\tlearn: 0.0112332\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1289:\tlearn: 0.0112073\ttotal: 1m 45s\tremaining: 1m 54s\n",
            "1290:\tlearn: 0.0111886\ttotal: 1m 45s\tremaining: 1m 53s\n",
            "1291:\tlearn: 0.0111785\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1292:\tlearn: 0.0111605\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1293:\tlearn: 0.0111398\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1294:\tlearn: 0.0111296\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1295:\tlearn: 0.0111202\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1296:\tlearn: 0.0111088\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1297:\tlearn: 0.0110934\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1298:\tlearn: 0.0110791\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1299:\tlearn: 0.0110657\ttotal: 1m 46s\tremaining: 1m 53s\n",
            "1300:\tlearn: 0.0110473\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "1301:\tlearn: 0.0110345\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "1302:\tlearn: 0.0110227\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "1303:\tlearn: 0.0109948\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "1304:\tlearn: 0.0109870\ttotal: 1m 46s\tremaining: 1m 52s\n",
            "1305:\tlearn: 0.0109772\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1306:\tlearn: 0.0109672\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1307:\tlearn: 0.0109454\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1308:\tlearn: 0.0109323\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1309:\tlearn: 0.0109191\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1310:\tlearn: 0.0109118\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1311:\tlearn: 0.0108974\ttotal: 1m 47s\tremaining: 1m 52s\n",
            "1312:\tlearn: 0.0108870\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1313:\tlearn: 0.0108735\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1314:\tlearn: 0.0108484\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1315:\tlearn: 0.0108412\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1316:\tlearn: 0.0108272\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1317:\tlearn: 0.0108098\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1318:\tlearn: 0.0108015\ttotal: 1m 47s\tremaining: 1m 51s\n",
            "1319:\tlearn: 0.0107802\ttotal: 1m 48s\tremaining: 1m 51s\n",
            "1320:\tlearn: 0.0107662\ttotal: 1m 48s\tremaining: 1m 51s\n",
            "1321:\tlearn: 0.0107386\ttotal: 1m 48s\tremaining: 1m 51s\n",
            "1322:\tlearn: 0.0107222\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1323:\tlearn: 0.0107184\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1324:\tlearn: 0.0107077\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1325:\tlearn: 0.0106944\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1326:\tlearn: 0.0106848\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1327:\tlearn: 0.0106645\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1328:\tlearn: 0.0106578\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1329:\tlearn: 0.0106409\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1330:\tlearn: 0.0106252\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1331:\tlearn: 0.0106094\ttotal: 1m 48s\tremaining: 1m 50s\n",
            "1332:\tlearn: 0.0105919\ttotal: 1m 48s\tremaining: 1m 49s\n",
            "1333:\tlearn: 0.0105807\ttotal: 1m 48s\tremaining: 1m 49s\n",
            "1334:\tlearn: 0.0105668\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1335:\tlearn: 0.0105396\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1336:\tlearn: 0.0105313\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1337:\tlearn: 0.0105248\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1338:\tlearn: 0.0105072\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1339:\tlearn: 0.0104935\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1340:\tlearn: 0.0104863\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1341:\tlearn: 0.0104722\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1342:\tlearn: 0.0104575\ttotal: 1m 49s\tremaining: 1m 49s\n",
            "1343:\tlearn: 0.0104532\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "1344:\tlearn: 0.0104457\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "1345:\tlearn: 0.0104264\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "1346:\tlearn: 0.0104092\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "1347:\tlearn: 0.0103972\ttotal: 1m 49s\tremaining: 1m 48s\n",
            "1348:\tlearn: 0.0103902\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1349:\tlearn: 0.0103737\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1350:\tlearn: 0.0103632\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1351:\tlearn: 0.0103481\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1352:\tlearn: 0.0103394\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1353:\tlearn: 0.0103262\ttotal: 1m 50s\tremaining: 1m 48s\n",
            "1354:\tlearn: 0.0103124\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1355:\tlearn: 0.0103000\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1356:\tlearn: 0.0102911\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1357:\tlearn: 0.0102724\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1358:\tlearn: 0.0102533\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1359:\tlearn: 0.0102288\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1360:\tlearn: 0.0102073\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1361:\tlearn: 0.0101937\ttotal: 1m 50s\tremaining: 1m 47s\n",
            "1362:\tlearn: 0.0101857\ttotal: 1m 51s\tremaining: 1m 47s\n",
            "1363:\tlearn: 0.0101774\ttotal: 1m 51s\tremaining: 1m 47s\n",
            "1364:\tlearn: 0.0101681\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1365:\tlearn: 0.0101548\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1366:\tlearn: 0.0101471\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1367:\tlearn: 0.0101285\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1368:\tlearn: 0.0101128\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1369:\tlearn: 0.0100990\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1370:\tlearn: 0.0100785\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1371:\tlearn: 0.0100623\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1372:\tlearn: 0.0100429\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1373:\tlearn: 0.0100182\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1374:\tlearn: 0.0099973\ttotal: 1m 51s\tremaining: 1m 46s\n",
            "1375:\tlearn: 0.0099803\ttotal: 1m 51s\tremaining: 1m 45s\n",
            "1376:\tlearn: 0.0099703\ttotal: 1m 51s\tremaining: 1m 45s\n",
            "1377:\tlearn: 0.0099619\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1378:\tlearn: 0.0099434\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1379:\tlearn: 0.0099320\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1380:\tlearn: 0.0099154\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1381:\tlearn: 0.0098996\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1382:\tlearn: 0.0098822\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1383:\tlearn: 0.0098694\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1384:\tlearn: 0.0098579\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1385:\tlearn: 0.0098511\ttotal: 1m 52s\tremaining: 1m 45s\n",
            "1386:\tlearn: 0.0098354\ttotal: 1m 52s\tremaining: 1m 44s\n",
            "1387:\tlearn: 0.0098194\ttotal: 1m 52s\tremaining: 1m 44s\n",
            "1388:\tlearn: 0.0097999\ttotal: 1m 52s\tremaining: 1m 44s\n",
            "1389:\tlearn: 0.0097872\ttotal: 1m 52s\tremaining: 1m 44s\n",
            "1390:\tlearn: 0.0097753\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1391:\tlearn: 0.0097587\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1392:\tlearn: 0.0097479\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1393:\tlearn: 0.0097294\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1394:\tlearn: 0.0097183\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1395:\tlearn: 0.0096999\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1396:\tlearn: 0.0096823\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1397:\tlearn: 0.0096684\ttotal: 1m 53s\tremaining: 1m 44s\n",
            "1398:\tlearn: 0.0096621\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1399:\tlearn: 0.0096513\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1400:\tlearn: 0.0096432\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1401:\tlearn: 0.0096329\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1402:\tlearn: 0.0096207\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1403:\tlearn: 0.0096061\ttotal: 1m 53s\tremaining: 1m 43s\n",
            "1404:\tlearn: 0.0095915\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "1405:\tlearn: 0.0095766\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "1406:\tlearn: 0.0095578\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "1407:\tlearn: 0.0095414\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "1408:\tlearn: 0.0095316\ttotal: 1m 54s\tremaining: 1m 43s\n",
            "1409:\tlearn: 0.0095243\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1410:\tlearn: 0.0095150\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1411:\tlearn: 0.0095062\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1412:\tlearn: 0.0094914\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1413:\tlearn: 0.0094794\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1414:\tlearn: 0.0094590\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1415:\tlearn: 0.0094393\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1416:\tlearn: 0.0094269\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1417:\tlearn: 0.0094174\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1418:\tlearn: 0.0094071\ttotal: 1m 54s\tremaining: 1m 42s\n",
            "1419:\tlearn: 0.0093964\ttotal: 1m 55s\tremaining: 1m 42s\n",
            "1420:\tlearn: 0.0093823\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1421:\tlearn: 0.0093697\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1422:\tlearn: 0.0093596\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1423:\tlearn: 0.0093521\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1424:\tlearn: 0.0093332\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1425:\tlearn: 0.0093136\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1426:\tlearn: 0.0093005\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1427:\tlearn: 0.0092863\ttotal: 1m 55s\tremaining: 1m 41s\n",
            "1428:\tlearn: 0.0092679\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1429:\tlearn: 0.0092613\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1430:\tlearn: 0.0092480\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1431:\tlearn: 0.0092424\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1432:\tlearn: 0.0092291\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1433:\tlearn: 0.0092144\ttotal: 1m 56s\tremaining: 1m 41s\n",
            "1434:\tlearn: 0.0092017\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1435:\tlearn: 0.0091925\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1436:\tlearn: 0.0091751\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1437:\tlearn: 0.0091613\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1438:\tlearn: 0.0091422\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1439:\tlearn: 0.0091259\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1440:\tlearn: 0.0091118\ttotal: 1m 56s\tremaining: 1m 40s\n",
            "1441:\tlearn: 0.0091046\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "1442:\tlearn: 0.0090911\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "1443:\tlearn: 0.0090794\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "1444:\tlearn: 0.0090709\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "1445:\tlearn: 0.0090580\ttotal: 1m 57s\tremaining: 1m 40s\n",
            "1446:\tlearn: 0.0090295\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1447:\tlearn: 0.0090189\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1448:\tlearn: 0.0090070\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1449:\tlearn: 0.0089967\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1450:\tlearn: 0.0089899\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1451:\tlearn: 0.0089691\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1452:\tlearn: 0.0089606\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1453:\tlearn: 0.0089455\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1454:\tlearn: 0.0089321\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1455:\tlearn: 0.0089153\ttotal: 1m 57s\tremaining: 1m 39s\n",
            "1456:\tlearn: 0.0088891\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1457:\tlearn: 0.0088816\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1458:\tlearn: 0.0088668\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1459:\tlearn: 0.0088556\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1460:\tlearn: 0.0088430\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1461:\tlearn: 0.0088368\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1462:\tlearn: 0.0088203\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1463:\tlearn: 0.0087998\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1464:\tlearn: 0.0087860\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1465:\tlearn: 0.0087774\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1466:\tlearn: 0.0087590\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1467:\tlearn: 0.0087531\ttotal: 1m 58s\tremaining: 1m 38s\n",
            "1468:\tlearn: 0.0087487\ttotal: 1m 58s\tremaining: 1m 37s\n",
            "1469:\tlearn: 0.0087328\ttotal: 1m 58s\tremaining: 1m 37s\n",
            "1470:\tlearn: 0.0087270\ttotal: 1m 58s\tremaining: 1m 37s\n",
            "1471:\tlearn: 0.0087239\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1472:\tlearn: 0.0087163\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1473:\tlearn: 0.0087028\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1474:\tlearn: 0.0086787\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1475:\tlearn: 0.0086606\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1476:\tlearn: 0.0086521\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1477:\tlearn: 0.0086335\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1478:\tlearn: 0.0086215\ttotal: 1m 59s\tremaining: 1m 37s\n",
            "1479:\tlearn: 0.0086156\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1480:\tlearn: 0.0086009\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1481:\tlearn: 0.0085899\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1482:\tlearn: 0.0085810\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1483:\tlearn: 0.0085768\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1484:\tlearn: 0.0085668\ttotal: 1m 59s\tremaining: 1m 36s\n",
            "1485:\tlearn: 0.0085572\ttotal: 2m\tremaining: 1m 36s\n",
            "1486:\tlearn: 0.0085526\ttotal: 2m\tremaining: 1m 36s\n",
            "1487:\tlearn: 0.0085408\ttotal: 2m\tremaining: 1m 36s\n",
            "1488:\tlearn: 0.0085250\ttotal: 2m\tremaining: 1m 36s\n",
            "1489:\tlearn: 0.0085172\ttotal: 2m\tremaining: 1m 36s\n",
            "1490:\tlearn: 0.0085055\ttotal: 2m\tremaining: 1m 35s\n",
            "1491:\tlearn: 0.0084923\ttotal: 2m\tremaining: 1m 35s\n",
            "1492:\tlearn: 0.0084830\ttotal: 2m\tremaining: 1m 35s\n",
            "1493:\tlearn: 0.0084701\ttotal: 2m\tremaining: 1m 35s\n",
            "1494:\tlearn: 0.0084588\ttotal: 2m\tremaining: 1m 35s\n",
            "1495:\tlearn: 0.0084502\ttotal: 2m\tremaining: 1m 35s\n",
            "1496:\tlearn: 0.0084354\ttotal: 2m\tremaining: 1m 35s\n",
            "1497:\tlearn: 0.0084254\ttotal: 2m\tremaining: 1m 35s\n",
            "1498:\tlearn: 0.0084212\ttotal: 2m\tremaining: 1m 35s\n",
            "1499:\tlearn: 0.0084096\ttotal: 2m 1s\tremaining: 1m 35s\n",
            "1500:\tlearn: 0.0084050\ttotal: 2m 1s\tremaining: 1m 35s\n",
            "1501:\tlearn: 0.0083922\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1502:\tlearn: 0.0083857\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1503:\tlearn: 0.0083723\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1504:\tlearn: 0.0083643\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1505:\tlearn: 0.0083563\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1506:\tlearn: 0.0083441\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1507:\tlearn: 0.0083353\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1508:\tlearn: 0.0083292\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1509:\tlearn: 0.0083226\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1510:\tlearn: 0.0083226\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1511:\tlearn: 0.0083077\ttotal: 2m 1s\tremaining: 1m 34s\n",
            "1512:\tlearn: 0.0083012\ttotal: 2m 1s\tremaining: 1m 33s\n",
            "1513:\tlearn: 0.0082945\ttotal: 2m 1s\tremaining: 1m 33s\n",
            "1514:\tlearn: 0.0082779\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1515:\tlearn: 0.0082707\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1516:\tlearn: 0.0082547\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1517:\tlearn: 0.0082433\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1518:\tlearn: 0.0082335\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1519:\tlearn: 0.0082263\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1520:\tlearn: 0.0082153\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1521:\tlearn: 0.0082036\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1522:\tlearn: 0.0081938\ttotal: 2m 2s\tremaining: 1m 33s\n",
            "1523:\tlearn: 0.0081885\ttotal: 2m 2s\tremaining: 1m 32s\n",
            "1524:\tlearn: 0.0081746\ttotal: 2m 2s\tremaining: 1m 32s\n",
            "1525:\tlearn: 0.0081568\ttotal: 2m 2s\tremaining: 1m 32s\n",
            "1526:\tlearn: 0.0081508\ttotal: 2m 2s\tremaining: 1m 32s\n",
            "1527:\tlearn: 0.0081322\ttotal: 2m 2s\tremaining: 1m 32s\n",
            "1528:\tlearn: 0.0081227\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1529:\tlearn: 0.0081180\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1530:\tlearn: 0.0081116\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1531:\tlearn: 0.0080951\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1532:\tlearn: 0.0080885\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1533:\tlearn: 0.0080837\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1534:\tlearn: 0.0080780\ttotal: 2m 3s\tremaining: 1m 32s\n",
            "1535:\tlearn: 0.0080726\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1536:\tlearn: 0.0080665\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1537:\tlearn: 0.0080624\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1538:\tlearn: 0.0080470\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1539:\tlearn: 0.0080394\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1540:\tlearn: 0.0080326\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1541:\tlearn: 0.0080239\ttotal: 2m 3s\tremaining: 1m 31s\n",
            "1542:\tlearn: 0.0080160\ttotal: 2m 4s\tremaining: 1m 31s\n",
            "1543:\tlearn: 0.0080096\ttotal: 2m 4s\tremaining: 1m 31s\n",
            "1544:\tlearn: 0.0079993\ttotal: 2m 4s\tremaining: 1m 31s\n",
            "1545:\tlearn: 0.0079910\ttotal: 2m 4s\tremaining: 1m 31s\n",
            "1546:\tlearn: 0.0079855\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1547:\tlearn: 0.0079695\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1548:\tlearn: 0.0079538\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1549:\tlearn: 0.0079486\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1550:\tlearn: 0.0079375\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1551:\tlearn: 0.0079296\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1552:\tlearn: 0.0079203\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1553:\tlearn: 0.0079128\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1554:\tlearn: 0.0079031\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1555:\tlearn: 0.0078953\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1556:\tlearn: 0.0078714\ttotal: 2m 4s\tremaining: 1m 30s\n",
            "1557:\tlearn: 0.0078639\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1558:\tlearn: 0.0078552\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1559:\tlearn: 0.0078438\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1560:\tlearn: 0.0078295\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1561:\tlearn: 0.0078245\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1562:\tlearn: 0.0078130\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1563:\tlearn: 0.0078014\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1564:\tlearn: 0.0077973\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1565:\tlearn: 0.0077893\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1566:\tlearn: 0.0077817\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1567:\tlearn: 0.0077757\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1568:\tlearn: 0.0077684\ttotal: 2m 5s\tremaining: 1m 29s\n",
            "1569:\tlearn: 0.0077517\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "1570:\tlearn: 0.0077452\ttotal: 2m 5s\tremaining: 1m 28s\n",
            "1571:\tlearn: 0.0077418\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1572:\tlearn: 0.0077271\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1573:\tlearn: 0.0077213\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1574:\tlearn: 0.0077111\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1575:\tlearn: 0.0076976\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1576:\tlearn: 0.0076868\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1577:\tlearn: 0.0076792\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1578:\tlearn: 0.0076656\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1579:\tlearn: 0.0076520\ttotal: 2m 6s\tremaining: 1m 28s\n",
            "1580:\tlearn: 0.0076454\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "1581:\tlearn: 0.0076313\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "1582:\tlearn: 0.0076274\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "1583:\tlearn: 0.0076212\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "1584:\tlearn: 0.0076163\ttotal: 2m 6s\tremaining: 1m 27s\n",
            "1585:\tlearn: 0.0076108\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1586:\tlearn: 0.0076019\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1587:\tlearn: 0.0075928\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1588:\tlearn: 0.0075757\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1589:\tlearn: 0.0075682\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1590:\tlearn: 0.0075613\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1591:\tlearn: 0.0075486\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1592:\tlearn: 0.0075403\ttotal: 2m 7s\tremaining: 1m 27s\n",
            "1593:\tlearn: 0.0075371\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "1594:\tlearn: 0.0075305\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "1595:\tlearn: 0.0075250\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "1596:\tlearn: 0.0075198\ttotal: 2m 7s\tremaining: 1m 26s\n",
            "1597:\tlearn: 0.0075130\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1598:\tlearn: 0.0075070\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1599:\tlearn: 0.0074966\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1600:\tlearn: 0.0074912\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1601:\tlearn: 0.0074850\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1602:\tlearn: 0.0074798\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1603:\tlearn: 0.0074746\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1604:\tlearn: 0.0074704\ttotal: 2m 8s\tremaining: 1m 26s\n",
            "1605:\tlearn: 0.0074608\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "1606:\tlearn: 0.0074495\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "1607:\tlearn: 0.0074438\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "1608:\tlearn: 0.0074313\ttotal: 2m 8s\tremaining: 1m 25s\n",
            "1609:\tlearn: 0.0074229\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1610:\tlearn: 0.0074148\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1611:\tlearn: 0.0074077\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1612:\tlearn: 0.0074024\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1613:\tlearn: 0.0073969\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1614:\tlearn: 0.0073905\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1615:\tlearn: 0.0073728\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1616:\tlearn: 0.0073649\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1617:\tlearn: 0.0073580\ttotal: 2m 9s\tremaining: 1m 25s\n",
            "1618:\tlearn: 0.0073503\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "1619:\tlearn: 0.0073423\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "1620:\tlearn: 0.0073317\ttotal: 2m 9s\tremaining: 1m 24s\n",
            "1621:\tlearn: 0.0073231\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1622:\tlearn: 0.0073176\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1623:\tlearn: 0.0073116\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1624:\tlearn: 0.0073037\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1625:\tlearn: 0.0072959\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1626:\tlearn: 0.0072857\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1627:\tlearn: 0.0072789\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1628:\tlearn: 0.0072730\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1629:\tlearn: 0.0072663\ttotal: 2m 10s\tremaining: 1m 24s\n",
            "1630:\tlearn: 0.0072607\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "1631:\tlearn: 0.0072533\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "1632:\tlearn: 0.0072373\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "1633:\tlearn: 0.0072334\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "1634:\tlearn: 0.0072246\ttotal: 2m 10s\tremaining: 1m 23s\n",
            "1635:\tlearn: 0.0072148\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1636:\tlearn: 0.0072052\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1637:\tlearn: 0.0071988\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1638:\tlearn: 0.0071914\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1639:\tlearn: 0.0071798\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1640:\tlearn: 0.0071648\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1641:\tlearn: 0.0071575\ttotal: 2m 11s\tremaining: 1m 23s\n",
            "1642:\tlearn: 0.0071523\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1643:\tlearn: 0.0071486\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1644:\tlearn: 0.0071406\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1645:\tlearn: 0.0071364\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1646:\tlearn: 0.0071284\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1647:\tlearn: 0.0071127\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1648:\tlearn: 0.0070995\ttotal: 2m 11s\tremaining: 1m 22s\n",
            "1649:\tlearn: 0.0070891\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1650:\tlearn: 0.0070823\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1651:\tlearn: 0.0070670\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1652:\tlearn: 0.0070582\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1653:\tlearn: 0.0070507\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1654:\tlearn: 0.0070368\ttotal: 2m 12s\tremaining: 1m 21s\n",
            "1655:\tlearn: 0.0070279\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1656:\tlearn: 0.0070217\ttotal: 2m 12s\tremaining: 1m 22s\n",
            "1657:\tlearn: 0.0070166\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1658:\tlearn: 0.0070081\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1659:\tlearn: 0.0070026\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1660:\tlearn: 0.0069985\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1661:\tlearn: 0.0069917\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1662:\tlearn: 0.0069878\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1663:\tlearn: 0.0069796\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1664:\tlearn: 0.0069727\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1665:\tlearn: 0.0069665\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1666:\tlearn: 0.0069567\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1667:\tlearn: 0.0069512\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1668:\tlearn: 0.0069420\ttotal: 2m 13s\tremaining: 1m 21s\n",
            "1669:\tlearn: 0.0069338\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1670:\tlearn: 0.0069283\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1671:\tlearn: 0.0069181\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1672:\tlearn: 0.0069126\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1673:\tlearn: 0.0069019\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1674:\tlearn: 0.0068962\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1675:\tlearn: 0.0068883\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1676:\tlearn: 0.0068824\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1677:\tlearn: 0.0068693\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1678:\tlearn: 0.0068541\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1679:\tlearn: 0.0068488\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1680:\tlearn: 0.0068385\ttotal: 2m 14s\tremaining: 1m 20s\n",
            "1681:\tlearn: 0.0068303\ttotal: 2m 14s\tremaining: 1m 19s\n",
            "1682:\tlearn: 0.0068163\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1683:\tlearn: 0.0068072\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1684:\tlearn: 0.0068029\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1685:\tlearn: 0.0067951\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1686:\tlearn: 0.0067897\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1687:\tlearn: 0.0067832\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1688:\tlearn: 0.0067778\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1689:\tlearn: 0.0067705\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1690:\tlearn: 0.0067628\ttotal: 2m 15s\tremaining: 1m 19s\n",
            "1691:\tlearn: 0.0067556\ttotal: 2m 16s\tremaining: 1m 19s\n",
            "1692:\tlearn: 0.0067515\ttotal: 2m 16s\tremaining: 1m 19s\n",
            "1693:\tlearn: 0.0067469\ttotal: 2m 16s\tremaining: 1m 19s\n",
            "1694:\tlearn: 0.0067355\ttotal: 2m 16s\tremaining: 1m 19s\n",
            "1695:\tlearn: 0.0067225\ttotal: 2m 16s\tremaining: 1m 19s\n",
            "1696:\tlearn: 0.0067192\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1697:\tlearn: 0.0067079\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1698:\tlearn: 0.0066989\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1699:\tlearn: 0.0066892\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1700:\tlearn: 0.0066818\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1701:\tlearn: 0.0066753\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1702:\tlearn: 0.0066676\ttotal: 2m 16s\tremaining: 1m 18s\n",
            "1703:\tlearn: 0.0066608\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1704:\tlearn: 0.0066543\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1705:\tlearn: 0.0066493\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1706:\tlearn: 0.0066342\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1707:\tlearn: 0.0066272\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1708:\tlearn: 0.0066228\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1709:\tlearn: 0.0066130\ttotal: 2m 17s\tremaining: 1m 18s\n",
            "1710:\tlearn: 0.0066029\ttotal: 2m 17s\tremaining: 1m 17s\n",
            "1711:\tlearn: 0.0066000\ttotal: 2m 17s\tremaining: 1m 17s\n",
            "1712:\tlearn: 0.0065911\ttotal: 2m 17s\tremaining: 1m 17s\n",
            "1713:\tlearn: 0.0065839\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1714:\tlearn: 0.0065691\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1715:\tlearn: 0.0065622\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1716:\tlearn: 0.0065572\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1717:\tlearn: 0.0065454\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1718:\tlearn: 0.0065374\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1719:\tlearn: 0.0065295\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1720:\tlearn: 0.0065237\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1721:\tlearn: 0.0065182\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1722:\tlearn: 0.0065130\ttotal: 2m 18s\tremaining: 1m 17s\n",
            "1723:\tlearn: 0.0065048\ttotal: 2m 19s\tremaining: 1m 17s\n",
            "1724:\tlearn: 0.0064974\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1725:\tlearn: 0.0064913\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1726:\tlearn: 0.0064824\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1727:\tlearn: 0.0064759\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1728:\tlearn: 0.0064699\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1729:\tlearn: 0.0064699\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1730:\tlearn: 0.0064697\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1731:\tlearn: 0.0064616\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1732:\tlearn: 0.0064551\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1733:\tlearn: 0.0064457\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1734:\tlearn: 0.0064397\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1735:\tlearn: 0.0064254\ttotal: 2m 19s\tremaining: 1m 16s\n",
            "1736:\tlearn: 0.0064185\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1737:\tlearn: 0.0064143\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1738:\tlearn: 0.0064115\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1739:\tlearn: 0.0064074\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1740:\tlearn: 0.0064023\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1741:\tlearn: 0.0063972\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1742:\tlearn: 0.0063931\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1743:\tlearn: 0.0063888\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1744:\tlearn: 0.0063888\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1745:\tlearn: 0.0063822\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1746:\tlearn: 0.0063822\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1747:\tlearn: 0.0063822\ttotal: 2m 20s\tremaining: 1m 15s\n",
            "1748:\tlearn: 0.0063796\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1749:\tlearn: 0.0063699\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1750:\tlearn: 0.0063613\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1751:\tlearn: 0.0063563\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1752:\tlearn: 0.0063478\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1753:\tlearn: 0.0063416\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1754:\tlearn: 0.0063331\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1755:\tlearn: 0.0063281\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1756:\tlearn: 0.0063235\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1757:\tlearn: 0.0063212\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1758:\tlearn: 0.0063182\ttotal: 2m 21s\tremaining: 1m 14s\n",
            "1759:\tlearn: 0.0063143\ttotal: 2m 22s\tremaining: 1m 14s\n",
            "1760:\tlearn: 0.0063031\ttotal: 2m 22s\tremaining: 1m 14s\n",
            "1761:\tlearn: 0.0062984\ttotal: 2m 22s\tremaining: 1m 14s\n",
            "1762:\tlearn: 0.0062984\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1763:\tlearn: 0.0062932\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1764:\tlearn: 0.0062858\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1765:\tlearn: 0.0062787\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1766:\tlearn: 0.0062726\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1767:\tlearn: 0.0062626\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1768:\tlearn: 0.0062546\ttotal: 2m 22s\tremaining: 1m 13s\n",
            "1769:\tlearn: 0.0062478\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1770:\tlearn: 0.0062477\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1771:\tlearn: 0.0062406\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1772:\tlearn: 0.0062352\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1773:\tlearn: 0.0062292\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1774:\tlearn: 0.0062249\ttotal: 2m 23s\tremaining: 1m 13s\n",
            "1775:\tlearn: 0.0062143\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1776:\tlearn: 0.0062143\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1777:\tlearn: 0.0062050\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1778:\tlearn: 0.0062009\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1779:\tlearn: 0.0061925\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1780:\tlearn: 0.0061885\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1781:\tlearn: 0.0061838\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1782:\tlearn: 0.0061777\ttotal: 2m 23s\tremaining: 1m 12s\n",
            "1783:\tlearn: 0.0061777\ttotal: 2m 24s\tremaining: 1m 12s\n",
            "1784:\tlearn: 0.0061723\ttotal: 2m 24s\tremaining: 1m 12s\n",
            "1785:\tlearn: 0.0061723\ttotal: 2m 24s\tremaining: 1m 12s\n",
            "1786:\tlearn: 0.0061675\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1787:\tlearn: 0.0061651\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1788:\tlearn: 0.0061623\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1789:\tlearn: 0.0061585\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1790:\tlearn: 0.0061534\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1791:\tlearn: 0.0061463\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1792:\tlearn: 0.0061398\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1793:\tlearn: 0.0061370\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1794:\tlearn: 0.0061332\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1795:\tlearn: 0.0061290\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1796:\tlearn: 0.0061209\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1797:\tlearn: 0.0061169\ttotal: 2m 24s\tremaining: 1m 11s\n",
            "1798:\tlearn: 0.0061130\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1799:\tlearn: 0.0061086\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1800:\tlearn: 0.0061049\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1801:\tlearn: 0.0060982\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1802:\tlearn: 0.0060963\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1803:\tlearn: 0.0060904\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1804:\tlearn: 0.0060851\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1805:\tlearn: 0.0060799\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1806:\tlearn: 0.0060750\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1807:\tlearn: 0.0060665\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1808:\tlearn: 0.0060589\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1809:\tlearn: 0.0060555\ttotal: 2m 25s\tremaining: 1m 10s\n",
            "1810:\tlearn: 0.0060555\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "1811:\tlearn: 0.0060495\ttotal: 2m 25s\tremaining: 1m 9s\n",
            "1812:\tlearn: 0.0060459\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1813:\tlearn: 0.0060403\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1814:\tlearn: 0.0060350\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1815:\tlearn: 0.0060301\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1816:\tlearn: 0.0060264\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1817:\tlearn: 0.0060213\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1818:\tlearn: 0.0060178\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1819:\tlearn: 0.0060107\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1820:\tlearn: 0.0060053\ttotal: 2m 26s\tremaining: 1m 9s\n",
            "1821:\tlearn: 0.0059980\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1822:\tlearn: 0.0059942\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1823:\tlearn: 0.0059942\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1824:\tlearn: 0.0059895\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1825:\tlearn: 0.0059842\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1826:\tlearn: 0.0059774\ttotal: 2m 26s\tremaining: 1m 8s\n",
            "1827:\tlearn: 0.0059700\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1828:\tlearn: 0.0059616\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1829:\tlearn: 0.0059577\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1830:\tlearn: 0.0059518\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1831:\tlearn: 0.0059489\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1832:\tlearn: 0.0059390\ttotal: 2m 27s\tremaining: 1m 8s\n",
            "1833:\tlearn: 0.0059350\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1834:\tlearn: 0.0059349\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1835:\tlearn: 0.0059314\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1836:\tlearn: 0.0059278\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1837:\tlearn: 0.0059221\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1838:\tlearn: 0.0059170\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1839:\tlearn: 0.0059124\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1840:\tlearn: 0.0059067\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1841:\tlearn: 0.0058994\ttotal: 2m 27s\tremaining: 1m 7s\n",
            "1842:\tlearn: 0.0058909\ttotal: 2m 28s\tremaining: 1m 7s\n",
            "1843:\tlearn: 0.0058873\ttotal: 2m 28s\tremaining: 1m 7s\n",
            "1844:\tlearn: 0.0058823\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1845:\tlearn: 0.0058763\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1846:\tlearn: 0.0058713\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1847:\tlearn: 0.0058688\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1848:\tlearn: 0.0058688\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1849:\tlearn: 0.0058619\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1850:\tlearn: 0.0058547\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1851:\tlearn: 0.0058502\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1852:\tlearn: 0.0058463\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1853:\tlearn: 0.0058392\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1854:\tlearn: 0.0058319\ttotal: 2m 28s\tremaining: 1m 6s\n",
            "1855:\tlearn: 0.0058319\ttotal: 2m 29s\tremaining: 1m 6s\n",
            "1856:\tlearn: 0.0058319\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1857:\tlearn: 0.0058319\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1858:\tlearn: 0.0058319\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1859:\tlearn: 0.0058258\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1860:\tlearn: 0.0058184\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1861:\tlearn: 0.0058167\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1862:\tlearn: 0.0058148\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1863:\tlearn: 0.0058148\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1864:\tlearn: 0.0058148\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1865:\tlearn: 0.0058148\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1866:\tlearn: 0.0058105\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1867:\tlearn: 0.0058047\ttotal: 2m 29s\tremaining: 1m 5s\n",
            "1868:\tlearn: 0.0057974\ttotal: 2m 29s\tremaining: 1m 4s\n",
            "1869:\tlearn: 0.0057913\ttotal: 2m 29s\tremaining: 1m 4s\n",
            "1870:\tlearn: 0.0057874\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1871:\tlearn: 0.0057803\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1872:\tlearn: 0.0057710\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1873:\tlearn: 0.0057668\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1874:\tlearn: 0.0057624\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1875:\tlearn: 0.0057586\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1876:\tlearn: 0.0057532\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1877:\tlearn: 0.0057472\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1878:\tlearn: 0.0057414\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1879:\tlearn: 0.0057386\ttotal: 2m 30s\tremaining: 1m 4s\n",
            "1880:\tlearn: 0.0057324\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1881:\tlearn: 0.0057291\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1882:\tlearn: 0.0057258\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1883:\tlearn: 0.0057162\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1884:\tlearn: 0.0057118\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1885:\tlearn: 0.0057118\ttotal: 2m 30s\tremaining: 1m 3s\n",
            "1886:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1887:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1888:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1889:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1890:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1891:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1892:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 3s\n",
            "1893:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 2s\n",
            "1894:\tlearn: 0.0057118\ttotal: 2m 31s\tremaining: 1m 2s\n",
            "1895:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1896:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1897:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1898:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1899:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1900:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1901:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1902:\tlearn: 0.0057118\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1903:\tlearn: 0.0057080\ttotal: 2m 32s\tremaining: 1m 2s\n",
            "1904:\tlearn: 0.0057043\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1905:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1906:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1907:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1908:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1909:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1910:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1911:\tlearn: 0.0057009\ttotal: 2m 32s\tremaining: 1m 1s\n",
            "1912:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "1913:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "1914:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "1915:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m 1s\n",
            "1916:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1917:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1918:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1919:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1920:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1921:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1922:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1923:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1924:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1925:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1926:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 1m\n",
            "1927:\tlearn: 0.0057008\ttotal: 2m 33s\tremaining: 60s\n",
            "1928:\tlearn: 0.0057008\ttotal: 2m 34s\tremaining: 59.9s\n",
            "1929:\tlearn: 0.0057008\ttotal: 2m 34s\tremaining: 59.8s\n",
            "1930:\tlearn: 0.0057008\ttotal: 2m 34s\tremaining: 59.7s\n",
            "1931:\tlearn: 0.0057008\ttotal: 2m 34s\tremaining: 59.6s\n",
            "1932:\tlearn: 0.0057008\ttotal: 2m 34s\tremaining: 59.5s\n",
            "1933:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59.4s\n",
            "1934:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59.3s\n",
            "1935:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59.3s\n",
            "1936:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59.2s\n",
            "1937:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59.1s\n",
            "1938:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 59s\n",
            "1939:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.9s\n",
            "1940:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.8s\n",
            "1941:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.7s\n",
            "1942:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.6s\n",
            "1943:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.6s\n",
            "1944:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.5s\n",
            "1945:\tlearn: 0.0057007\ttotal: 2m 34s\tremaining: 58.4s\n",
            "1946:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 58.3s\n",
            "1947:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 58.2s\n",
            "1948:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 58.1s\n",
            "1949:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 58.1s\n",
            "1950:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 58s\n",
            "1951:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.9s\n",
            "1952:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.8s\n",
            "1953:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.7s\n",
            "1954:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.6s\n",
            "1955:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.6s\n",
            "1956:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.5s\n",
            "1957:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.4s\n",
            "1958:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.3s\n",
            "1959:\tlearn: 0.0057007\ttotal: 2m 35s\tremaining: 57.2s\n",
            "1960:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 57.1s\n",
            "1961:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 57s\n",
            "1962:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 57s\n",
            "1963:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.9s\n",
            "1964:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.8s\n",
            "1965:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.7s\n",
            "1966:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.6s\n",
            "1967:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.5s\n",
            "1968:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.4s\n",
            "1969:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.4s\n",
            "1970:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.3s\n",
            "1971:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.2s\n",
            "1972:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56.1s\n",
            "1973:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 56s\n",
            "1974:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 55.9s\n",
            "1975:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 55.8s\n",
            "1976:\tlearn: 0.0057007\ttotal: 2m 36s\tremaining: 55.7s\n",
            "1977:\tlearn: 0.0057007\ttotal: 2m 37s\tremaining: 55.7s\n",
            "1978:\tlearn: 0.0057007\ttotal: 2m 37s\tremaining: 55.6s\n",
            "1979:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.5s\n",
            "1980:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.4s\n",
            "1981:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.3s\n",
            "1982:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.2s\n",
            "1983:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.1s\n",
            "1984:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55.1s\n",
            "1985:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 55s\n",
            "1986:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.9s\n",
            "1987:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.8s\n",
            "1988:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.7s\n",
            "1989:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.6s\n",
            "1990:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.5s\n",
            "1991:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.5s\n",
            "1992:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.4s\n",
            "1993:\tlearn: 0.0057006\ttotal: 2m 37s\tremaining: 54.3s\n",
            "1994:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 54.2s\n",
            "1995:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 54.1s\n",
            "1996:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 54s\n",
            "1997:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.9s\n",
            "1998:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.9s\n",
            "1999:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.8s\n",
            "2000:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.7s\n",
            "2001:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.6s\n",
            "2002:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.5s\n",
            "2003:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.4s\n",
            "2004:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.4s\n",
            "2005:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.3s\n",
            "2006:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.2s\n",
            "2007:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53.1s\n",
            "2008:\tlearn: 0.0057006\ttotal: 2m 38s\tremaining: 53s\n",
            "2009:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.9s\n",
            "2010:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.8s\n",
            "2011:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.8s\n",
            "2012:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.7s\n",
            "2013:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.6s\n",
            "2014:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.5s\n",
            "2015:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.4s\n",
            "2016:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.3s\n",
            "2017:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.2s\n",
            "2018:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.2s\n",
            "2019:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52.1s\n",
            "2020:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 52s\n",
            "2021:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 51.9s\n",
            "2022:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 51.8s\n",
            "2023:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 51.7s\n",
            "2024:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 51.6s\n",
            "2025:\tlearn: 0.0057006\ttotal: 2m 39s\tremaining: 51.6s\n",
            "2026:\tlearn: 0.0057006\ttotal: 2m 40s\tremaining: 51.5s\n",
            "2027:\tlearn: 0.0057006\ttotal: 2m 40s\tremaining: 51.4s\n",
            "2028:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 51.3s\n",
            "2029:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 51.2s\n",
            "2030:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 51.1s\n",
            "2031:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 51s\n",
            "2032:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 51s\n",
            "2033:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.9s\n",
            "2034:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.8s\n",
            "2035:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.7s\n",
            "2036:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.6s\n",
            "2037:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.5s\n",
            "2038:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.4s\n",
            "2039:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.4s\n",
            "2040:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.3s\n",
            "2041:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.2s\n",
            "2042:\tlearn: 0.0057005\ttotal: 2m 40s\tremaining: 50.1s\n",
            "2043:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 50s\n",
            "2044:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.9s\n",
            "2045:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.9s\n",
            "2046:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.8s\n",
            "2047:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.7s\n",
            "2048:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.6s\n",
            "2049:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.5s\n",
            "2050:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.4s\n",
            "2051:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.4s\n",
            "2052:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.3s\n",
            "2053:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.2s\n",
            "2054:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49.1s\n",
            "2055:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49s\n",
            "2056:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 49s\n",
            "2057:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 48.9s\n",
            "2058:\tlearn: 0.0057005\ttotal: 2m 41s\tremaining: 48.8s\n",
            "2059:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.7s\n",
            "2060:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.6s\n",
            "2061:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.5s\n",
            "2062:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.4s\n",
            "2063:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.3s\n",
            "2064:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.3s\n",
            "2065:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.2s\n",
            "2066:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48.1s\n",
            "2067:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 48s\n",
            "2068:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.9s\n",
            "2069:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.8s\n",
            "2070:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.8s\n",
            "2071:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.7s\n",
            "2072:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.6s\n",
            "2073:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.5s\n",
            "2074:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.4s\n",
            "2075:\tlearn: 0.0057005\ttotal: 2m 42s\tremaining: 47.3s\n",
            "2076:\tlearn: 0.0057004\ttotal: 2m 43s\tremaining: 47.2s\n",
            "2077:\tlearn: 0.0057004\ttotal: 2m 43s\tremaining: 47.2s\n",
            "2078:\tlearn: 0.0057004\ttotal: 2m 43s\tremaining: 47.1s\n",
            "2079:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 47s\n",
            "2080:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.9s\n",
            "2081:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.8s\n",
            "2082:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.7s\n",
            "2083:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.7s\n",
            "2084:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.6s\n",
            "2085:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.5s\n",
            "2086:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.4s\n",
            "2087:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.3s\n",
            "2088:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.2s\n",
            "2089:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.2s\n",
            "2090:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46.1s\n",
            "2091:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 46s\n",
            "2092:\tlearn: 0.0056951\ttotal: 2m 43s\tremaining: 45.9s\n",
            "2093:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.8s\n",
            "2094:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.7s\n",
            "2095:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.6s\n",
            "2096:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.6s\n",
            "2097:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.5s\n",
            "2098:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.4s\n",
            "2099:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.3s\n",
            "2100:\tlearn: 0.0056951\ttotal: 2m 44s\tremaining: 45.2s\n",
            "2101:\tlearn: 0.0056892\ttotal: 2m 44s\tremaining: 45.1s\n",
            "2102:\tlearn: 0.0056835\ttotal: 2m 44s\tremaining: 45.1s\n",
            "2103:\tlearn: 0.0056803\ttotal: 2m 44s\tremaining: 45s\n",
            "2104:\tlearn: 0.0056758\ttotal: 2m 44s\tremaining: 44.9s\n",
            "2105:\tlearn: 0.0056719\ttotal: 2m 44s\tremaining: 44.9s\n",
            "2106:\tlearn: 0.0056719\ttotal: 2m 44s\tremaining: 44.8s\n",
            "2107:\tlearn: 0.0056719\ttotal: 2m 44s\tremaining: 44.7s\n",
            "2108:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.6s\n",
            "2109:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.5s\n",
            "2110:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.4s\n",
            "2111:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.4s\n",
            "2112:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.3s\n",
            "2113:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.2s\n",
            "2114:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44.1s\n",
            "2115:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 44s\n",
            "2116:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.9s\n",
            "2117:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.9s\n",
            "2118:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.8s\n",
            "2119:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.7s\n",
            "2120:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.6s\n",
            "2121:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.5s\n",
            "2122:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.4s\n",
            "2123:\tlearn: 0.0056719\ttotal: 2m 45s\tremaining: 43.4s\n",
            "2124:\tlearn: 0.0056719\ttotal: 2m 46s\tremaining: 43.3s\n",
            "2125:\tlearn: 0.0056719\ttotal: 2m 46s\tremaining: 43.2s\n",
            "2126:\tlearn: 0.0056719\ttotal: 2m 46s\tremaining: 43.1s\n",
            "2127:\tlearn: 0.0056719\ttotal: 2m 46s\tremaining: 43s\n",
            "2128:\tlearn: 0.0056719\ttotal: 2m 46s\tremaining: 42.9s\n",
            "2129:\tlearn: 0.0056683\ttotal: 2m 46s\tremaining: 42.9s\n",
            "2130:\tlearn: 0.0056636\ttotal: 2m 46s\tremaining: 42.8s\n",
            "2131:\tlearn: 0.0056592\ttotal: 2m 46s\tremaining: 42.7s\n",
            "2132:\tlearn: 0.0056564\ttotal: 2m 46s\tremaining: 42.6s\n",
            "2133:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.5s\n",
            "2134:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.5s\n",
            "2135:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.4s\n",
            "2136:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.3s\n",
            "2137:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.2s\n",
            "2138:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42.1s\n",
            "2139:\tlearn: 0.0056513\ttotal: 2m 46s\tremaining: 42s\n",
            "2140:\tlearn: 0.0056459\ttotal: 2m 46s\tremaining: 42s\n",
            "2141:\tlearn: 0.0056411\ttotal: 2m 47s\tremaining: 41.9s\n",
            "2142:\tlearn: 0.0056411\ttotal: 2m 47s\tremaining: 41.8s\n",
            "2143:\tlearn: 0.0056382\ttotal: 2m 47s\tremaining: 41.7s\n",
            "2144:\tlearn: 0.0056382\ttotal: 2m 47s\tremaining: 41.6s\n",
            "2145:\tlearn: 0.0056382\ttotal: 2m 47s\tremaining: 41.5s\n",
            "2146:\tlearn: 0.0056339\ttotal: 2m 47s\tremaining: 41.5s\n",
            "2147:\tlearn: 0.0056275\ttotal: 2m 47s\tremaining: 41.4s\n",
            "2148:\tlearn: 0.0056190\ttotal: 2m 47s\tremaining: 41.3s\n",
            "2149:\tlearn: 0.0056147\ttotal: 2m 47s\tremaining: 41.2s\n",
            "2150:\tlearn: 0.0056061\ttotal: 2m 47s\tremaining: 41.1s\n",
            "2151:\tlearn: 0.0055975\ttotal: 2m 47s\tremaining: 41.1s\n",
            "2152:\tlearn: 0.0055937\ttotal: 2m 48s\tremaining: 41.1s\n",
            "2153:\tlearn: 0.0055937\ttotal: 2m 48s\tremaining: 41s\n",
            "2154:\tlearn: 0.0055899\ttotal: 2m 48s\tremaining: 40.9s\n",
            "2155:\tlearn: 0.0055861\ttotal: 2m 48s\tremaining: 40.9s\n",
            "2156:\tlearn: 0.0055808\ttotal: 2m 48s\tremaining: 40.8s\n",
            "2157:\tlearn: 0.0055754\ttotal: 2m 48s\tremaining: 40.7s\n",
            "2158:\tlearn: 0.0055690\ttotal: 2m 48s\tremaining: 40.6s\n",
            "2159:\tlearn: 0.0055635\ttotal: 2m 48s\tremaining: 40.5s\n",
            "2160:\tlearn: 0.0055614\ttotal: 2m 48s\tremaining: 40.5s\n",
            "2161:\tlearn: 0.0055547\ttotal: 2m 48s\tremaining: 40.4s\n",
            "2162:\tlearn: 0.0055489\ttotal: 2m 48s\tremaining: 40.3s\n",
            "2163:\tlearn: 0.0055442\ttotal: 2m 49s\tremaining: 40.2s\n",
            "2164:\tlearn: 0.0055355\ttotal: 2m 49s\tremaining: 40.1s\n",
            "2165:\tlearn: 0.0055310\ttotal: 2m 49s\tremaining: 40.1s\n",
            "2166:\tlearn: 0.0055229\ttotal: 2m 49s\tremaining: 40s\n",
            "2167:\tlearn: 0.0055174\ttotal: 2m 49s\tremaining: 39.9s\n",
            "2168:\tlearn: 0.0055108\ttotal: 2m 49s\tremaining: 39.8s\n",
            "2169:\tlearn: 0.0055080\ttotal: 2m 49s\tremaining: 39.7s\n",
            "2170:\tlearn: 0.0055040\ttotal: 2m 49s\tremaining: 39.7s\n",
            "2171:\tlearn: 0.0055012\ttotal: 2m 49s\tremaining: 39.6s\n",
            "2172:\tlearn: 0.0054970\ttotal: 2m 49s\tremaining: 39.5s\n",
            "2173:\tlearn: 0.0054924\ttotal: 2m 49s\tremaining: 39.4s\n",
            "2174:\tlearn: 0.0054855\ttotal: 2m 49s\tremaining: 39.3s\n",
            "2175:\tlearn: 0.0054855\ttotal: 2m 49s\tremaining: 39.3s\n",
            "2176:\tlearn: 0.0054821\ttotal: 2m 49s\tremaining: 39.2s\n",
            "2177:\tlearn: 0.0054770\ttotal: 2m 49s\tremaining: 39.1s\n",
            "2178:\tlearn: 0.0054770\ttotal: 2m 50s\tremaining: 39s\n",
            "2179:\tlearn: 0.0054770\ttotal: 2m 50s\tremaining: 38.9s\n",
            "2180:\tlearn: 0.0054717\ttotal: 2m 50s\tremaining: 38.9s\n",
            "2181:\tlearn: 0.0054648\ttotal: 2m 50s\tremaining: 38.8s\n",
            "2182:\tlearn: 0.0054606\ttotal: 2m 50s\tremaining: 38.7s\n",
            "2183:\tlearn: 0.0054605\ttotal: 2m 50s\tremaining: 38.6s\n",
            "2184:\tlearn: 0.0054550\ttotal: 2m 50s\tremaining: 38.5s\n",
            "2185:\tlearn: 0.0054433\ttotal: 2m 50s\tremaining: 38.5s\n",
            "2186:\tlearn: 0.0054384\ttotal: 2m 50s\tremaining: 38.4s\n",
            "2187:\tlearn: 0.0054341\ttotal: 2m 50s\tremaining: 38.3s\n",
            "2188:\tlearn: 0.0054341\ttotal: 2m 50s\tremaining: 38.2s\n",
            "2189:\tlearn: 0.0054341\ttotal: 2m 50s\tremaining: 38.1s\n",
            "2190:\tlearn: 0.0054305\ttotal: 2m 50s\tremaining: 38s\n",
            "2191:\tlearn: 0.0054273\ttotal: 2m 50s\tremaining: 38s\n",
            "2192:\tlearn: 0.0054221\ttotal: 2m 50s\tremaining: 37.9s\n",
            "2193:\tlearn: 0.0054220\ttotal: 2m 51s\tremaining: 37.8s\n",
            "2194:\tlearn: 0.0054181\ttotal: 2m 51s\tremaining: 37.7s\n",
            "2195:\tlearn: 0.0054155\ttotal: 2m 51s\tremaining: 37.6s\n",
            "2196:\tlearn: 0.0054126\ttotal: 2m 51s\tremaining: 37.6s\n",
            "2197:\tlearn: 0.0054126\ttotal: 2m 51s\tremaining: 37.5s\n",
            "2198:\tlearn: 0.0054056\ttotal: 2m 51s\tremaining: 37.4s\n",
            "2199:\tlearn: 0.0054056\ttotal: 2m 51s\tremaining: 37.3s\n",
            "2200:\tlearn: 0.0054001\ttotal: 2m 51s\tremaining: 37.2s\n",
            "2201:\tlearn: 0.0053958\ttotal: 2m 51s\tremaining: 37.2s\n",
            "2202:\tlearn: 0.0053958\ttotal: 2m 51s\tremaining: 37.1s\n",
            "2203:\tlearn: 0.0053914\ttotal: 2m 51s\tremaining: 37s\n",
            "2204:\tlearn: 0.0053881\ttotal: 2m 51s\tremaining: 36.9s\n",
            "2205:\tlearn: 0.0053825\ttotal: 2m 51s\tremaining: 36.9s\n",
            "2206:\tlearn: 0.0053825\ttotal: 2m 51s\tremaining: 36.8s\n",
            "2207:\tlearn: 0.0053804\ttotal: 2m 52s\tremaining: 36.7s\n",
            "2208:\tlearn: 0.0053762\ttotal: 2m 52s\tremaining: 36.6s\n",
            "2209:\tlearn: 0.0053734\ttotal: 2m 52s\tremaining: 36.5s\n",
            "2210:\tlearn: 0.0053734\ttotal: 2m 52s\tremaining: 36.4s\n",
            "2211:\tlearn: 0.0053719\ttotal: 2m 52s\tremaining: 36.4s\n",
            "2212:\tlearn: 0.0053662\ttotal: 2m 52s\tremaining: 36.3s\n",
            "2213:\tlearn: 0.0053634\ttotal: 2m 52s\tremaining: 36.2s\n",
            "2214:\tlearn: 0.0053614\ttotal: 2m 52s\tremaining: 36.1s\n",
            "2215:\tlearn: 0.0053552\ttotal: 2m 52s\tremaining: 36.1s\n",
            "2216:\tlearn: 0.0053500\ttotal: 2m 52s\tremaining: 36s\n",
            "2217:\tlearn: 0.0053482\ttotal: 2m 52s\tremaining: 35.9s\n",
            "2218:\tlearn: 0.0053444\ttotal: 2m 52s\tremaining: 35.8s\n",
            "2219:\tlearn: 0.0053424\ttotal: 2m 52s\tremaining: 35.7s\n",
            "2220:\tlearn: 0.0053424\ttotal: 2m 52s\tremaining: 35.6s\n",
            "2221:\tlearn: 0.0053424\ttotal: 2m 52s\tremaining: 35.6s\n",
            "2222:\tlearn: 0.0053424\ttotal: 2m 52s\tremaining: 35.5s\n",
            "2223:\tlearn: 0.0053424\ttotal: 2m 53s\tremaining: 35.4s\n",
            "2224:\tlearn: 0.0053405\ttotal: 2m 53s\tremaining: 35.3s\n",
            "2225:\tlearn: 0.0053405\ttotal: 2m 53s\tremaining: 35.2s\n",
            "2226:\tlearn: 0.0053405\ttotal: 2m 53s\tremaining: 35.2s\n",
            "2227:\tlearn: 0.0053405\ttotal: 2m 53s\tremaining: 35.1s\n",
            "2228:\tlearn: 0.0053361\ttotal: 2m 53s\tremaining: 35s\n",
            "2229:\tlearn: 0.0053307\ttotal: 2m 53s\tremaining: 34.9s\n",
            "2230:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.8s\n",
            "2231:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.8s\n",
            "2232:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.7s\n",
            "2233:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.6s\n",
            "2234:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.5s\n",
            "2235:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.4s\n",
            "2236:\tlearn: 0.0053277\ttotal: 2m 53s\tremaining: 34.3s\n",
            "2237:\tlearn: 0.0053240\ttotal: 2m 53s\tremaining: 34.3s\n",
            "2238:\tlearn: 0.0053195\ttotal: 2m 53s\tremaining: 34.2s\n",
            "2239:\tlearn: 0.0053195\ttotal: 2m 54s\tremaining: 34.1s\n",
            "2240:\tlearn: 0.0053195\ttotal: 2m 54s\tremaining: 34s\n",
            "2241:\tlearn: 0.0053195\ttotal: 2m 54s\tremaining: 33.9s\n",
            "2242:\tlearn: 0.0053195\ttotal: 2m 54s\tremaining: 33.9s\n",
            "2243:\tlearn: 0.0053195\ttotal: 2m 54s\tremaining: 33.8s\n",
            "2244:\tlearn: 0.0053183\ttotal: 2m 54s\tremaining: 33.7s\n",
            "2245:\tlearn: 0.0053182\ttotal: 2m 54s\tremaining: 33.6s\n",
            "2246:\tlearn: 0.0053142\ttotal: 2m 54s\tremaining: 33.5s\n",
            "2247:\tlearn: 0.0053142\ttotal: 2m 54s\tremaining: 33.5s\n",
            "2248:\tlearn: 0.0053110\ttotal: 2m 54s\tremaining: 33.4s\n",
            "2249:\tlearn: 0.0053080\ttotal: 2m 54s\tremaining: 33.3s\n",
            "2250:\tlearn: 0.0053040\ttotal: 2m 54s\tremaining: 33.2s\n",
            "2251:\tlearn: 0.0052991\ttotal: 2m 54s\tremaining: 33.1s\n",
            "2252:\tlearn: 0.0052991\ttotal: 2m 54s\tremaining: 33.1s\n",
            "2253:\tlearn: 0.0052991\ttotal: 2m 54s\tremaining: 33s\n",
            "2254:\tlearn: 0.0052991\ttotal: 2m 54s\tremaining: 32.9s\n",
            "2255:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.8s\n",
            "2256:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.7s\n",
            "2257:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.7s\n",
            "2258:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.6s\n",
            "2259:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.5s\n",
            "2260:\tlearn: 0.0052991\ttotal: 2m 55s\tremaining: 32.4s\n",
            "2261:\tlearn: 0.0052942\ttotal: 2m 55s\tremaining: 32.3s\n",
            "2262:\tlearn: 0.0052912\ttotal: 2m 55s\tremaining: 32.3s\n",
            "2263:\tlearn: 0.0052851\ttotal: 2m 55s\tremaining: 32.2s\n",
            "2264:\tlearn: 0.0052830\ttotal: 2m 55s\tremaining: 32.1s\n",
            "2265:\tlearn: 0.0052732\ttotal: 2m 55s\tremaining: 32s\n",
            "2266:\tlearn: 0.0052670\ttotal: 2m 55s\tremaining: 31.9s\n",
            "2267:\tlearn: 0.0052623\ttotal: 2m 55s\tremaining: 31.9s\n",
            "2268:\tlearn: 0.0052571\ttotal: 2m 55s\tremaining: 31.8s\n",
            "2269:\tlearn: 0.0052502\ttotal: 2m 55s\tremaining: 31.7s\n",
            "2270:\tlearn: 0.0052435\ttotal: 2m 56s\tremaining: 31.6s\n",
            "2271:\tlearn: 0.0052386\ttotal: 2m 56s\tremaining: 31.5s\n",
            "2272:\tlearn: 0.0052341\ttotal: 2m 56s\tremaining: 31.5s\n",
            "2273:\tlearn: 0.0052341\ttotal: 2m 56s\tremaining: 31.4s\n",
            "2274:\tlearn: 0.0052341\ttotal: 2m 56s\tremaining: 31.3s\n",
            "2275:\tlearn: 0.0052260\ttotal: 2m 56s\tremaining: 31.2s\n",
            "2276:\tlearn: 0.0052225\ttotal: 2m 56s\tremaining: 31.2s\n",
            "2277:\tlearn: 0.0052224\ttotal: 2m 56s\tremaining: 31.1s\n",
            "2278:\tlearn: 0.0052180\ttotal: 2m 56s\tremaining: 31s\n",
            "2279:\tlearn: 0.0052180\ttotal: 2m 56s\tremaining: 30.9s\n",
            "2280:\tlearn: 0.0052153\ttotal: 2m 56s\tremaining: 30.8s\n",
            "2281:\tlearn: 0.0052153\ttotal: 2m 56s\tremaining: 30.8s\n",
            "2282:\tlearn: 0.0052118\ttotal: 2m 56s\tremaining: 30.7s\n",
            "2283:\tlearn: 0.0052067\ttotal: 2m 56s\tremaining: 30.6s\n",
            "2284:\tlearn: 0.0052036\ttotal: 2m 56s\tremaining: 30.5s\n",
            "2285:\tlearn: 0.0052020\ttotal: 2m 57s\tremaining: 30.4s\n",
            "2286:\tlearn: 0.0051979\ttotal: 2m 57s\tremaining: 30.4s\n",
            "2287:\tlearn: 0.0051979\ttotal: 2m 57s\tremaining: 30.3s\n",
            "2288:\tlearn: 0.0051933\ttotal: 2m 57s\tremaining: 30.2s\n",
            "2289:\tlearn: 0.0051895\ttotal: 2m 57s\tremaining: 30.1s\n",
            "2290:\tlearn: 0.0051849\ttotal: 2m 57s\tremaining: 30s\n",
            "2291:\tlearn: 0.0051849\ttotal: 2m 57s\tremaining: 30s\n",
            "2292:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.9s\n",
            "2293:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.8s\n",
            "2294:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.7s\n",
            "2295:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.6s\n",
            "2296:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.6s\n",
            "2297:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.5s\n",
            "2298:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.4s\n",
            "2299:\tlearn: 0.0051821\ttotal: 2m 57s\tremaining: 29.3s\n",
            "2300:\tlearn: 0.0051804\ttotal: 2m 58s\tremaining: 29.2s\n",
            "2301:\tlearn: 0.0051769\ttotal: 2m 58s\tremaining: 29.2s\n",
            "2302:\tlearn: 0.0051712\ttotal: 2m 58s\tremaining: 29.1s\n",
            "2303:\tlearn: 0.0051672\ttotal: 2m 58s\tremaining: 29s\n",
            "2304:\tlearn: 0.0051634\ttotal: 2m 58s\tremaining: 28.9s\n",
            "2305:\tlearn: 0.0051598\ttotal: 2m 58s\tremaining: 28.9s\n",
            "2306:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.8s\n",
            "2307:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.7s\n",
            "2308:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.6s\n",
            "2309:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.5s\n",
            "2310:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.5s\n",
            "2311:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.4s\n",
            "2312:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.3s\n",
            "2313:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.2s\n",
            "2314:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.1s\n",
            "2315:\tlearn: 0.0051543\ttotal: 2m 58s\tremaining: 28.1s\n",
            "2316:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 28s\n",
            "2317:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.9s\n",
            "2318:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.8s\n",
            "2319:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.7s\n",
            "2320:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.6s\n",
            "2321:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.6s\n",
            "2322:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.5s\n",
            "2323:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.4s\n",
            "2324:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.3s\n",
            "2325:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.2s\n",
            "2326:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.2s\n",
            "2327:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27.1s\n",
            "2328:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 27s\n",
            "2329:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 26.9s\n",
            "2330:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 26.9s\n",
            "2331:\tlearn: 0.0051543\ttotal: 2m 59s\tremaining: 26.8s\n",
            "2332:\tlearn: 0.0051542\ttotal: 2m 59s\tremaining: 26.7s\n",
            "2333:\tlearn: 0.0051542\ttotal: 3m\tremaining: 26.6s\n",
            "2334:\tlearn: 0.0051527\ttotal: 3m\tremaining: 26.5s\n",
            "2335:\tlearn: 0.0051481\ttotal: 3m\tremaining: 26.5s\n",
            "2336:\tlearn: 0.0051462\ttotal: 3m\tremaining: 26.4s\n",
            "2337:\tlearn: 0.0051422\ttotal: 3m\tremaining: 26.3s\n",
            "2338:\tlearn: 0.0051388\ttotal: 3m\tremaining: 26.2s\n",
            "2339:\tlearn: 0.0051388\ttotal: 3m\tremaining: 26.1s\n",
            "2340:\tlearn: 0.0051341\ttotal: 3m\tremaining: 26.1s\n",
            "2341:\tlearn: 0.0051341\ttotal: 3m\tremaining: 26s\n",
            "2342:\tlearn: 0.0051297\ttotal: 3m\tremaining: 25.9s\n",
            "2343:\tlearn: 0.0051297\ttotal: 3m\tremaining: 25.8s\n",
            "2344:\tlearn: 0.0051246\ttotal: 3m\tremaining: 25.7s\n",
            "2345:\tlearn: 0.0051187\ttotal: 3m\tremaining: 25.7s\n",
            "2346:\tlearn: 0.0051128\ttotal: 3m\tremaining: 25.6s\n",
            "2347:\tlearn: 0.0051128\ttotal: 3m\tremaining: 25.5s\n",
            "2348:\tlearn: 0.0051128\ttotal: 3m 1s\tremaining: 25.4s\n",
            "2349:\tlearn: 0.0051103\ttotal: 3m 1s\tremaining: 25.4s\n",
            "2350:\tlearn: 0.0051102\ttotal: 3m 1s\tremaining: 25.3s\n",
            "2351:\tlearn: 0.0051102\ttotal: 3m 1s\tremaining: 25.2s\n",
            "2352:\tlearn: 0.0051102\ttotal: 3m 1s\tremaining: 25.1s\n",
            "2353:\tlearn: 0.0051102\ttotal: 3m 1s\tremaining: 25s\n",
            "2354:\tlearn: 0.0051064\ttotal: 3m 1s\tremaining: 25s\n",
            "2355:\tlearn: 0.0051064\ttotal: 3m 1s\tremaining: 24.9s\n",
            "2356:\tlearn: 0.0050987\ttotal: 3m 1s\tremaining: 24.8s\n",
            "2357:\tlearn: 0.0050987\ttotal: 3m 1s\tremaining: 24.7s\n",
            "2358:\tlearn: 0.0050987\ttotal: 3m 1s\tremaining: 24.6s\n",
            "2359:\tlearn: 0.0050987\ttotal: 3m 1s\tremaining: 24.6s\n",
            "2360:\tlearn: 0.0050987\ttotal: 3m 1s\tremaining: 24.5s\n",
            "2361:\tlearn: 0.0050928\ttotal: 3m 1s\tremaining: 24.4s\n",
            "2362:\tlearn: 0.0050903\ttotal: 3m 1s\tremaining: 24.3s\n",
            "2363:\tlearn: 0.0050898\ttotal: 3m 2s\tremaining: 24.3s\n",
            "2364:\tlearn: 0.0050858\ttotal: 3m 2s\tremaining: 24.2s\n",
            "2365:\tlearn: 0.0050858\ttotal: 3m 2s\tremaining: 24.1s\n",
            "2366:\tlearn: 0.0050858\ttotal: 3m 2s\tremaining: 24s\n",
            "2367:\tlearn: 0.0050798\ttotal: 3m 2s\tremaining: 24s\n",
            "2368:\tlearn: 0.0050755\ttotal: 3m 2s\tremaining: 23.9s\n",
            "2369:\tlearn: 0.0050703\ttotal: 3m 2s\tremaining: 23.8s\n",
            "2370:\tlearn: 0.0050703\ttotal: 3m 2s\tremaining: 23.7s\n",
            "2371:\tlearn: 0.0050657\ttotal: 3m 2s\tremaining: 23.6s\n",
            "2372:\tlearn: 0.0050599\ttotal: 3m 2s\tremaining: 23.6s\n",
            "2373:\tlearn: 0.0050562\ttotal: 3m 2s\tremaining: 23.5s\n",
            "2374:\tlearn: 0.0050562\ttotal: 3m 2s\tremaining: 23.4s\n",
            "2375:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23.3s\n",
            "2376:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23.3s\n",
            "2377:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23.2s\n",
            "2378:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23.1s\n",
            "2379:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23s\n",
            "2380:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 23s\n",
            "2381:\tlearn: 0.0050561\ttotal: 3m 3s\tremaining: 22.9s\n",
            "2382:\tlearn: 0.0050560\ttotal: 3m 3s\tremaining: 22.8s\n",
            "2383:\tlearn: 0.0050560\ttotal: 3m 3s\tremaining: 22.7s\n",
            "2384:\tlearn: 0.0050560\ttotal: 3m 3s\tremaining: 22.6s\n",
            "2385:\tlearn: 0.0050560\ttotal: 3m 3s\tremaining: 22.6s\n",
            "2386:\tlearn: 0.0050560\ttotal: 3m 3s\tremaining: 22.5s\n",
            "2387:\tlearn: 0.0050530\ttotal: 3m 3s\tremaining: 22.4s\n",
            "2388:\tlearn: 0.0050530\ttotal: 3m 4s\tremaining: 22.3s\n",
            "2389:\tlearn: 0.0050488\ttotal: 3m 4s\tremaining: 22.3s\n",
            "2390:\tlearn: 0.0050488\ttotal: 3m 4s\tremaining: 22.2s\n",
            "2391:\tlearn: 0.0050487\ttotal: 3m 4s\tremaining: 22.1s\n",
            "2392:\tlearn: 0.0050487\ttotal: 3m 4s\tremaining: 22s\n",
            "2393:\tlearn: 0.0050450\ttotal: 3m 4s\tremaining: 22s\n",
            "2394:\tlearn: 0.0050400\ttotal: 3m 4s\tremaining: 21.9s\n",
            "2395:\tlearn: 0.0050400\ttotal: 3m 4s\tremaining: 21.8s\n",
            "2396:\tlearn: 0.0050400\ttotal: 3m 5s\tremaining: 21.8s\n",
            "2397:\tlearn: 0.0050400\ttotal: 3m 5s\tremaining: 21.7s\n",
            "2398:\tlearn: 0.0050400\ttotal: 3m 5s\tremaining: 21.6s\n",
            "2399:\tlearn: 0.0050385\ttotal: 3m 5s\tremaining: 21.5s\n",
            "2400:\tlearn: 0.0050362\ttotal: 3m 5s\tremaining: 21.5s\n",
            "2401:\tlearn: 0.0050362\ttotal: 3m 5s\tremaining: 21.4s\n",
            "2402:\tlearn: 0.0050362\ttotal: 3m 5s\tremaining: 21.3s\n",
            "2403:\tlearn: 0.0050362\ttotal: 3m 5s\tremaining: 21.2s\n",
            "2404:\tlearn: 0.0050327\ttotal: 3m 5s\tremaining: 21.2s\n",
            "2405:\tlearn: 0.0050300\ttotal: 3m 5s\tremaining: 21.1s\n",
            "2406:\tlearn: 0.0050266\ttotal: 3m 5s\tremaining: 21s\n",
            "2407:\tlearn: 0.0050266\ttotal: 3m 5s\tremaining: 20.9s\n",
            "2408:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.9s\n",
            "2409:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.8s\n",
            "2410:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.7s\n",
            "2411:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.6s\n",
            "2412:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.5s\n",
            "2413:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.5s\n",
            "2414:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.4s\n",
            "2415:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.3s\n",
            "2416:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.2s\n",
            "2417:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.2s\n",
            "2418:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20.1s\n",
            "2419:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 20s\n",
            "2420:\tlearn: 0.0050266\ttotal: 3m 6s\tremaining: 19.9s\n",
            "2421:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.8s\n",
            "2422:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.8s\n",
            "2423:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.7s\n",
            "2424:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.6s\n",
            "2425:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.5s\n",
            "2426:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.5s\n",
            "2427:\tlearn: 0.0050266\ttotal: 3m 7s\tremaining: 19.4s\n",
            "2428:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 19.3s\n",
            "2429:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 19.2s\n",
            "2430:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 19.2s\n",
            "2431:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 19.1s\n",
            "2432:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 19s\n",
            "2433:\tlearn: 0.0050265\ttotal: 3m 7s\tremaining: 18.9s\n",
            "2434:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.8s\n",
            "2435:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.8s\n",
            "2436:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.7s\n",
            "2437:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.6s\n",
            "2438:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.5s\n",
            "2439:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.4s\n",
            "2440:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.4s\n",
            "2441:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.3s\n",
            "2442:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.2s\n",
            "2443:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.1s\n",
            "2444:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18.1s\n",
            "2445:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 18s\n",
            "2446:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 17.9s\n",
            "2447:\tlearn: 0.0050265\ttotal: 3m 8s\tremaining: 17.8s\n",
            "2448:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.8s\n",
            "2449:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.7s\n",
            "2450:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.6s\n",
            "2451:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.5s\n",
            "2452:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.4s\n",
            "2453:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.4s\n",
            "2454:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.3s\n",
            "2455:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.2s\n",
            "2456:\tlearn: 0.0050264\ttotal: 3m 9s\tremaining: 17.1s\n",
            "2457:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17.1s\n",
            "2458:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 17s\n",
            "2459:\tlearn: 0.0050265\ttotal: 3m 9s\tremaining: 16.9s\n",
            "2460:\tlearn: 0.0050264\ttotal: 3m 9s\tremaining: 16.8s\n",
            "2461:\tlearn: 0.0050264\ttotal: 3m 9s\tremaining: 16.7s\n",
            "2462:\tlearn: 0.0050264\ttotal: 3m 9s\tremaining: 16.7s\n",
            "2463:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.6s\n",
            "2464:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.5s\n",
            "2465:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.4s\n",
            "2466:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.3s\n",
            "2467:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.3s\n",
            "2468:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.2s\n",
            "2469:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16.1s\n",
            "2470:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16s\n",
            "2471:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 16s\n",
            "2472:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 15.9s\n",
            "2473:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 15.8s\n",
            "2474:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 15.7s\n",
            "2475:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 15.6s\n",
            "2476:\tlearn: 0.0050264\ttotal: 3m 10s\tremaining: 15.6s\n",
            "2477:\tlearn: 0.0050232\ttotal: 3m 10s\tremaining: 15.5s\n",
            "2478:\tlearn: 0.0050232\ttotal: 3m 10s\tremaining: 15.4s\n",
            "2479:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 15.3s\n",
            "2480:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 15.2s\n",
            "2481:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 15.2s\n",
            "2482:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 15.1s\n",
            "2483:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 15s\n",
            "2484:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.9s\n",
            "2485:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.9s\n",
            "2486:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.8s\n",
            "2487:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.7s\n",
            "2488:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.6s\n",
            "2489:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.5s\n",
            "2490:\tlearn: 0.0050232\ttotal: 3m 11s\tremaining: 14.5s\n",
            "2491:\tlearn: 0.0050231\ttotal: 3m 11s\tremaining: 14.4s\n",
            "2492:\tlearn: 0.0050231\ttotal: 3m 11s\tremaining: 14.3s\n",
            "2493:\tlearn: 0.0050231\ttotal: 3m 11s\tremaining: 14.2s\n",
            "2494:\tlearn: 0.0050231\ttotal: 3m 11s\tremaining: 14.2s\n",
            "2495:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 14.1s\n",
            "2496:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 14s\n",
            "2497:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.9s\n",
            "2498:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.8s\n",
            "2499:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.8s\n",
            "2500:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.7s\n",
            "2501:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.6s\n",
            "2502:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.5s\n",
            "2503:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.5s\n",
            "2504:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.4s\n",
            "2505:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.3s\n",
            "2506:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.2s\n",
            "2507:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.1s\n",
            "2508:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13.1s\n",
            "2509:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 13s\n",
            "2510:\tlearn: 0.0050231\ttotal: 3m 12s\tremaining: 12.9s\n",
            "2511:\tlearn: 0.0050231\ttotal: 3m 13s\tremaining: 12.8s\n",
            "2512:\tlearn: 0.0050231\ttotal: 3m 13s\tremaining: 12.8s\n",
            "2513:\tlearn: 0.0050231\ttotal: 3m 13s\tremaining: 12.7s\n",
            "2514:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.6s\n",
            "2515:\tlearn: 0.0050231\ttotal: 3m 13s\tremaining: 12.5s\n",
            "2516:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.4s\n",
            "2517:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.4s\n",
            "2518:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.3s\n",
            "2519:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.2s\n",
            "2520:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.1s\n",
            "2521:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12.1s\n",
            "2522:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 12s\n",
            "2523:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 11.9s\n",
            "2524:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 11.8s\n",
            "2525:\tlearn: 0.0050230\ttotal: 3m 13s\tremaining: 11.7s\n",
            "2526:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.7s\n",
            "2527:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.6s\n",
            "2528:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.5s\n",
            "2529:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.4s\n",
            "2530:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.4s\n",
            "2531:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.3s\n",
            "2532:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.2s\n",
            "2533:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11.1s\n",
            "2534:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11s\n",
            "2535:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 11s\n",
            "2536:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 10.9s\n",
            "2537:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 10.8s\n",
            "2538:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 10.7s\n",
            "2539:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 10.7s\n",
            "2540:\tlearn: 0.0050230\ttotal: 3m 14s\tremaining: 10.6s\n",
            "2541:\tlearn: 0.0050229\ttotal: 3m 14s\tremaining: 10.5s\n",
            "2542:\tlearn: 0.0050229\ttotal: 3m 14s\tremaining: 10.4s\n",
            "2543:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 10.3s\n",
            "2544:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 10.3s\n",
            "2545:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 10.2s\n",
            "2546:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 10.1s\n",
            "2547:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 10s\n",
            "2548:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.96s\n",
            "2549:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.89s\n",
            "2550:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.81s\n",
            "2551:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.73s\n",
            "2552:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.66s\n",
            "2553:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.58s\n",
            "2554:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.5s\n",
            "2555:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.43s\n",
            "2556:\tlearn: 0.0050229\ttotal: 3m 15s\tremaining: 9.35s\n",
            "2557:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 9.27s\n",
            "2558:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 9.2s\n",
            "2559:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 9.12s\n",
            "2560:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 9.04s\n",
            "2561:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.96s\n",
            "2562:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.89s\n",
            "2563:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.81s\n",
            "2564:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.73s\n",
            "2565:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.65s\n",
            "2566:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.58s\n",
            "2567:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.5s\n",
            "2568:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.42s\n",
            "2569:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.35s\n",
            "2570:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.27s\n",
            "2571:\tlearn: 0.0050229\ttotal: 3m 16s\tremaining: 8.19s\n",
            "2572:\tlearn: 0.0050229\ttotal: 3m 17s\tremaining: 8.12s\n",
            "2573:\tlearn: 0.0050229\ttotal: 3m 17s\tremaining: 8.04s\n",
            "2574:\tlearn: 0.0050229\ttotal: 3m 17s\tremaining: 7.96s\n",
            "2575:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.89s\n",
            "2576:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.81s\n",
            "2577:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.73s\n",
            "2578:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.66s\n",
            "2579:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.58s\n",
            "2580:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.5s\n",
            "2581:\tlearn: 0.0050228\ttotal: 3m 17s\tremaining: 7.42s\n",
            "2582:\tlearn: 0.0050197\ttotal: 3m 17s\tremaining: 7.35s\n",
            "2583:\tlearn: 0.0050175\ttotal: 3m 17s\tremaining: 7.27s\n",
            "2584:\tlearn: 0.0050141\ttotal: 3m 17s\tremaining: 7.19s\n",
            "2585:\tlearn: 0.0050141\ttotal: 3m 17s\tremaining: 7.12s\n",
            "2586:\tlearn: 0.0050141\ttotal: 3m 17s\tremaining: 7.04s\n",
            "2587:\tlearn: 0.0050141\ttotal: 3m 17s\tremaining: 6.96s\n",
            "2588:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.88s\n",
            "2589:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.81s\n",
            "2590:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.73s\n",
            "2591:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.66s\n",
            "2592:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.58s\n",
            "2593:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.5s\n",
            "2594:\tlearn: 0.0050141\ttotal: 3m 18s\tremaining: 6.42s\n",
            "2595:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 6.35s\n",
            "2596:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 6.27s\n",
            "2597:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 6.19s\n",
            "2598:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 6.12s\n",
            "2599:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 6.04s\n",
            "2600:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 5.96s\n",
            "2601:\tlearn: 0.0050140\ttotal: 3m 18s\tremaining: 5.89s\n",
            "2602:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.81s\n",
            "2603:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.73s\n",
            "2604:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.66s\n",
            "2605:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.58s\n",
            "2606:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.5s\n",
            "2607:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.43s\n",
            "2608:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.35s\n",
            "2609:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.27s\n",
            "2610:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.2s\n",
            "2611:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.12s\n",
            "2612:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 5.04s\n",
            "2613:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 4.96s\n",
            "2614:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 4.89s\n",
            "2615:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 4.81s\n",
            "2616:\tlearn: 0.0050140\ttotal: 3m 19s\tremaining: 4.74s\n",
            "2617:\tlearn: 0.0050140\ttotal: 3m 20s\tremaining: 4.66s\n",
            "2618:\tlearn: 0.0050140\ttotal: 3m 20s\tremaining: 4.58s\n",
            "2619:\tlearn: 0.0050140\ttotal: 3m 20s\tremaining: 4.51s\n",
            "2620:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.43s\n",
            "2621:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.35s\n",
            "2622:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.28s\n",
            "2623:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.2s\n",
            "2624:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.12s\n",
            "2625:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 4.05s\n",
            "2626:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.97s\n",
            "2627:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.89s\n",
            "2628:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.82s\n",
            "2629:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.74s\n",
            "2630:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.66s\n",
            "2631:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.59s\n",
            "2632:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.51s\n",
            "2633:\tlearn: 0.0050139\ttotal: 3m 20s\tremaining: 3.43s\n",
            "2634:\tlearn: 0.0050139\ttotal: 3m 21s\tremaining: 3.36s\n",
            "2635:\tlearn: 0.0050139\ttotal: 3m 21s\tremaining: 3.28s\n",
            "2636:\tlearn: 0.0050139\ttotal: 3m 21s\tremaining: 3.21s\n",
            "2637:\tlearn: 0.0050117\ttotal: 3m 21s\tremaining: 3.13s\n",
            "2638:\tlearn: 0.0050071\ttotal: 3m 21s\tremaining: 3.06s\n",
            "2639:\tlearn: 0.0050071\ttotal: 3m 21s\tremaining: 2.98s\n",
            "2640:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.91s\n",
            "2641:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.83s\n",
            "2642:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.75s\n",
            "2643:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.68s\n",
            "2644:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.6s\n",
            "2645:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.52s\n",
            "2646:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.45s\n",
            "2647:\tlearn: 0.0050071\ttotal: 3m 22s\tremaining: 2.37s\n",
            "2648:\tlearn: 0.0050070\ttotal: 3m 22s\tremaining: 2.29s\n",
            "2649:\tlearn: 0.0050070\ttotal: 3m 22s\tremaining: 2.22s\n",
            "2650:\tlearn: 0.0050070\ttotal: 3m 22s\tremaining: 2.14s\n",
            "2651:\tlearn: 0.0050070\ttotal: 3m 22s\tremaining: 2.07s\n",
            "2652:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.99s\n",
            "2653:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.91s\n",
            "2654:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.84s\n",
            "2655:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.76s\n",
            "2656:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.68s\n",
            "2657:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.61s\n",
            "2658:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.53s\n",
            "2659:\tlearn: 0.0050070\ttotal: 3m 23s\tremaining: 1.45s\n",
            "2660:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 1.38s\n",
            "2661:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 1.3s\n",
            "2662:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 1.22s\n",
            "2663:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 1.15s\n",
            "2664:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 1.07s\n",
            "2665:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 994ms\n",
            "2666:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 917ms\n",
            "2667:\tlearn: 0.0050069\ttotal: 3m 23s\tremaining: 841ms\n",
            "2668:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 765ms\n",
            "2669:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 688ms\n",
            "2670:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 611ms\n",
            "2671:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 535ms\n",
            "2672:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 459ms\n",
            "2673:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 382ms\n",
            "2674:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 306ms\n",
            "2675:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 229ms\n",
            "2676:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 153ms\n",
            "2677:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 76.4ms\n",
            "2678:\tlearn: 0.0050069\ttotal: 3m 24s\tremaining: 0us\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x225b9dcf390>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param = {\n",
        "    \"random_state\": 42,\n",
        "    'early_stopping_rounds': 20,\n",
        "    'loss_function': 'Logloss',\n",
        "    'learning_rate': 0.040075823012502135, \n",
        "    'bagging_temperature': 1.388074362792074, \n",
        "    'n_estimators': 2679, \n",
        "    'max_depth': 8, \n",
        "    'random_strength': 43, \n",
        "    'l2_leaf_reg': 1.814211208460745e-05, \n",
        "    'min_child_samples': 44, \n",
        "    'max_bin': 288\n",
        "}\n",
        "model = cb.CatBoostClassifier(**param)\n",
        "\n",
        "model.fit(Pool(data['X_train'], label=data['y_train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_clf_eval(y_test, y_pred=None):\n",
        "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
        "\n",
        "    print(\"오차행렬:\\n\", confusion)\n",
        "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
        "    print(\"정밀도: {:.4f}\".format(precision))\n",
        "    print(\"재현율: {:.4f}\".format(recall))\n",
        "    print(\"F1: {:.4f}\".format(F1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터로 예측\n",
        "pred = model.predict(data['X_valid'])\n",
        "pred = [val == 1 for val in pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "오차행렬:\n",
            " [[10134    98]\n",
            " [   92 10140]]\n",
            "\n",
            "정확도: 0.9907\n",
            "정밀도: 0.9910\n",
            "재현율: 0.9904\n",
            "F1: 0.9907\n"
          ]
        }
      ],
      "source": [
        "get_clf_eval(data['y_valid'], pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 데이터 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 예측에 필요한 데이터 분리\n",
        "test_pred = model.predict(data['test'].drop([\"is_converted\", \"id\"], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_pred = [val == 1 for val in test_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "547"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_pred) # True로 예측된 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 제출 파일 작성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission_fe.csv\")\n",
        "df_sub[\"is_converted\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
